<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="ch_configuring-openstack-compute"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns5="http://www.w3.org/1999/xhtml"
         xmlns:ns4="http://www.w3.org/2000/svg"
         xmlns:ns3="http://www.w3.org/1998/Math/MathML"
         xmlns:ns="http://docbook.org/ns/docbook">
  <title>Configuring OpenStack Compute</title>

  <para>The OpenStack system has several key projects that are separate
  installations but can work together depending on your cloud needs: OpenStack
  Compute, OpenStack Object Storage, and OpenStack Image Store. There are basic configuration
  decisions to make, and the <link xlink:href="http://docs.openstack.org/trunk/openstack-compute/install/content/">OpenStack Install Guide</link>
    covers a basic walkthrough.</para>

    <section xml:id="configuring-openstack-compute-basics">
    <?dbhtml stop-chunking?>
    <title>Post-Installation Configuration for OpenStack Compute</title>

    <para>Configuring your Compute installation involves many
      configuration files - the <filename>nova.conf</filename> file,
      the <filename>api-paste.ini</filename> file, and related Image and Identity
      management configuration files. This section contains the basics
      for a simple multi-node installation, but Compute can be
      configured many ways. You can find networking options and
      hypervisor options described in separate chapters.</para>

    <section xml:id="setting-flags-in-nova-conf-file">
      <title>Setting Configuration Options in the
          <filename>nova.conf</filename> File</title>

      <para>The configuration file <filename>nova.conf</filename> is
        installed in <filename>/etc/nova</filename> by default. A
        default set of options are already configured in
          <filename>nova.conf</filename> when you install manually.   </para>

      <para>Starting with the default file, you must define the
        following required items in
          <filename>/etc/nova/nova.conf</filename>. The options are
        described below. You can place comments in the
          <filename>nova.conf</filename> file by entering a new line
        with a <literal>#</literal> sign at the beginning of the line.
        To see a listing of all possible configuration options, refer
        to the <link linkend="compute-options-reference">Compute Options Reference</link>.</para>

      <para>Here is a simple example <filename>nova.conf</filename>
        file for a small private cloud, with all the cloud controller
        services, database server, and messaging server on the same
        server. In this case, CONTROLLER_IP represents the IP address
        of a central server, BRIDGE_INTERFACE represents the bridge
        such as br100, the NETWORK_INTERFACE represents an interface
        to your VLAN setup, and passwords are represented as
        DB_PASSWORD_COMPUTE for your Compute (nova) database password,
        and RABBIT PASSWORD represents the password to your rabbit
        installation.</para>

      <programlisting>
      <xi:include  parse="text" href="../openstack-install/samples/nova.conf"/>
      </programlisting>

      <note>
        <para>If your OpenStack deployment uses Qpid as the message queue instead
        of RabbitMQ (e.g., on Fedora, CentOS, RHEL), you would see 
        <literal>qpid_hostname</literal> instead of <literal>rabbit_host</literal>
        in the <filename>nova.conf</filename> file.</para></note>

<para>Create a <literal>nova</literal> group, so you can set permissions on the
      configuration file:</para>

      <screen><prompt>$</prompt> <userinput>sudo addgroup nova</userinput></screen>

      <para>The <filename>nova.conf</filename> file should have its
        owner set to <literal>root:nova</literal>, and mode set to
          <literal>0640</literal>, since the file could contain your
        MySQL server’s username and password. You also want to ensure
        that the <literal>nova</literal> user belongs to the
          <literal>nova</literal> group.</para>

      <screen><prompt>$</prompt> <userinput>sudo usermod -g nova nova</userinput>
<prompt>$</prompt> <userinput>chown -R <option>username</option>:nova /etc/nova</userinput>
<prompt>$</prompt> <userinput>chmod 640 /etc/nova/nova.conf</userinput></screen>
    </section>

    <section xml:id="setting-up-openstack-compute-environment-on-the-compute-node">
      <title>Setting Up OpenStack Compute Environment on the Compute
      Node</title>

      <para>These are the commands you run to ensure the database
        schema is current:</para>

      <screen><prompt>$</prompt> <userinput>nova-manage db sync</userinput></screen>
    </section>
    <section xml:id="creating-credentials"><title>Creating Credentials</title>
      <para>The credentials you will use to launch
        instances, bundle images, and all the other assorted
        API functions can be sourced in a single file, such as
        creating one called <filename>/creds/openrc</filename>. </para>
    <para>Here's an example <filename>openrc</filename> file you can download from
        the Dashboard in Settings > Project Settings >
        Download RC File. </para>
      <para>
        <programlisting language="bash">
#!/bin/bash
# *NOTE*: Using the 2.0 *auth api* does not mean that compute api is 2.0.  We
# will use the 1.1 *compute api*
export OS_AUTH_URL=http://50.56.12.206:5000/v2.0
export OS_TENANT_ID=27755fd279ce43f9b17ad2d65d45b75c
export OS_USERNAME=vish
export OS_PASSWORD=$OS_PASSWORD_INPUT
export OS_AUTH_USER=norm
export OS_AUTH_KEY=$OS_PASSWORD_INPUT
export OS_AUTH_TENANT=27755fd279ce43f9b17ad2d65d45b75c
export OS_AUTH_STRATEGY=keystone
</programlisting>
      </para>
      <para>You also may want to enable EC2 access for the
        euca2ools. Here is an example <filename>ec2rc</filename> file for enabling
        EC2 access with the required credentials.</para>
      <para>
        <programlisting language="bash">
export NOVA_KEY_DIR=/root/creds/
export EC2_ACCESS_KEY="EC2KEY:USER"
export EC2_SECRET_KEY="SECRET_KEY"
export EC2_URL="http://$NOVA-API-IP:8773/services/Cloud"
export S3_URL="http://$NOVA-API-IP:3333"
export EC2_USER_ID=42 # nova does not use user id, but bundling requires it
export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem
export EC2_CERT=${NOVA_KEY_DIR}/cert.pem
export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem
export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set
alias ec2-bundle-image="ec2-bundle-image --cert ${EC2_CERT} --privatekey ${EC2_PRIVATE_KEY} --user 42 --ec2cert ${NOVA_CERT}"
alias ec2-upload-bundle="ec2-upload-bundle -a ${EC2_ACCESS_KEY} -s ${EC2_SECRET_KEY} --url ${S3_URL} --ec2cert ${NOVA_CERT}"</programlisting>
      </para>
      <para>Lastly, here is an example openrc file that works
        with nova client and ec2
        tools.</para>
      <programlisting language="bash">
export OS_PASSWORD=${ADMIN_PASSWORD:-secrete}
export OS_AUTH_URL=${OS_AUTH_URL:-http://$SERVICE_HOST:5000/v2.0}
export NOVA_VERSION=${NOVA_VERSION:-1.1}
export OS_REGION_NAME=${OS_REGION_NAME:-RegionOne}
export EC2_URL=${EC2_URL:-http://$SERVICE_HOST:8773/services/Cloud}
export EC2_ACCESS_KEY=${DEMO_ACCESS}
export EC2_SECRET_KEY=${DEMO_SECRET}
export S3_URL=http://$SERVICE_HOST:3333
export EC2_USER_ID=42 # nova does not use user id, but bundling requires it
export EC2_PRIVATE_KEY=${NOVA_KEY_DIR}/pk.pem
export EC2_CERT=${NOVA_KEY_DIR}/cert.pem
export NOVA_CERT=${NOVA_KEY_DIR}/cacert.pem
export EUCALYPTUS_CERT=${NOVA_CERT} # euca-bundle-image seems to require this set</programlisting>
      <para>Next, add these credentials to your environment
        prior to running any nova client commands or nova
        commands. </para>
      <screen><prompt>$</prompt> <userinput>cat /root/creds/openrc >> ~/.bashrc
source ~/.bashrc</userinput></screen>
    </section>
    <section xml:id="creating-certifications">
      <title>Creating Certificates</title>
      <para>You can create certificates contained within pem
        files using these nova client
        commands, ensuring you have set up your environment variables for the nova client:
        <screen><prompt>#</prompt> <userinput>nova x509-get-root-cert</userinput>
<prompt>#</prompt> <userinput>nova x509-create-cert </userinput></screen>
      </para>
     </section>

    <section xml:id="creating-networks">
      <title>Creating networks</title>
      <para>You need to populate the database with the network configuration information that
        Compute obtains from the <filename>nova.conf</filename> file. You can find out more about
        the <command>nova network-create</command> command with <userinput>nova help network-create</userinput>.
    </para>
      <para>Here is an example of what this looks like with real values entered. This example would
        be appropriate for FlatDHCP mode, for VLAN Manager mode you would also need to specify a
        VLAN.</para>
      <screen><prompt>$</prompt> <userinput>nova network-create novanet --fixed-range-v4 192.168.0.0/24</userinput></screen>
      <para>For this example, the number of IPs is <literal>/24</literal> since that falls inside
        the <literal>/16</literal> range that was set in <literal>fixed-range</literal> in
          <filename>nova.conf</filename>. Currently, there can only be one network, and this set up
        would use the max IPs available in a <literal>/24</literal>. You can choose values that let
        you use any valid amount that you would like. </para>
      <para>OpenStack Compute assumes that the first IP address is your network (like
          <literal>192.168.0.0</literal>), that the 2nd IP is your gateway
          (<literal>192.168.0.1</literal>), and that the broadcast is the very last IP in the range
        you defined (<literal>192.168.0.255</literal>). You can alter the gateway using the
          <literal>--gateway</literal> flag when invoking <command>nova network-create</command>.
        You are unlikely to need to modify the network or broadcast addresseses, but if you do, you
        will need to manually edit the <literal>networks</literal> table in the database.</para>
     </section>

    <section xml:id="enabling-access-to-vms-on-the-compute-node">
      <title>Enabling Access to VMs on the Compute Node</title>

      <para>One of the most commonly missed configuration areas is not
      allowing the proper access to VMs. Use nova client commands to enable access. Below, you
      will find the commands to allow <command>ping</command> and
      <command>ssh</command> to your VMs :</para>

      <note>
        <para>These commands need to be run as root only if the credentials
        used to interact with <command>nova-api</command> have been put under
        <filename>/root/.bashrc</filename>. If the EC2 credentials have been
        put into another user's <filename>.bashrc</filename> file, then, it is
        necessary to run these commands as the user.</para>
      </note>

      <screen><prompt>$</prompt> <userinput>nova secgroup-add-rule default  icmp -1 -1 0.0.0.0/0</userinput>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default  tcp 22 22 0.0.0.0/0</userinput></screen>

      <para>Another common issue is you cannot ping or SSH to your instances
      after issuing the <command>euca-authorize</command> commands. Something
      to look at is the amount of <command>dnsmasq</command> processes that
      are running. If you have a running instance, check to see that TWO
      <command>dnsmasq</command> processes are running. If not, perform the
      following:</para>

      <screen><prompt>$</prompt> <userinput>sudo killall dnsmasq</userinput>
<prompt>$</prompt> <userinput>sudo service nova-network restart</userinput></screen>

      <para>If you get the <literal>instance not found</literal> message while
      performing the restart, that means the service was not previously
      running. You simply need to start it instead of restarting it: </para>

      <screen><prompt>$</prompt> <userinput>sudo service nova-network start</userinput></screen>
    </section>

    <section xml:id="configuring-multiple-compute-nodes">
      <title>Configuring Multiple Compute Nodes</title>

      <para>If your goal is to split your VM load across more than one server,
      you can connect an additional <command>nova-compute</command> node to a
      cloud controller node. This configuring can be reproduced on multiple
      compute servers to start building a true multi-node OpenStack Compute
      cluster.</para>

      <para>To build out and scale the Compute platform, you spread out
      services amongst many servers. While there are additional ways to
      accomplish the build-out, this section describes adding compute nodes,
      and the service we are scaling out is called
      <command>nova-compute</command>.</para>

      <para>For a multi-node install you only make changes to
          <filename>nova.conf</filename> and copy it to additional
        compute nodes. Ensure each <filename>nova.conf</filename> file
        points to the correct IP addresses for the respective
        services. </para>

      <para>By default, Nova sets the bridge device based on the
        setting in <literal>flat_network_bridge</literal>. Now you can
        edit <filename>/etc/network/interfaces</filename> with the
        following template, updated with your IP information.</para>

<programlisting language="bash">
# The loopback network interface
auto lo
    iface lo inet loopback

# The primary network interface
auto br100
iface br100 inet static
    bridge_ports    eth0
    bridge_stp      off
    bridge_maxwait  0
    bridge_fd       0
    address <replaceable>xxx.xxx.xxx.xxx</replaceable>
    netmask <replaceable>xxx.xxx.xxx.xxx</replaceable>
    network <replaceable>xxx.xxx.xxx.xxx</replaceable>
    broadcast <replaceable>xxx.xxx.xxx.xxx</replaceable>
    gateway <replaceable>xxx.xxx.xxx.xxx</replaceable>
    # dns-* options are implemented by the resolvconf package, if installed
    dns-nameservers <replaceable>xxx.xxx.xxx.xxx</replaceable>
</programlisting>

      <para>Restart networking:</para>

      <screen><prompt>$</prompt> <userinput>sudo service networking restart</userinput></screen>

      <para>With <filename>nova.conf</filename> updated and networking set,
      configuration is nearly complete. First, bounce the relevant services to
      take the latest updates:</para>

      <screen><prompt>$</prompt> <userinput>sudo service libvirtd restart</userinput>
$ <userinput>sudo service nova-compute restart</userinput></screen>

      <para>To avoid issues with KVM and permissions with Nova, run the
      following commands to ensure we have VM's that are running
      optimally:</para>

      <screen><prompt>#</prompt> <userinput>chgrp kvm /dev/kvm</userinput>
<prompt>#</prompt> <userinput>chmod g+rwx /dev/kvm</userinput></screen>

      <para>If you want to use the 10.04 Ubuntu Enterprise Cloud images that
      are readily available at
      http://uec-images.ubuntu.com/releases/10.04/release/, you may run into
      delays with booting. Any server that does not have
      <command>nova-api</command> running on it needs this iptables entry so
      that UEC images can get metadata info. On compute nodes, configure the
      iptables with this next step:</para>

      <screen><prompt>#</prompt> <userinput>iptables -t nat -A PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination <replaceable>$NOVA_API_IP</replaceable>:8773</userinput></screen>

      <para>Lastly, confirm that your compute node is talking to your cloud
      controller. From the cloud controller, run this database query:</para>

      <screen><prompt>$</prompt> <userinput>mysql -u<replaceable>$MYSQL_USER</replaceable> -p<replaceable>$MYSQL_PASS</replaceable> nova -e 'select * from services;'</userinput></screen>

      <para>In return, you should see something similar to this:</para>

      <screen><computeroutput>+---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+
| created_at          | updated_at          | deleted_at | deleted | id | host     | binary         | topic     | report_count | disabled | availability_zone |
+---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+
| 2011-01-28 22:52:46 | 2011-02-03 06:55:48 | NULL       |       0 |  1 | osdemo02 | nova-network   | network   |        46064 |        0 | nova              |
| 2011-01-28 22:52:48 | 2011-02-03 06:55:57 | NULL       |       0 |  2 | osdemo02 | nova-compute   | compute   |        46056 |        0 | nova              |
| 2011-01-28 22:52:52 | 2011-02-03 06:55:50 | NULL       |       0 |  3 | osdemo02 | nova-scheduler | scheduler |        46065 |        0 | nova              |
| 2011-01-29 23:49:29 | 2011-02-03 06:54:26 | NULL       |       0 |  4 | osdemo01 | nova-compute   | compute   |        37050 |        0 | nova              |
| 2011-01-30 23:42:24 | 2011-02-03 06:55:44 | NULL       |       0 |  9 | osdemo04 | nova-compute   | compute   |        28484 |        0 | nova              |
| 2011-01-30 21:27:28 | 2011-02-03 06:54:23 | NULL       |       0 |  8 | osdemo05 | nova-compute   | compute   |        29284 |        0 | nova              |
+---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+</computeroutput>       </screen>

      <para>You can see that <literal>osdemo0{1,2,4,5}</literal> are all
      running <command>nova-compute</command>. When you start spinning up
      instances, they will allocate on any node that is running
      <command>nova-compute</command> from this list.</para>
    </section>

    <section xml:id="determining-version-of-compute">
      <title>Determining the Version of Compute</title>

      <para>You can find the version of the installation by using the
      <command>nova-manage</command> command:</para>

      <screen><prompt>$</prompt> <userinput>nova-manage version list</userinput></screen>
    </section>

      <section xml:id="diagnose-compute">
        <title>Diagnose your compute nodes</title>

        <para>You can obtain extra informations about the running virtual machines: their CPU usage,
        the memory, the disk IO or network IO, per instance, by running the <command>nova
          diagnostics</command> command with a server ID:</para>

        <screen><prompt>$</prompt> <userinput>nova diagnostics &lt;serverID&gt;</userinput></screen>
        <para>The output of this command will vary depending on the hypervisor. Example output when
        the hypervisor is Xen:
        <screen><computeroutput>+----------------+-----------------+
|    Property    |      Value      |
+----------------+-----------------+
| cpu0           | 4.3627          |
| memory         | 1171088064.0000 |
| memory_target  | 1171088064.0000 |
| vbd_xvda_read  | 0.0             |
| vbd_xvda_write | 0.0             |
| vif_0_rx       | 3223.6870       |
| vif_0_tx       | 0.0             |
| vif_1_rx       | 104.4955        |
| vif_1_tx       | 0.0             |
+----------------+-----------------+</computeroutput></screen>
        While the command should work with any hypervisor that is controlled through libvirt (e.g.,
        KVM, QEMU, LXC), it has only been tested with KVM. Example output when the hypervisor is
        KVM:</para>
        <screen><computeroutput>+------------------+------------+
| Property         | Value      |
+------------------+------------+
| cpu0_time        | 2870000000 |
| memory           | 524288     |
| vda_errors       | -1         |
| vda_read         | 262144     |
| vda_read_req     | 112        |
| vda_write        | 5606400    |
| vda_write_req    | 376        |
| vnet0_rx         | 63343      |
| vnet0_rx_drop    | 0          |
| vnet0_rx_errors  | 0          |
| vnet0_rx_packets | 431        |
| vnet0_tx         | 4905       |
| vnet0_tx_drop    | 0          |
| vnet0_tx_errors  | 0          |
| vnet0_tx_packets | 45         |
+------------------+------------+</computeroutput></screen>
      </section>
    </section>

  <section xml:id="general-compute-configuration-overview">
    <title>General Compute Configuration Overview</title>

    <para>Most configuration information is available in the
        <filename>nova.conf</filename> configuration option file. Here
      are some general purpose configuration options that you can use
      to learn more about the configuration option file and the node.
      The configuration file nova.conf is typically stored in
        <filename>/etc/nova/nova.conf</filename>.</para>

    <para>You can use a particular configuration option file by using
      the <literal>option</literal> (<filename>nova.conf</filename>)
      parameter when running one of the <literal>nova-*</literal> services.
      This inserts configuration option definitions from the given configuration
      file name, which may be useful for debugging or performance
      tuning. Here are some general purpose configuration
      options.</para>

    <para>If you want to maintain the state of all the services, you
        can use the <literal>state_path</literal> configuration option to indicate a
        top-level directory for storing data related to the state of
        Compute including images if you are using the Compute object
        store. </para>
  <xi:include href="../common/tables/nova-common.xml"/>
  </section>

  <section xml:id="sample-nova-configuration-files">
  <title>Example <filename>nova.conf</filename> Configuration Files</title>
  <para>
    The following sections describe many of the configuration option settings that can go into the
    <filename>nova.conf</filename> files. Copies of each <filename>nova.conf</filename> file need
    to be copied to each compute node. Here are some sample <filename>nova.conf</filename>
    files that offer examples of specific configurations.
   </para>
    <simplesect>
      <title>KVM, Flat, MySQL, and Glance, OpenStack or EC2 API</title>

      <para>This example <filename>nova.conf</filename> file is from an
      internal Rackspace test system used for demonstrations.</para>
<programlisting>
      <xi:include parse="text" href="../openstack-install/samples/nova.conf"/>
</programlisting>
      <figure xml:id="Nova_conf_KVM_Flat">
        <title>KVM, Flat, MySQL, and Glance, OpenStack or EC2 API</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="figures/SCH_5004_V00_NUAC-Network_mode_KVM_Flat_OpenStack.png"
                       contentwidth="6in"/>
          </imageobject>
        </mediaobject>
      </figure>
    </simplesect>

    <simplesect>
      <title>XenServer, Flat networking, MySQL, and Glance, OpenStack
      API</title>

      <para>This example <filename>nova.conf</filename> file is from an
      internal Rackspace test system.</para>

      <programlisting>
verbose
nodaemon
sql_connection=mysql://root:&lt;password&gt;@127.0.0.1/nova
network_manager=nova.network.manager.FlatManager
image_service=nova.image.glance.GlanceImageService
flat_network_bridge=xenbr0
compute_driver=xenapi.XenAPIDriver
xenapi_connection_url=https://&lt;XenServer IP&gt;
xenapi_connection_username=root
xenapi_connection_password=supersecret
rescue_timeout=86400
xenapi_inject_image=false
use_ipv6=true

# To enable flat_injected, currently only works on Debian-based systems
flat_injected=true
ipv6_backend=account_identifier
ca_path=./nova/CA

# Add the following to your conf file if you're running on Ubuntu Maverick
xenapi_remap_vbd_dev=true
            </programlisting>

      <figure xml:id="Nova_conf_XEN_Flat">
        <title>KVM, Flat, MySQL, and Glance, OpenStack or EC2 API</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="figures/SCH_5005_V00_NUAC-Network_mode_XEN_Flat_OpenStack.png"
                       scale="60"/>
          </imageobject>
        </mediaobject>
      </figure>
    </simplesect>
  </section>
  <section xml:id="configuring-logging">
    <title>Configuring Logging</title>
    <para>You can use <filename>nova.conf</filename> configuration
      options to indicate where Compute will log events, the level of
      logging, and customize log formats.</para>
    <para>To customize log formats for
      OpenStack Compute, use these configuration option
      settings.</para>
    <xi:include href="../common/tables/nova-logging.xml"/>
  </section>

  <section xml:id="configuring-hypervisors">
    <title>Configuring Hypervisors</title>

    <para>OpenStack Compute requires a hypervisor and supports several
      hypervisors and virtualization standards. Configuring and
      running OpenStack Compute to use a particular hypervisor takes
      several installation and configuration steps. The
        <literal>libvirt_type</literal> configuration option indicates
      which hypervisor will be used. Refer to <link
        linkend="hypervisor-configuration-basics">Hypervisor Configuration Basics</link> for more
      details. To customize hypervisor support in OpenStack Compute,
      refer to these configuration settings in
        <filename>nova.conf</filename>.</para>
    <xi:include href="../common/tables/nova-hypervisor.xml"/>
  </section>

  <section xml:id="configuring-authentication-authorization">
    <title>Configuring Authentication and Authorization</title>

    <para>There are different methods of authentication for the
      OpenStack Compute project, including no authentication. The preferred 
      system is the OpenStack Identity Service, code-named Keystone. Refer to
      <link linkend="ch-identity-mgmt-config">Identity Management</link> for additional information. </para>
    <para>To customize authorization settings for Compute, see these
      configuration settings in <filename>nova.conf</filename>.</para>

    <xi:include href="../common/tables/nova-authentication.xml"/>
    <para>To customize certificate authority settings for Compute, see these configuration settings in <filename>nova.conf</filename>.</para>
    <xi:include href="../common/tables/nova-ca.xml"/>
    <para>To customize Compute and the Identity service to use LDAP as a backend, refer to these configuration settings in <filename>nova.conf</filename>.</para>
    <xi:include href="../common/tables/nova-ldap.xml"/>
  </section>

  <section xml:id="configuring-compute-to-use-ipv6-addresses">
    <title>Configuring Compute to use IPv6 Addresses</title>

    <para>You can configure Compute to use both IPv4 and IPv6 addresses for
    communication by putting it into a IPv4/IPv6 dual stack mode. In IPv4/IPv6
    dual stack mode, instances can acquire their IPv6 global unicast address
    by stateless address autoconfiguration mechanism [RFC 4862/2462].
    IPv4/IPv6 dual stack mode works with <literal>VlanManager</literal> and <literal>FlatDHCPManager</literal>
    networking modes. In <literal>VlanManager</literal>, different 64bit global routing prefix is used for
    each project. In <literal>FlatDHCPManager</literal>, one 64bit global routing prefix is used
    for all instances.</para>

    <para>This configuration has been tested with VM images
    that have IPv6 stateless address autoconfiguration capability (must use
    EUI-64 address for stateless address autoconfiguration), a requirement for
    any VM you want to run with an IPv6 address. Each node that executes a
    <literal>nova-*</literal> service must have <literal>python-netaddr</literal>
    and <literal>radvd</literal> installed.</para>

    <para>On all nova-nodes, install python-netaddr:</para>

    <screen><prompt>$</prompt> <userinput>sudo apt-get install -y python-netaddr</userinput></screen>

    <para>On all <literal>nova-network</literal> nodes install <literal>radvd</literal> and configure IPv6
    networking:</para>

    <screen><prompt>$</prompt> <userinput>sudo apt-get install -y radvd</userinput>
<prompt>$</prompt> <userinput>sudo bash -c "echo 1 &gt; /proc/sys/net/ipv6/conf/all/forwarding"</userinput>
<prompt>$</prompt> <userinput>sudo bash -c "echo 0 &gt; /proc/sys/net/ipv6/conf/all/accept_ra"</userinput></screen>

    <para>Edit the <filename>nova.conf</filename> file on all nodes to
      set the use_ipv6 configuration option to True. Restart all
      nova- services.</para>

    <para>When using the command <command>nova network-create</command> you can add a fixed range
      for IPv6 addresses. You must specify public or private after the create parameter.</para>

    <screen><prompt>$</prompt> <userinput>nova network-create public --fixed-range-v4 <replaceable>fixed_range</replaceable> --vlan <replaceable>vlan_id</replaceable> --vpn <replaceable>vpn_start</replaceable> --fixed-range-v6 <replaceable>fixed_range_v6</replaceable></userinput></screen>

    <para>You can set IPv6 global routing prefix by using the <literal>--fixed_range_v6</literal>
      parameter. The default is: <literal>fd00::/48</literal>. When you use
        <literal>FlatDHCPManager</literal>, the command uses the original value of
        <literal>--fixed_range_v6</literal>. When you use <literal>VlanManager</literal>, the
      command creates prefixes of subnet by incrementing subnet id. Guest VMs uses this prefix for
      generating their IPv6 global unicast address.</para>

    <para>Here is a usage example for <literal>VlanManager</literal>:</para>

    <screen><prompt>$</prompt> <userinput>nova network-create public --fixed-range-v4 10.0.1.0/24 --vlan 100 --vpn 1000 --fixed-range-v6 fd00:1::/48 </userinput></screen>

    <para>Here is a usage example for <literal>FlatDHCPManager</literal>:</para>

    <screen><prompt>$</prompt> <userinput>nova network-create public  --fixed-range-v4 10.0.2.0/24 --fixed-range-v6 fd00:1::/48 </userinput></screen>

    <xi:include href="../common/tables/nova-ipv6.xml"/>
  </section>

  <section xml:id="configuring-compute-to-use-the-image-service">
    <title>Configuring Image Service and Storage for Compute</title>

    <para>Compute relies on an external image service to store virtual
      machine images and maintain a catalog of available images. Compute
      is configured by default to use the OpenStack Image service (Glance),
      which is the only currently supported image service.</para>

    <xi:include href="../common/tables/nova-glance.xml"/>

    <para>If your installation requires the use of euca2ools for registering
      new images, you will need to run the <literal>nova-objectstore</literal> service.
      This service provides an Amazon S3 frontend for Glance, which is needed
      because euca2ools can only upload images to an S3-compatible image
      store.</para>
    <xi:include href="../common/tables/nova-s3.xml"/>
  </section>

  <section xml:id="configuring-migrations">
    <?dbhtml stop-chunking?>
    <title>Configuring Migrations</title>

    <note>
      <para>This feature is for cloud administrators only.
      </para>
    </note>

    <para>Migration allows an administrator to move a virtual machine instance from one compute host
      to another. This feature is useful when a compute host requires maintenance. Migration can also
      be useful to redistribute the load when many VM instances are running on a specific physical machine. </para>

    <para>There are two types of migration:
      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Migration</emphasis> (or non-live migration):
          In this case the instance will be shut down (and the instance will
          know that it has been rebooted) for a period of time in order to be
          moved to another hypervisor.</para>
        </listitem>
        <listitem>
          <para><emphasis role="bold">Live migration</emphasis> (or true live migration):
          Almost no instance downtime, it is useful when the instances must be kept
          running during the migration.</para>
        </listitem>
      </itemizedlist>
    </para>

    <para>There are two types of <emphasis role="bold">live migration</emphasis>:
      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Shared storage based live migration</emphasis>: In this case both hypervisors have access to a shared storage.</para>
        </listitem>
        <listitem>
          <para><emphasis role="bold">Block live migration</emphasis>: for this type of migration, no shared storage is required.</para>
        </listitem>
      </itemizedlist>
    </para>

    <para>The following sections describe how to configure your hosts and compute nodes
      for migrations using the KVM and XenServer hypervisors.
    </para>

    <section xml:id="configuring-migrations-kvm-libvirt">
    <title>KVM-Libvirt</title>

    <para><emphasis role="bold">Prerequisites</emphasis>
      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Hypervisor:</emphasis> KVM with libvirt</para>
        </listitem>
        <listitem>
            <para><emphasis role="bold">Shared storage:</emphasis> <filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename> (eg
              <filename>/var/lib/nova/instances</filename>) has to be mounted by shared storage. This guide uses NFS but
              other options, including the <link
              xlink:href="http://gluster.org/community/documentation//index.php/OSConnect">OpenStack
              Gluster Connector</link> are available.</para>
        </listitem>
        <listitem>
          <para><emphasis role="bold">Instances:</emphasis> Instance can be migrated with iSCSI
            based volumes</para>
        </listitem>
      </itemizedlist>
      <note>
        <para>Migrations done by the Compute service do not use libvirt's live migration
          functionality by default. Because of this, guests are suspended before migration and may
          therefore experience several minutes of downtime. See <link linkend="true-live-migration-kvm-libvirt">True Migration for KVM and Libvirt</link>
           for more details.</para>
      </note>
      <note>
          <para> This guide assumes the default value for <literal>instances_path</literal> in your nova.conf
              (<filename><replaceable>NOVA-INST-DIR</replaceable>/instances</filename>). If you have changed the <literal>state_path</literal>
              or <literal>instances_path</literal>
              variables, please modify accordingly. </para>
      </note>
      <note>
        <para>You must specify <literal>vncserver_listen=0.0.0.0</literal> or live migration will
          not work correctly. </para>
      </note>
    </para>

    <para><emphasis role="bold">Example Nova Installation Environment</emphasis> <itemizedlist>
        <listitem>
            <para>Prepare 3 servers at least; for example, <literal>HostA</literal>, <literal>HostB</literal>
                and <literal>HostC</literal></para>
        </listitem>

        <listitem>
            <para><literal>HostA</literal> is the "Cloud Controller", and should be running: <literal>nova-api</literal>,
                <literal>nova-scheduler</literal>, <literal>nova-network</literal>, <literal>cinder-volume</literal>,
                <literal>nova-objectstore</literal>.</para>
        </listitem>

        <listitem>
            <para><literal>HostB</literal> and <literal>HostC</literal> are the "compute nodes", running <literal>nova-compute</literal>.</para>
        </listitem>

        <listitem>
            <para>Ensure that, <literal><replaceable>NOVA-INST-DIR</replaceable></literal> (set with <literal>state_path</literal> in <filename>nova.conf</filename>) is same on
            all hosts.</para>
        </listitem>

        <listitem>
            <para>In this example, <literal>HostA</literal> will be the NFSv4 server which exports <filename><replaceable>NOVA-INST-DIR</replaceable>/instances</filename>,
                and <literal>HostB</literal> and <literal>HostC</literal> mount it.</para>
        </listitem>
      </itemizedlist></para>

    <para><emphasis role="bold">System configuration</emphasis></para>

    <para><orderedlist>
        <listitem>
          <para>Configure your DNS or <filename>/etc/hosts</filename> and
             ensure it is consistent across all hosts. Make sure that the three hosts
            can perform name resolution with each other. As a test,
            use the <command>ping</command> command to ping each host from one
            another.</para>

          <screen><prompt>$</prompt> <userinput>ping HostA</userinput>
<prompt>$</prompt> <userinput>ping HostB</userinput>
<prompt>$</prompt> <userinput>ping HostC</userinput></screen>
        </listitem>
	<listitem><para>Ensure that the UID and GID of your nova and libvirt users
         are identical between each of your servers. This ensures that the permissions
         on the NFS mount will work correctly.</para>
	</listitem>
        <listitem>
          <para>Follow the instructions at

<link xlink:href="https://help.ubuntu.com/community/SettingUpNFSHowTo">the Ubuntu NFS HowTo to
    setup an NFS server on <literal>HostA</literal>, and NFS Clients on <literal>HostB</literal> and <literal>HostC</literal>.</link> </para>
        <para> Our aim is to export <filename><replaceable>NOVA-INST-DIR</replaceable>/instances</filename> from <literal>HostA</literal>,
            and have it readable and writable by the nova user on <literal>HostB</literal> and <literal>HostC</literal>.</para>
        </listitem>
        <listitem>
        <para>
        Using your knowledge from the Ubuntu documentation, configure the
        NFS server at <literal>HostA</literal> by adding a line to <filename>/etc/exports</filename>
          <programlisting><replaceable>NOVA-INST-DIR</replaceable>/instances HostA/255.255.0.0(rw,sync,fsid=0,no_root_squash)</programlisting>
        </para>
          <para>Change the subnet mask (<literal>255.255.0.0</literal>) to the appropriate
              value to include the IP addresses of <literal>HostB</literal> and <literal>HostC</literal>. Then
              restart the NFS server.</para>

          <screen><prompt>$</prompt> <userinput>/etc/init.d/nfs-kernel-server restart</userinput>
<prompt>$</prompt> <userinput>/etc/init.d/idmapd restart</userinput></screen>
        </listitem>
        <listitem>
           <para>Set the 'execute/search' bit on your shared directory </para>
          <para>On both compute nodes, make sure to enable the
            'execute/search' bit to allow qemu to be able to use the images
            within the directories. On all hosts, execute the
            following command: </para>
          <screen><prompt>$</prompt> <userinput>chmod o+x <replaceable>NOVA-INST-DIR</replaceable>/instances</userinput> </screen>
        </listitem>
        <listitem>
          <para>Configure NFS at HostB and HostC by adding below to
              <filename>/etc/fstab</filename>.</para>

          <programlisting>HostA:/ /<replaceable>NOVA-INST-DIR</replaceable>/instances nfs4 defaults 0 0</programlisting>

          <para>Then ensure that the exported
            directory can be mounted.</para>

          <screen><prompt>$</prompt> <userinput>mount -a -v</userinput></screen>

          <para>Check that "<filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename>"
          directory can be seen at HostA</para>

          <screen><prompt>$</prompt> <userinput>ls -ld <filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename></userinput></screen>


          <programlisting language="bash">
drwxr-xr-x 2 nova nova 4096 2012-05-19 14:34 nova-install-dir/instances/
          </programlisting>

          <para>Perform the same check at HostB and HostC - paying special
           attention to the permissions (nova should be able to write)</para>

          <screen><prompt>$</prompt> <userinput>ls -ld <filename><replaceable>NOVA-INST-DIR</replaceable>/instances/</filename></userinput></screen>

          <programlisting language="bash">
drwxr-xr-x 2 nova nova 4096 2012-05-07 14:34 nova-install-dir/instances/
          </programlisting>

          <screen><prompt>$</prompt> <userinput>df -k</userinput></screen>

          <programlisting language="bash">
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/sda1            921514972   4180880 870523828   1% /
none                  16498340      1228  16497112   1% /dev
none                  16502856         0  16502856   0% /dev/shm
none                  16502856       368  16502488   1% /var/run
none                  16502856         0  16502856   0% /var/lock
none                  16502856         0  16502856   0% /lib/init/rw
HostA:        921515008 101921792 772783104  12% /var/lib/nova/instances  ( &lt;--- this line is important.)
</programlisting>
        </listitem>
      <listitem>
        <para>Update the libvirt configurations so that the calls can be made securely. These methods enable remote access over TCP and are not documented here, please consult your network administrator for assistance in deciding how to configure access.</para>
        <para>
          <itemizedlist>
            <listitem>
              <para>SSH tunnel to libvirtd's UNIX socket</para></listitem>
            <listitem><para>libvirtd TCP socket, with GSSAPI/Kerberos for
              auth+data encryption</para></listitem>
            <listitem><para>libvirtd TCP socket, with TLS for encryption and x509 client certs for authentication</para></listitem>
            <listitem><para>libvirtd TCP socket, with TLS for encryption and Kerberos for authentication</para>
            </listitem>
          </itemizedlist>
        </para>
        <para>Restart libvirt. After you run the command, ensure that
          libvirt is successfully restarted:</para>
        <screen><prompt>$</prompt> <userinput>stop libvirt-bin &amp;&amp; start libvirt-bin</userinput>
<prompt>$</prompt> <userinput>ps -ef | grep libvirt</userinput></screen>
        <screen><computeroutput>root 1145 1 0 Nov27 ? 00:00:03 /usr/sbin/libvirtd -d -l</computeroutput></screen>
      </listitem>
        <listitem>
        <para> Configure your firewall to allow libvirt to communicate between nodes.</para>
        <para>Information about ports used with libvirt can be found at <link xlink:href="http://libvirt.org/remote.html#Remote_libvirtd_configuration">the libvirt documentation</link>
        By default, libvirt listens on TCP port 16509 and an ephemeral TCP range from 49152 to
        49261 is used for the KVM communications. As this guide has disabled libvirt auth, you
        should take good care that these ports are only open to hosts within your installation.
        </para>
        </listitem>
        <listitem>
          <para>You can now configure options for live migration. In
            most cases, you do not need to configure any options. The
            following chart is for advanced usage only.</para>
        </listitem>
      </orderedlist></para>
    <xi:include href="../common/tables/nova-livemigration.xml"/>

    <section xml:id="true-live-migration-kvm-libvirt">
      <title>Enabling true live migration</title>
      <para>By default, the Compute service does not use libvirt's live migration functionality. To
        enable this functionality, add the following line to <filename>nova.conf</filename>:
        <programlisting>live_migration_flag=VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE</programlisting>The
        Compute service does not use libvirt's live miration by default because there is a risk that
        the migration process will never terminate. This can happen if the guest operating system
        dirties blocks on the disk faster than they can migrated. </para>
    </section>
    </section>

    <section xml:id="configuring-migrations-xenserver">
    <title>XenServer</title>

    <section xml:id="configuring-migrations-xenserver-shared-storage">
    <title>Shared Storage</title>
      <para><emphasis role="bold">Prerequisites</emphasis>
        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Compatible XenServer hypervisors.</emphasis> For more information,
            please refer to the <link xlink:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#pooling_homogeneity_requirements">Requirements for Creating Resource Pools</link>
            section of the XenServer Administrator's Guide.
            </para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Shared storage:</emphasis> an NFS export,
            visible to all XenServer hosts.
            <note>
              <para>Please check the <link xlink:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701">NFS VHD</link>
              section of the XenServer Administrator's Guide for the supported
              NFS versions.
              </para>
            </note>
            </para>
          </listitem>
        </itemizedlist>
      </para>

      <para>
      In order to use shared storage live migration with XenServer hypervisors,
      the hosts must be joined to a XenServer pool. In order to create that pool,
      a host aggregate must be created with special metadata. This metadata will be
      used by the XAPI plugins to establish the pool.
      </para>

      <orderedlist>
        <listitem>
          <para>
          Add an NFS VHD storage to your master XenServer, and set it as default SR. For more information, please refer to the
          <link xlink:href="http://docs.vmd.citrix.com/XenServer/6.0.0/1.0/en_gb/reference.html#id1002701">NFS VHD</link> section of the XenServer Administrator's Guide.
          </para>
        </listitem>

        <listitem>
          <para>
          Configure all the compute nodes to use the default sr for pool operations, by including:
          <programlisting>sr_matching_filter=default-sr:true</programlisting>
          in your <filename>nova.conf</filename> configuration files across your compute nodes.
          </para>
        </listitem>

        <listitem>
          <para>
          Create a host aggregate
          <programlisting>$ nova aggregate-create &lt;name-for-pool&gt; &lt;availability-zone&gt;</programlisting>
          The command will display a table which contains the id of the newly created aggregate.
          Now add special metadata to the aggregate, to mark it as a hypervisor pool
          <programlisting>$ nova aggregate-set-metadata &lt;aggregate-id&gt; hypervisor_pool=true
$ nova aggregate-set-metadata &lt;aggregate-id&gt; operational_state=created</programlisting>
          Make the first compute node part of that aggregate
          <programlisting>$ nova aggregate-add-host &lt;aggregate-id&gt; &lt;name-of-master-compute&gt;</programlisting>
          At this point, the host is part of a XenServer pool.
          </para>
        </listitem>

        <listitem>
          <para>
          Add additional hosts to the pool:
          <programlisting>$ nova aggregate-add-host &lt;aggregate-id&gt; &lt;compute-host-name&gt;</programlisting>
          <note>
            <para>At this point the added compute node and the host will be shut down, in order to
            join the host to the XenServer pool. The operation will fail, if any server other than the
            compute node is running/suspended on your host.</para>
          </note>
          </para>
        </listitem>
      </orderedlist>

    </section> <!-- End of Shared Storage -->

    <section xml:id="configuring-migrations-xenserver-block-migration">
    <title>Block migration</title>
      <para><emphasis role="bold">Prerequisites</emphasis>
        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Compatible XenServer hypervisors.</emphasis> The hypervisors must support the Storage XenMotion feature. Please refer
            to the manual of your XenServer to make sure your edition has this feature.
            </para>
          </listitem>
        </itemizedlist>
        <note>
          <para>Please note, that you need to use an extra option <literal>--block-migrate</literal> for the live migration
          command, in order to use block migration.</para>
        </note>
        <note>
          <para>Please note, that block migration works only with EXT local storage SRs,
          and the server should not have any volumes attached.</para>
        </note>
      </para>
    </section> <!-- End of Block migration -->

    </section> <!-- End of XenServer/Migration -->
  </section> <!-- End of configuring migrations -->
  <section xml:id="configuring-resize">
    <?dbhtml stop-chunking?>
    <title>Configuring Resize</title>
    <para>Resize (or Server resize) is the ability to change the
    flavor of a server, thus allowing it to upscale or downscale
    according to user needs. In order for this feature
    to work properly, some underlying virt layers may need further
    configuration; this section describes the required configuration
    steps for each hypervisor layer provided by OpenStack.</para>
    <section xml:id="xenserver-resize">
        <title>XenServer</title>
            <para>To get resize to work with XenServer (and XCP), please refer
            to the <link linkend="xapi-resize-setup">Dom0 Modifications for
            Resize/Migration Support</link> section.</para>
    </section> <!-- End of XenServer/Resize -->
  </section> <!-- End of configuring resize -->
  <section xml:id="installing-moosefs-as-backend">
    <title>Installing MooseFS as shared storage for the instances directory</title>
    <para> In the previous section we presented a convenient way to deploy a shared storage using
      NFS. For better transactions performance, you could deploy MooseFS instead. </para>
    <para>MooseFS (Moose File System) is a shared file system ; it implements the same rough
      concepts of shared storage solutions - such as Ceph, Lustre or even GlusterFS. </para>
    <para>
      <emphasis role="bold">Main concepts </emphasis>
      <itemizedlist>
        <listitem>
          <para> A metadata server (MDS), also called master server, which manages the file
            repartition, their access and the namespace.</para>
        </listitem>
        <listitem>
          <para>A metalogger server (MLS) which backs up the MDS logs, including, objects, chunks,
            sessions and object metadata</para>
        </listitem>
        <listitem>
          <para>A chunk server (CSS) which store the data as chunks
            and replicate them across the chunkservers</para>
        </listitem>
        <listitem>
          <para>A client, which talks with the MDS and interact with the CSS. MooseFS clients manage
            MooseFS filesystem using FUSE</para>
        </listitem>
      </itemizedlist> For more informations, please see the <link
        xlink:href="http://www.moosefs.org/">Official project website</link>
    </para>
    <para>Our setup will be made the following way : </para>
    <para>
      <itemizedlist>
        <listitem>
          <para> Two compute nodes running both MooseFS chunkserver and client services. </para>
        </listitem>
        <listitem>
          <para> One MooseFS master server, running the metadata service. </para>
        </listitem>
        <listitem>
          <para> One MooseFS slave server, running the metalogger service. </para>
        </listitem>
      </itemizedlist> For that particular walkthrough, we will use the following network schema : </para>
    <para>
      <itemizedlist>
        <listitem>
            <para><literal>10.0.10.15</literal> for the MooseFS metadata server admin IP</para>
        </listitem>
        <listitem>
            <para><literal>10.0.10.16</literal> for the MooseFS metadata server main IP</para>
        </listitem>
        <listitem>
            <para><literal>10.0.10.17</literal> for the MooseFS metalogger server admin IP</para>
        </listitem>
        <listitem>
            <para><literal>10.0.10.18</literal> for the MooseFS metalogger server main IP</para>
        </listitem>
        <listitem>
            <para><literal>10.0.10.19</literal> for the MooseFS first chunkserver IP</para>
        </listitem>
        <listitem>
            <para><literal>10.0.10.20</literal> for the MooseFS second chunkserver IP</para>
        </listitem>
      </itemizedlist>
      <figure xml:id="moose-FS-deployment">
        <title>MooseFS deployment for OpenStack</title>
        <mediaobject>
          <imageobject>
            <imagedata fileref="figures/moosefs/SCH_5008_V00_NUAC-MooseFS_OpenStack.png" scale="60"
            />
          </imageobject>
        </mediaobject>
      </figure>
    </para>
    <section xml:id="installing-moosefs-metadata-metalogger-servers">
      <title> Installing the MooseFS metadata and metalogger servers</title>
      <para>Both components could be run anywhere , as long as the MooseFS chunkservers can reach
        the MooseFS master server. </para>
      <para>In our deployment, both MooseFS master and slave run their services inside a virtual
        machine ; you just need to make sure to allocate enough memory to the MooseFS metadata
        server, all the metadata being stored in RAM when the service runs. </para>
      <para>
        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Hosts entry configuration</emphasis></para>
            <para>In the <filename>/etc/hosts</filename> add the following entry :
              <programlisting>
10.0.10.16   mfsmaster
        </programlisting></para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Required packages</emphasis></para>
            <para>Install the required packages by running the following commands :
              <screen os="ubuntu"><prompt>$</prompt> <userinput>apt-get install zlib1g-dev python pkg-config</userinput> </screen>
              <screen os="rhel;fedora;centos"><prompt>$</prompt> <userinput>yum install make automake gcc gcc-c++ kernel-devel python26 pkg-config</userinput></screen>
            </para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">User and group creation</emphasis></para>
            <para> Create the adequate user and group :
              <screen><prompt>$</prompt> <userinput>groupadd mfs &amp;&amp; useradd -g mfs mfs </userinput></screen>
            </para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Download the sources</emphasis></para>
            <para> Go to the <link xlink:href="http://www.moosefs.org/download.html">MooseFS download page</link>
              and fill the download form in order to obtain your URL for the package.
            </para>
            <para/>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Extract and configure the sources</emphasis></para>
            <para>Extract the package and compile it :
              <screen><prompt>$</prompt> <userinput>tar -zxvf mfs-1.6.25.tar.gz &amp;&amp; cd mfs-1.6.25 </userinput></screen>
              For the MooseFS master server installation, we disable from the compilation the
              mfschunkserver and mfsmount components :
              <screen><prompt>$</prompt> <userinput>./configure --prefix=/usr --sysconfdir=/etc/moosefs --localstatedir=/var/lib --with-default-user=mfs --with-default-group=mfs --disable-mfschunkserver --disable-mfsmount</userinput></screen><screen><prompt>$</prompt> <userinput>make &amp;&amp; make install</userinput></screen>
            </para>
          </listitem>        
          <listitem>
            <para><emphasis role="bold">Create configuration files</emphasis></para>
            <para> We will keep the default settings, for tuning performance, you can read the <link
              xlink:href="http://www.moosefs.org/moosefs-faq.html">MooseFS official FAQ</link>
            </para>
            <para><screen><prompt>$</prompt> <userinput>cd /etc/moosefs</userinput></screen>
              <screen><prompt>$</prompt> <userinput>cp mfsmaster.cfg.dist mfsmaster.cfg </userinput></screen>
              <screen><prompt>$</prompt> <userinput>cp mfsmetalogger.cfg.dist mfsmetalogger.cfg </userinput></screen>
              <screen><prompt>$</prompt> <userinput>cp mfsexports.cfg.dist mfsexports.cfg </userinput></screen>
              In <filename>/etc/moosefs/mfsexports.cfg</filename> edit the second line in order to
              restrict the access to our private network : </para>
            <programlisting>
10.0.10.0/24          /       rw,alldirs,maproot=0
            </programlisting>
            <para>
              Create the metadata file :
              <screen><prompt>$</prompt> <userinput>cd /var/lib/mfs &amp;&amp; cp metadata.mfs.empty metadata.mfs</userinput></screen>
            </para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Power up the MooseFS mfsmaster service</emphasis></para>
            <para>You can now start the <literal>mfsmaster</literal> and <literal>mfscgiserv</literal> deamons on the MooseFS
                metadataserver (The <literal>mfscgiserv</literal> is a webserver which allows you to see via a
                web interface the MooseFS status realtime) :
                <screen><prompt>$</prompt> <userinput>/usr/sbin/mfsmaster start &amp;&amp; /usr/sbin/mfscgiserv start</userinput></screen>Open
                the following url in your browser : http://10.0.10.16:9425 to see the MooseFS status
                page</para>
            <para/>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Power up the MooseFS metalogger service</emphasis></para>
            <para>          
              <screen><prompt>$</prompt> <userinput>/usr/sbin/mfsmetalogger start</userinput></screen>
            </para>
          </listitem>
        </orderedlist>
      </para>
      <para/>
    </section>
    <section xml:id="installing-moosefs-chunk-client-services">
      <title>Installing the MooseFS chunk and client services</title>
      <para> In the first part, we will install the last version of FUSE, and proceed to the
        installation of the MooseFS chunk and client in the second part. </para>
      <para/>
      <para><emphasis role="bold">Installing FUSE</emphasis></para>
      <para>
        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Required package</emphasis></para>
            <para>
              <screen os="ubuntu"><prompt>$</prompt> <userinput>apt-get install util-linux</userinput> </screen>
              <screen os="rhel;fedora;centos"><prompt>$</prompt> <userinput>yum install util-linux</userinput></screen></para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Download the sources and configure them</emphasis></para>
            <para> For that setup we will retrieve the last version of fuse to make sure every
              function will be available :
              <screen><prompt>$</prompt> <userinput>wget http://downloads.sourceforge.net/project/fuse/fuse-2.X/2.9.1/fuse-2.9.1.tar.gz &amp;&amp; tar -zxvf fuse-2.9.1.tar.gz &amp;&amp; cd fuse-2.9.1</userinput></screen><screen><prompt>$</prompt> <userinput>./configure &amp;&amp; make &amp;&amp; make install</userinput></screen>
            </para>
          </listitem>
        </orderedlist>
      </para>
      <para><emphasis role="bold">Installing the MooseFS chunk and client services</emphasis></para>
      <para> For installing both services, you can follow the same steps that were presented before
        (Steps 1 to 4) : <orderedlist>
          <listitem>
            <para> Hosts entry configuration</para>
          </listitem>
          <listitem>
            <para>Required packages</para>
          </listitem>
          <listitem>
            <para>User and group creation</para>
          </listitem>
          <listitem>
            <para>Download the sources</para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Extract and configure the sources</emphasis></para>
            <para>Extract the package and compile it :
              <screen><prompt>$</prompt> <userinput>tar -zxvf mfs-1.6.25.tar.gz &amp;&amp; cd mfs-1.6.25 </userinput></screen>
              For the MooseFS chunk server installation, we only disable from the compilation the
              mfsmaster component :
              <screen><prompt>$</prompt> <userinput>./configure --prefix=/usr --sysconfdir=/etc/moosefs --localstatedir=/var/lib --with-default-user=mfs --with-default-group=mfs --disable-mfsmaster</userinput></screen><screen><prompt>$</prompt> <userinput>make &amp;&amp; make install</userinput></screen>
            </para>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Create configuration files</emphasis></para>
            <para> The chunk servers configuration is relatively easy to setup. You only need to
              create on every server directories that will be used for storing the datas of your
              cluster.</para>
            <para><screen><prompt>$</prompt> <userinput>cd /etc/moosefs</userinput></screen>
              <screen><prompt>$</prompt> <userinput>cp  mfschunkserver.cfg.dist mfschunkserver.cfg</userinput></screen>
              <screen><prompt>$</prompt> <userinput>cp  mfshdd.cfg.dist mfshdd.cfg</userinput></screen>
              <screen><prompt>$</prompt> <userinput>mkdir /mnt/mfschunks{1,2} &amp;&amp; chown -R mfs:mfs /mnt/mfschunks{1,2}</userinput></screen>
              Edit <filename>/etc/moosefs/mfhdd.cfg</filename> and add the directories you created
              to make them part of the cluster : </para>
            <programlisting>
# mount points of HDD drives
#
#/mnt/hd1
#/mnt/hd2
#etc.

/mnt/mfschunks1
/mnt/mfschunks2
            </programlisting>
          </listitem>
          <listitem>
            <para><emphasis role="bold">Power up the MooseFS mfschunkserver service</emphasis></para>
            <para>          
              <screen><prompt>$</prompt> <userinput>/usr/sbin/mfschunkserver start</userinput></screen>
            </para>
          </listitem>
        </orderedlist>
      </para>
    </section>
    <section xml:id="access-to-cluster-storage">
      <title>Access to your cluster storage</title>
      <para> You can now access your cluster space from the compute node, (both acting as
        chunkservers) : <screen><prompt>$</prompt> <userinput>mfsmount /var/lib/nova/instances -H mfsmaster</userinput></screen>
        <computeroutput> mfsmaster accepted connection with parameters: read-write,restricted_ip ;
          root mapped to root:root </computeroutput>
        <screen><prompt>$</prompt> <userinput>mount</userinput></screen><programlisting>
/dev/cciss/c0d0p1 on / type ext4 (rw,errors=remount-ro)
proc on /proc type proc (rw,noexec,nosuid,nodev)
none on /sys type sysfs (rw,noexec,nosuid,nodev)
fusectl on /sys/fs/fuse/connections type fusectl (rw)
none on /sys/kernel/debug type debugfs (rw)
none on /sys/kernel/security type securityfs (rw)
none on /dev type devtmpfs (rw,mode=0755)
none on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)
none on /dev/shm type tmpfs (rw,nosuid,nodev)
none on /var/run type tmpfs (rw,nosuid,mode=0755)
none on /var/lock type tmpfs (rw,noexec,nosuid,nodev)
none on /var/lib/ureadahead/debugfs type debugfs (rw,relatime)
<emphasis role="bold">mfsmaster:9421 on /var/lib/nova/instances type fuse.mfs (rw,allow_other,default_permissions)</emphasis>
        </programlisting>You
        can interact with it the way you would interact with a classical mount, using build-in linux
        commands (cp, rm, etc...).
      </para>
      <para> The MooseFS client has several tools for managing the objects within the cluster (set
        replication goals, etc..). You can see the list of the available tools by running
        <screen><prompt>$</prompt> <userinput>mfs &lt;TAB&gt; &lt;TAB&gt;</userinput> </screen><programlisting>
mfsappendchunks   mfschunkserver    mfsfileinfo       mfsgetgoal        mfsmount          mfsrsetgoal       mfssetgoal        mfstools
mfscgiserv        mfsdeleattr       mfsfilerepair     mfsgettrashtime   mfsrgetgoal       mfsrsettrashtime  mfssettrashtime   
mfscheckfile      mfsdirinfo        mfsgeteattr       mfsmakesnapshot   mfsrgettrashtime  mfsseteattr       mfssnapshot           
        </programlisting>You
        can read the manual for every command. You can also see the <link xlink:href="http://linux.die.net/man/1/mfsrgetgoal">online help</link>
      </para>
      <para><emphasis role="bold">Add an entry into the fstab file</emphasis></para>
      <para>
        In order to make sure to have the storage mounted, you can add an entry into the <filename>/etc/fstab</filename> on both compute nodes : 
        <programlisting>
mfsmount 	/var/lib/nova/instances fuse mfsmaster=mfsmaster,_netdev 	0 	0
        </programlisting>
      </para>
    </section>
  </section>
  <section xml:id="configuring-database-connections">
    <title>Configuring Database Connections</title>

    <para>You can configure OpenStack Compute to use any SQLAlchemy-compatible
        database. The database name is <literal>nova</literal> and entries to it are mostly written
        by the <literal>nova-scheduler</literal> service, although all the services need to be able
        to update entries in the database. Use these settings to configure the
        connection string for the nova database.</para>
        <xi:include href="../common/tables/nova-db.xml"/>
  </section>

  <!-- Oslo rpc mechanism (i.e. Rabbit, Qpid, ZeroMQ) -->
  <xi:include href="../common/rpc.xml" />

  <section xml:id="configuring-compute-API">
  <title>Configuring the Compute API</title>
  <simplesect>
    <title>Configuring Compute API password handling</title>

    <para> The OpenStack Compute API allows the user to specify an admin
    password when creating (or rebuilding) a server instance.  If no
    password is specified, a randomly generated password is used. The
    password is returned in the API response.</para>

    <para>In practice, the handling of the admin password depends on the
    hypervisor in use, and may require additional configuration of the
    instance, such as installing an agent to handle the password setting.
    If the hypervisor and instance configuration do not support the
    setting of a password at server create time, then the password
    returned by the create API call will be misleading, since it was
    ignored.
    </para>

    <para>To prevent this confusion, the configuration option
          <literal>enable_instance_password</literal> can be used to
        disable the return of the admin password for installations
        that don't support setting instance passwords.</para>

    <table rules="all">
      <caption>Description of nova.conf API related configuration
          options</caption>

      <thead>
        <tr>
          <td>Configuration option</td>

          <td>Default</td>

          <td>Description</td>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td><literal>enable_instance_password</literal></td>

          <td><literal>true</literal></td>

          <td>When true, the create and rebuild compute API calls return the server admin password.  When false,
          the server admin password is not included in API responses.</td>
        </tr>

      </tbody>
    </table>

  </simplesect>

  <simplesect>
    <title>Configuring Compute API Rate Limiting</title>

    <para>OpenStack Compute supports API rate limiting for the OpenStack API.
    The rate limiting allows an administrator to configure limits on the type
    and number of API calls that can be made in a specific time
    interval.</para>

    <para>When API rate limits are exceeded, HTTP requests will return a error
    with a status code of 413 "Request entity too large", and will also
    include a 'Retry-After' HTTP header. The response body will include the
    error details, and the delay before the request should be retried.</para>

    <para>Rate limiting is not available for the EC2 API.</para>
  </simplesect>
    <simplesect>
      <title>Specifying Limits</title>

      <para>Limits are specified using five values:</para>

      <itemizedlist>
        <listitem>
          <para>The <emphasis role="bold">HTTP method</emphasis> used in the
          API call, typically one of GET, PUT, POST, or DELETE.</para>
        </listitem>

        <listitem>
          <para>A <emphasis role="bold">human readable URI</emphasis> that is
          used as a friendly description of where the limit is applied.</para>
        </listitem>

        <listitem>
          <para>A <emphasis role="bold">regular expression</emphasis>. The
          limit will be applied to all URI's that match the regular expression
          and HTTP Method.</para>
        </listitem>

        <listitem>
          <para>A <emphasis role="bold">limit value </emphasis> that specifies
          the maximum count of units before the limit takes effect.</para>
        </listitem>

        <listitem>
          <para>An <emphasis role="bold">interval</emphasis> that specifies
          time frame the limit is applied to. The interval can be SECOND,
          MINUTE, HOUR, or DAY.</para>
        </listitem>
      </itemizedlist>

      <para>Rate limits are applied in order, relative to the HTTP method,
      going from least to most specific. For example, although the default
      threshold for POST to */servers is 50 per day, one cannot POST to
      */servers more than 10 times within a single minute because the rate
      limits for any POST is 10/min.</para>
    </simplesect>

    <simplesect>
      <title>Default Limits</title>

      <para>OpenStack compute is normally installed with the following limits
      enabled:</para>

      <table rules="all">
        <caption>Default API Rate Limits</caption>

        <thead>
          <tr>
            <td>HTTP method</td>

            <td>API URI</td>

            <td>API regular expression</td>

            <td>Limit</td>
          </tr>
        </thead>

        <tbody>
          <tr>
            <td>POST</td>

            <td>any URI (*)</td>

            <td>.*</td>

            <td>10 per minute</td>
          </tr>

          <tr>
            <td>POST</td>

            <td>/servers</td>

            <td>^/servers</td>

            <td>50 per day</td>
          </tr>

          <tr>
            <td>PUT</td>

            <td>any URI (*)</td>

            <td>.*</td>

            <td>10 per minute</td>
          </tr>

          <tr>
            <td>GET</td>

            <td>*changes-since*</td>

            <td>.*changes-since.*</td>

            <td>3 per minute</td>
          </tr>

          <tr>
            <td>DELETE</td>

            <td>any URI (*)</td>

            <td>.*</td>

            <td>100 per minute</td>
          </tr>
        </tbody>
      </table>
    </simplesect>

    <simplesect>
      <title>Configuring and Changing Limits</title>

      <para>The actual limits are specified in the file
      <filename>etc/nova/api-paste.ini</filename>, as part of the WSGI
      pipeline.</para>

      <para>To enable limits, ensure the '<literal>ratelimit</literal>' filter
      is included in the API pipeline specification. If the
      '<literal>ratelimit</literal>' filter is removed from the pipeline,
      limiting will be disabled. There should also be a definition for the
      ratelimit filter. The lines will appear as follows:</para>

      <programlisting language="bash">
[pipeline:openstack_compute_api_v2]
pipeline = faultwrap authtoken keystonecontext ratelimit osapi_compute_app_v2

[pipeline:openstack_volume_api_v1]
pipeline = faultwrap authtoken keystonecontext ratelimit osapi_volume_app_v1

[filter:ratelimit]
paste.filter_factory = nova.api.openstack.compute.limits:RateLimitingMiddleware.factory
            </programlisting>

      <para>To modify the limits, add a '<literal>limits</literal>'
      specification to the <literal>[filter:ratelimit]</literal> section of
      the file. The limits are specified in the order HTTP method, friendly
      URI, regex, limit, and interval. The following example specifies the
      default rate limiting values:</para>

      <programlisting language="bash">
[filter:ratelimit]
paste.filter_factory = nova.api.openstack.compute.limits:RateLimitingMiddleware.factory
limits =(POST, "*", .*, 10, MINUTE);(POST, "*/servers", ^/servers, 50, DAY);(PUT, "*", .*, 10, MINUTE);(GET, "*changes-since*", .*changes-since.*, 3, MINUTE);(DELETE, "*", .*, 100, MINUTE)
            </programlisting>
    </simplesect>
  </section>

  <section xml:id="configuring-ec2-API">
  <title>Configuring the EC2 API</title>

  <para>You can use <filename>nova.conf</filename> configuration
  options to control which network address and port the EC2 API will
  listen on, the formatting of some API responses, and authentication
  related options.</para>

  <para>To customize these options for OpenStack EC2 API, use these
  configuration option settings.</para>

  <xi:include href="../common/tables/nova-ec2.xml" />

  </section>
   <section xml:id="configuring-quotas">
        <title>Configuring Quotas</title>

        <para>For tenants, quota controls are available to limit the (flag and default shown in
parenthesis):<itemizedlist>
                <listitem>
                    <para>Number of volumes which may be created (volumes=10)</para>
                </listitem>
                <listitem>
                    <para>Total size of all volumes within a project as measured in GB (gigabytes=1000)</para>
                </listitem>
                <listitem>
                    <para>Number of instances which may be launched (instances=10)</para>
                </listitem>
                <listitem>
                    <para>Number of processor cores which may be allocated (cores=20)</para>
                </listitem>
                <listitem>
                    <para>Publicly accessible IP addresses for
            persistence in DNS assignment (floating_ips=10)</para>
                </listitem>
                <listitem>
                    <para>Privately (or publicly) accessible IP
            addresses for management purposes (fixed_ips=-1
            unlimited)</para>
                </listitem>
                <listitem>
                    <para>Amount of RAM that can be allocated in MB (ram=512000)</para>
                </listitem>
                <listitem>
                    <para>Number of files that can be injected (injected_files=5)</para>
                </listitem>
                 <listitem>
                    <para>Maximal size of injected files in B (injected_file_content_bytes=10240)</para>
                </listitem> 
                <listitem>
                    <para>Number of security groups that may be created (security_groups=10)</para>
                </listitem> 
                 <listitem>
                    <para>Number of rules per security group (security_group_rules=20)</para>
                </listitem>
        </itemizedlist></para>
        <para> The defaults may be modified by setting the variable in
         <literal>nova.conf</literal>, then restarting the <literal>nova-api</literal>
         service.</para>
        <para>To modify a value for a specific project, the <command>nova-manage</command>
         command should be used. For example:
        <screen><userinput><prompt>$</prompt> nova-manage project quota --project=1113f5f266f3477ac03da4e4f82d0568 --key=cores --value=40</userinput></screen>
        Alternately, quota settings are available through the OpenStack Dashboard in the "Edit
        Project" page. </para>
        <xi:include href="../common/tables/nova-quota.xml"/>
  </section>
</chapter>
