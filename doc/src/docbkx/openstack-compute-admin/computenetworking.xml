<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_networking">
    <title>Networking</title>
    <para>By understanding the available networking configuration
        options you can design the best configuration for your
        OpenStack Compute instances.</para>

    <section xml:id="networking-options">
        <title>Networking Options</title>
        <para>This section offers a brief overview of each concept in
            networking for Compute. </para>
        <para>In Compute, users organize their cloud resources in projects. A Compute project
            consists of a number of VM instances created by a user. For each VM instance, Compute
            assigns to it a private IP address. (Currently, Compute only supports Linux bridge
            networking that allows the virtual interfaces to connect to the outside network through
            the physical interface.)</para>
        <para>The Network Controller provides virtual networks to
            enable compute servers to interact with each other and
            with the public network.</para>
        <para>Currently, Compute supports three kinds of networks, implemented in three “Network
            Manager” types:<itemizedlist>
                <listitem>
                    <para>Flat Network Manager</para>
                </listitem>
                <listitem>
                    <para>Flat DHCP Network Manager</para>
                </listitem>
                <listitem>
                    <para>VLAN Network Manager</para>
                </listitem>
            </itemizedlist></para> <para>The three kinds of networks can co-exist in a cloud system. However,
            since you can't yet select the type of network for a given project, you cannot configure
            more than one type of network in a given Compute installation.</para>
        <para>Compute makes a distinction between <emphasis role="italic">fixed IPs</emphasis> and
                <emphasis role="italic">floating IPs</emphasis> for VM instances. Fixed IPs are IP
            addresses that are assigned to an instance on creation and stay the same until the
            instance is explicitly terminated. By contrast, floating IPs are addresses that can be
            dynamically associated with an instance. A floating IP address can be disassociated and
            associated with another instance at any time. A user can reserve a floating IP for their
            project. </para>
        <para>In Flat Mode, a network administrator specifies a subnet. The IP addresses for VM
            instances are grabbed from the subnet, and then injected into the image on launch. Each
            instance receives a fixed IP address from the pool of available addresses. A network
            administrator must configure the Linux networking bridge (typically named
                <literal>br100</literal>, although this configurable) both on the network controller
            hosting the network and on the cloud controllers hosting the instances. All instances of
            the system are attached to the same bridge, configured manually by the network
            administrator.</para>
        <para>
            <note>
                <para>The configuration injection currently only works on Linux-style systems that
                    keep networking configuration in
                    <filename>/etc/network/interfaces</filename>.</para>
            </note>
        </para>
        <para>In Flat DHCP Mode, you start a DHCP server to pass out
            IP addresses to VM instances from the specified subnet in
            addition to manually configuring the networking bridge. IP
            addresses for VM instances are grabbed from a subnet
            specified by the network administrator. </para>
        <para>Like Flat Mode, all instances are attached to a single
            bridge on the compute node. In addition a DHCP server is
            running to configure instances. In this mode, Compute does
            a bit more configuration in that it attempts to bridge
            into an ethernet device (eth0 by default). It will also
            run dnsmasq as a dhcpserver listening on this bridge.
            Instances receive their fixed IPs by doing a dhcpdiscover. </para>
        <para>In both flat modes, the network nodes do not act as a
            default gateway. Instances are given public IP addresses.
            Compute nodes have iptables/ebtables entries created per
            project and instance to protect against IP/MAC address
            spoofing and ARP poisoning. </para>
        <para>VLAN Network Mode is the default mode for OpenStack
            Compute. In this mode, Compute creates a VLAN and bridge
            for each project. For multiple machine installation, the
            VLAN Network Mode requires a switch that supports VLAN
            tagging (IEEE 802.1Q). The project gets a range of private
            IPs that are only accessible from inside the VLAN. In
            order for a user to access the instances in their project,
            a special VPN instance (code named cloudpipe) needs to be
            created. Compute generates a certificate and key for the
            user to access the VPN and starts the VPN automatically.
            It provides a private network segment for each project's
            instances that can be accessed via a dedicated VPN
            connection from the Internet. In this mode, each project
            gets its own VLAN, Linux networking bridge, and subnet. </para>
        <para>The subnets are specified by the network administrator,
            and are assigned dynamically to a project when required. A
            DHCP Server is started for each VLAN to pass out IP
            addresses to VM instances from the subnet assigned to the
            project. All instances belonging to one project are
            bridged into the same VLAN for that project. OpenStack
            Compute creates the Linux networking bridges and VLANs
            when required.</para>
        <note>
            <para>The " --force_ dhcp_release" configuration option, when set, would keep you from
                restarting the dnsmasq processes manually and refresh the leases tables everytime an
                instance is terminated. </para>
            <para>That configuration applies no matter the networking mode used. </para>
        </note>
        <para/>
    </section>
    <section xml:id="configuring-networking-on-the-compute-node">
        <title>Configuring Networking on the Compute Node</title>
        <para>To configure the Compute node's networking for the VM
            images, the overall steps are:</para>

        <orderedlist>
            <listitem>
                <para>Set the "network-manager" option in
                    nova.conf.</para>
            </listitem>
            <listitem>
                <para>Use the <code>nova-manage network create label
                        CIDR n n</code> command to create the subnet
                    that the VMs reside on.</para>
            </listitem>
            <listitem>
                <para>Integrate the bridge with your network. </para>
            </listitem>
        </orderedlist>
        <para>By default, Compute uses the VLAN Network Mode. You
            choose the networking mode for your virtual instances in
            the nova.conf file. Here are the three possible options: </para>
        <itemizedlist>
            <listitem>
                <para>network_manager=nova.network.manager.FlatManager</para>
                <para>Simple, non-VLAN networking</para>
            </listitem>
            <listitem>
                <para>network_manager=nova.network.manager.FlatDHCPManager</para>
                <para>Flat networking with DHCP, you must set a bridge
                    using the flat_network_bridge option</para>
            </listitem>
            <listitem>
                <para>network_manager=nova.network.manager.VlanManager</para>
                <para>VLAN networking with DHCP. This is the Default
                    if no network manager is defined in nova.conf.
                </para>
            </listitem>
        </itemizedlist>
        <para>Also, when you issue the nova-manage network create
            command, it uses the settings from the nova.conf
            configuration options file. Use the following command to
            create the subnet that your VMs will run on :
            <literallayout class="monospaced">nova-manage network create private 192.168.0.0/24 1 256           </literallayout>
        </para>
        <section xml:id="configuring-flat-networking">
            <title>Configuring Flat Networking</title>
            <para>FlatNetworking uses ethernet adapters configured as
                bridges to allow network traffic to transit between
                all the various nodes. This setup can be done with a
                single adapter on the physical host, or multiple. This
                option does not require a switch that does VLAN
                tagging as VLAN networking does, and is a common
                development installation or proof of concept setup.
                When you choose Flat networking, Nova does not manage
                networking at all. Instead, IP addresses are injected
                into the instance via the file system (or passed in
                via a guest agent). Metadata forwarding must be
                configured manually on the gateway if it is required
                within your network. </para>
            <para>To configure flat networking, ensure that your
                nova.conf file contains the following line:</para>
            <para>
                <programlisting>
network_manager=nova.network.manager.FlatManager
                </programlisting>
            </para>
            <para>Compute defaults to a bridge device named ‘br100’
                which is stored in the Nova database, so you can
                change the name of the bridge device by modifying the
                entry in the database. Consult the diagrams for
                additional configuration options.</para>
            <para>In any set up with FlatNetworking (either Flat or
                FlatDHCP), the host with nova-network on it is
                responsible for forwarding traffic from the private
                network configured with the
                    <literal>--fixed_range=</literal> directive in
                nova.conf and the
                    <literal>--flat_network_bridge</literal> setting.
                This host needs to have br100 configured and talking
                to any other nodes that are hosting VMs. With either
                of the Flat Networking options, the default gateway
                for the virtual machines is set to the host which is
                running nova-network. </para>
            <note>
                <para>When configuring FlatDHCP, failing to enable
                        <literal>flat_injection</literal> can prevent
                    guest VMs from receiving their IP information at
                    boot time.</para>
            </note>
            <para>Set the compute node's external IP address to be on
                the bridge and add eth0 to that bridge. To do this,
                edit your network interfaces configuration to look
                like the following example: </para>
            <para>
                <programlisting>
# The loopback network interface
auto lo
iface lo inet loopback

# Networking for OpenStack Compute
auto br100

iface br100 inet dhcp
    bridge_ports        eth0
    bridge_stp           off
    bridge_maxwait   0
    bridge_fd            0
 </programlisting>
            </para>
            <para>Next, restart networking to apply the changes:
                    <code>sudo /etc/init.d/networking
                restart</code></para>
            <para>For an all-in-one development setup, this diagram
                represents the network setup.</para>

            <para><figure>
                    <title>Flat network, all-in-one server
                        installation </title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="80"
                                fileref="figures/FlatNetworkSingleInterfaceAllInOne.png"
                            />
                        </imageobject>
                    </mediaobject>
                </figure></para>
            <para>For multiple compute nodes with a single network
                adapter, which you can use for smoke testing or a
                proof of concept, this diagram represents the network
                setup.</para>
            <figure>
                <title>Flat network, single interface, multiple
                    servers</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/FlatNetworkSingleInterface.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <para>For multiple compute nodes with multiple network
                adapters, this diagram represents the network setup.
                You may want to use this setup for separate admin and
                data traffic.</para>
            <figure xml:id="flat-dhcp-diagram">
                <title>Flat network, multiple interfaces, multiple
                    servers</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/FlatNetworkMultInterface.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
        </section>
        <section xml:id="configuring-flat-dhcp-networking">
            <title>Configuring Flat DHCP Networking</title>
            <para>With Flat DHCP, the host running nova-network acts
                as the gateway to the virtual nodes. You can run one
                nova-network per cluster. Set the option
                    <literal>network_host</literal> on the
                    <filename>nova.conf</filename> stored on the
                nova-compute node to tell it which host the
                nova-network is running on so it can communicate with
                nova-network. In any set up with FlatNetworking, the
                host with nova-network on it is responsible for
                forwarding traffic from the private network configured
                with the <literal>fixed_range=</literal> directive in
                    <filename>nova.conf</filename> and the
                    <literal>flat_network_bridge</literal> flag which
                you must also set to the name of the bridge (as there
                is no default). The nova-network service will track
                leases and releases in the database so it knows if a
                VM instance has stopped properly configuring via DHCP.
                Lastly, it sets up iptables rules to allow the VMs to
                communicate with the outside world and contact a
                special metadata server to retrieve information from
                the cloud.</para>
            <para>Compute hosts in the FlatDHCP model are responsible
                for bringing up a matching bridge and bridging the VM
                tap devices into the same ethernet device that the
                network host is on. The compute hosts do not need an
                IP address on the VM network, because the bridging
                puts the VMs and the network host on the same logical
                network. When a VM boots, the VM sends out DHCP
                packets, and the DHCP server on the network host
                responds with their assigned IP address.</para>

            <para>Visually, the setup looks like the diagram
                below:</para>
            <figure>
                <title>Flat DHCP network, multiple interfaces,
                    multiple servers</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="50"
                            fileref="figures/flatdchp-net.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure>
            <para>FlatDHCP doesn't create VLANs, it creates a bridge.
                This bridge works just fine on a single host, but when
                there are multiple hosts, traffic needs a way to get
                out of the bridge onto a physical interface. </para>
            <para>Be careful when setting up
                    <literal>--flat_interface</literal>. If you
                specify an interface that already has an IP it will
                break and if this is the interface you are connecting
                through with SSH, you cannot fix it unless you have
                ipmi/console access. In FlatDHCP mode, the setting for
                    <literal>--network_size</literal> should be number
                of IPs in the entire fixed range. If you are doing a
                /12 in CIDR notation, then this number would be 2^20
                or 1,048,576 IP addresses. That said, it will take a
                very long time for you to create your initial network,
                as an entry for each IP will be created in the
                database. </para>
            <para>If you have an unused interface on your hosts that
                has connectivity with no IP address, you can simply
                tell FlatDHCP to bridge into the interface by
                specifying
                        <literal>flat_interface=<replaceable>&lt;interface></replaceable></literal>
                in your configuration file. The network host will
                automatically add the gateway ip to this bridge. You
                can also add the interface to br100 manually and not
                set flat_interface. If this is the case for you, edit
                your nova.conf file to contain the following lines: </para>
            <para>
                <programlisting>
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
network_manager=nova.network.manager.FlatDHCPManager
fixed_range=10.0.0.0/8
flat_network_dhcp_start=10.0.0.2
flat_network_bridge=br100
flat_interface=eth2
flat_injected=False
public_interface=eth0
                </programlisting>
            </para>
            <para>Integrate your network interfaces to match this
                configuration.</para>
        </section>
        <section
            xml:id="outbound-traffic-flow-with-any-flat-networking">
            <title>Outbound Traffic Flow with Any Flat
                Networking</title>
            <para> In any set up with FlatNetworking, the host with nova-network on it is
                responsible for forwarding traffic from the private network configured with the
                    <literal>fixed_range=...</literal> directive in <filename>nova.conf</filename>.
                This host needs to have a bridge interface (e.g., <literal>br100</literal>) configured and talking to any other
                nodes that are hosting VMs. With either of the Flat Networking options, the default
                gateway for the virtual machines is set to the host which is running nova-network. </para>
            <para> When a virtual machine sends traffic out to the
                public networks, it sends it first to its default
                gateway, which is where nova-network is configured. </para>
            <figure>
                <title>Single adaptor hosts, first route</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/SingleInterfaceOutbound_1.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Next, the host on which nova-network is configured
                acts as a router and forwards the traffic out to the
                Internet.</para>
            <figure>
                <title>Single adaptor hosts, second route</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/SingleInterfaceOutbound_2.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <warning>
                <para>If you're using a single interface, then that
                    interface (often eth0) needs to be set into
                    promiscuous mode for the forwarding to happen
                    correctly. This does not appear to be needed if
                    you're running with physical hosts that have and
                    use two interfaces.</para>
            </warning>
        </section>
        <section xml:id="configuring-vlan-networking">
            <?dbhtml stop-chunking?>
            <title>Configuring VLAN Networking</title>
            <para>Compute can be configured so that the virtual machine instances of different
                projects (tenants) are in different subnets, with each subnet having a different
                VLAN tag. This can be useful in networking environments where you have a large IP
                space which is cut up into smaller subnets. The smaller subnets are then trunked
                together at the switch level (dividing layer 3 by layer 2) so that all machines in
                the larger IP space can communicate. The purpose of this is generally to control the
                size of broadcast domains. It can also be useful to provide an additional layer of
                isolation in a multi-tenant environment.</para>
            <note>
                <para>The terms <emphasis role="italic">network</emphasis> and <emphasis
                    role="italic">subnet</emphasis> are often used interchangeably in
                    discussions of VLAN mode. In all cases, we are referring to a range of IP
                    addresses specified by a <emphasis role="italic">subnet </emphasis>(e.g.,
                    <literal>172.16.20.0/24</literal>) that are on the same VLAN (layer 2
                    <emphasis role="italic">network</emphasis>). </para>
            </note>
            <para>Running in VLAN mode is more complex than the other network modes. In particular:<itemizedlist>
                    <listitem>
                        <para>IP forwarding must be enabled</para>
                    </listitem>
                <listitem>
                    <para>The hosts running nova-network and nova-compute must have the
                                <literal>8021q</literal> kernel module loaded</para>
                </listitem>
                    <listitem>
                        <para>Your networking switches must support VLAN tagging</para>
                    </listitem>
                    <listitem>
                        <para>Your networking switches must be configured to enable the specific
                            VLAN tags you specify in your Compute setup</para>
                    </listitem>
                    <listitem>
                        <para>You will need information about your networking setup from your
                            network administrator to configure Compute properly (e.g., netmask,
                            broadcast, gateway, ethernet device, VLAN IDs)</para>
                    </listitem>
                </itemizedlist></para>
            <para>To configure your nodes to support VLAN tagging, install the
                    <literal>vlan</literal> package and load the <literal>8021q</literal> kernel
                module, as
                root:<screen><prompt>#</prompt> <userinput>apt-get install vlan</userinput>
<prompt>#</prompt> <userinput>modprobe 8021q </userinput></screen></para>
            <para>To have this kernel module loaded on boot, add the following line to
                    <filename>/etc/modules</filename>:<programlisting>8021q</programlisting></para>
            <para>Here is an example of settings from <filename>/etc/nova/nova.conf</filename> for a
                host configured to run <command>nova-network</command> in VLAN mode</para>

            <programlisting>network_manager=nova.network.manager.VlanManager
vlan_interface=eth0
fixed_range=172.16.0.0/12
network_size=256                </programlisting>
<para>The <literal>network_manager=nova.network.manager.VlanManager</literal> option specifies VLAN
                mode, which happens to be the default networking mode. </para>
            <para>The bridges that are created by the network manager will be attached to the
                interface specified by <literal>vlan_interface</literal>, the example above uses the
                    <literal>eth0</literal> interface, which is the default. </para>
            <para>The <literal>fixed_range</literal> option is a CIDR block which describes the IP
                address space for all of the instances: this space will be divided up into subnets.
                This range is typically a <link
                    xlink:href="https://en.wikipedia.org/wiki/Private_network">private
                    network</link>. The example above uses the private range
                    <literal>172.16.0.0/12</literal>.</para>
            <para>The <literal>network_size</literal> option refers to the default number of IP
                addresses in each network, although this can be overriden at network creation time .
                The example above uses a network size of <literal>256</literal>, whicih corresponds
                to a <literal>/24</literal> network.</para>
            <para>Networks are created with the <command>nova-manage network create</command>
                command. Here is an example of how to create a network consistent with the above
                example configuration options, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network create --label=example-net --fixed_range_v4=172.16.169.0/24 --vlan=169 --bridge=br169 --project_id=a421ae28356b4cc3a25e1429a0b02e98</userinput> </screen></para>
            <para>This creates a network called <literal>example-net</literal> associated with
                tenant <literal>a421ae28356b4cc3a25e1429a0b02e98</literal>. The subnet is
                    <literal>172.16.169.0/24</literal> with a VLAN tag of <literal>169</literal>
                (the VLAN tag does not need to match the third byte of the address, though it is a
                useful convention to remember the association). This will create a bridge interface
                device called <literal>br169</literal> on the host running the nova-network service.
                This device will appear in the output of an <command>ifconfig</command>
                command.</para>
            <para>Each network is associated with one tenant. As in the example above, you may
                (optionally) specify this association at network creation time by using the
                    <literal>--project_id</literal> flag which corresponds to the tenant ID. Use the
                    <command>keystone tenant-list</command> command to list the tenants and
                corresponding IDs that you have already created.</para>
            <para>Instead of manually specifying a VLAN, bridge, and project id, you can create many
                networks at once and have the Compute service automatically associate these networks
                with tenants as needed, as well as automatically generating the VLAN IDs and bridge
                interface names. For example, the following command would create 100 networks, from
                    <literal>172.16.100.0/24</literal> to <literal>172.16.199.0/24</literal>. (This
                assumes the <literal>network_size=256</literal> option has been set at nova.conf,
                though this can also be specified by passing <literal>--network_size=256</literal>
                as a flag to the <command>nova-manage</command>
                command)<screen><prompt>#</prompt> nova-manage network create --num_networks=100 --fixed_range_v4=172.16.100.0/24 </screen></para>
            <para>The <command>nova-manage network create</command> command supports many
                configuration options, which are displayed when called with the
                    <literal>--help</literal> flag:</para>
            <programlisting>Usage: nova-manage network create &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --label=&lt;label>       Label for network (ex: public)
  --fixed_range_v4=&lt;x.x.x.x/yy>
                        IPv4 subnet (ex: 10.0.0.0/8)
  --num_networks=&lt;number>
                        Number of networks to create
  --network_size=&lt;number>
                        Number of IPs per network
  --vlan=&lt;vlan id>      vlan id
  --vpn=VPN_START       vpn start
  --fixed_range_v6=FIXED_RANGE_V6
                        IPv6 subnet (ex: fe80::/64)
  --gateway=GATEWAY     gateway
  --gateway_v6=GATEWAY_V6
                        ipv6 gateway
  --bridge=&lt;bridge>     VIFs on this network are connected to this bridge
  --bridge_interface=&lt;bridge interface>
                        the bridge is connected to this interface
  --multi_host=&lt;'T'|'F'>
                        Multi host
  --dns1=&lt;DNS Address>  First DNS
  --dns2=&lt;DNS Address>  Second DNS
  --uuid=&lt;network uuid>
                        Network UUID
  --fixed_cidr=&lt;x.x.x.x/yy>
                        IPv4 subnet for fixed IPS (ex: 10.20.0.0/16)
  --project_id=&lt;project id>
                        Project id
  --priority=&lt;number>   Network interface priority
                    </programlisting>
            <para>In particular, flags to the <command>nova-mange network create</command> command
                can be used to override settings from <filename>nova.conf</filename>:<variablelist>
                    <varlistentry>
                        <term><literal>--network_size</literal></term>
                        <listitem>
                            <para>Overrides the <literal>network_size</literal> configuration
                                option</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term><literal>--bridge_interface</literal></term>
                        <listitem>
                            <para>Overrides the <literal>vlan_interface</literal> configuration
                                option</para>
                        </listitem>
                    </varlistentry>
                </variablelist></para>
            <para>To view a list of the networks that have been created, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network list</userinput></screen></para>
            <para>To modify an existing network, use the <command>nova-manage network
                    modify</command> command, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network modify --help</userinput>
<computeroutput>Usage: nova-manage network modify &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --fixed_range=&lt;x.x.x.x/yy>
                        Network to modify
  --project=&lt;project name>
                        Project name to associate
  --host=&lt;host>         Host to associate
  --disassociate-project
                        Disassociate Network from Project
  --disassociate-host   Disassociate Host from Project</computeroutput></screen></para>
            <para>To delete a network, use <command>nova-manage network delete</command>, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network delete --help</userinput><computeroutput>
Usage: nova-manage network delete &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --fixed_range=&lt;x.x.x.x/yy>
                        Network to delete
  --uuid=&lt;uuid>         UUID of network to delete</computeroutput></screen></para>
            <para>Note that a network must first be disassociated from a project using the
                    <command>nova-manage network modify</command> command before it can be
                deleted.</para>
            <para>Creating a network will automatically cause the Compute database to populate with
                a list of available fixed IP addresses. You can view the list of fixed IP addresses
                and their associations with active virtual machines by doing, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage fix list</userinput></screen></para>
            <para>
                <warning>
                    <para>Due to <link xlink:href="https://bugs.launchpad.net/nova/+bug/754900"
                            >Compute bug #754900</link>, deleting a network with the
                            <command>nova-manage network delete</command> command does not delete
                        the associated fixed IP addresses. As a workaround, these fixed IP addresses
                        can be deleted by connecting to the <literal>nova</literal> database and
                        issuing the SQL query (this example assumes the deleted network id is
                            <literal>1</literal><screen><userinput>DELETE from fixed_ips where network_id=1;</userinput></screen></para>
                </warning>
            </para>

            <para>In certain cases, the network manager may not properly tear down bridges and VLANs
                when it is stopped. If you attempt to restart the network manager and it does not
                start, check the logs for errors indicating that a bridge device already exists. If
                this is the case, you will likely need to tear down the bridge and VLAN devices
                manually. It is also advisable to kill any remaining dnsmasq processes. These
                commands would stop the service, manually tear down the bridge and VLAN from the
                previous example, kill any remaining dnsmasq processes, and start the service up
                again, as root:</para>

            <screen><prompt>#</prompt> <userinput>stop nova-network</userinput>
<prompt>#</prompt> <userinput>vconfig rem vlan169</userinput>
<prompt>#</prompt> <userinput>ip link set br169 down</userinput>
<prompt>#</prompt> <userinput>brctl delbr br169</userinput>
<prompt>#</prompt> <userinput>killall dnsmasq</userinput>
<prompt>#</prompt> <userinput>start nova-network</userinput></screen>

            <para>If users need to access the instances in their project across a VPN, a special VPN
                instance (code named cloudpipe) needs to be created as described in the section
                titled <link linkend="cloudpipe-per-project-vpns">Cloudpipe — Per Project
                    VPNs</link>. </para>
            <section xml:id="vlan-known-issues">
                <title>Known issue with failed DHCP leases in VLAN configuration</title>
                <para>Text in this section was adapted from <link
                        xlink:href="https://lists.launchpad.net/openstack/msg11696.html">an email
                        from Vish Ishaya on the OpenStack mailing list</link>.</para>
                <para>There is an issue with the way Compute uses <link
                        xlink:href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</link> in
                    VLAN mode. Compute starts up a single copy of dnsmasq for each VLAN on the
                    network host (or on every host in multi_host mode). <link
                        xlink:href="http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2011q3/005233.html"
                        >The problem</link> is in the way that dnsmasq binds to an IP  address and
                    port. Both copies can respond to broadcast packets, but unicast packets can only
                    be answered by one of the copies.</para>
                <para>As a consequence, guests from only one project will get responses to their
                    unicast DHCP renew requests. Unicast projects from guests in other projects get
                    ignored. What happens next is different depending on the guest OS. Linux
                    generally will send a broadcast packet out after the unicast fails, and so the
                    only effect is a small (tens of ms) hiccup while the interface is reconfigured.
                    It can be much worse than that, however. There have been observed cases where
                    Windows just gives up and ends up with a non-configured interface.</para>
                <para>This bug was first noticed by some users of OpenStack who rolled their own
                    fix. In short, on Linux, if you set the <literal>SO_BINDTODEVICE</literal>
                    socket option, it will allow different daemons to share the port and respond to
                    unicast packets, as long as they listen on different interfaces. Simon Kelley,
                    the maintainer of dnsmasq, <link
                        xlink:href="http://thekelleys.org.uk/gitweb/?p=dnsmasq.git;a=commitdiff;h=9380ba70d67db6b69f817d8e318de5ba1e990b12"
                        >has integrated a fix</link> for the issue in dnsmaq version 2.61. </para>
                <para>If upgrading dnsmasq is out of the question, a possible workaround is to
                    minimize lease renewals with something like the following combination of config
                    options.
                    <programlisting># release leases immediately on terminate
force_dhcp_release
# one week lease time
dhcp_lease_time=604800
# two week disassociate timeout
fixed_ip_disassociate_timeout=1209600</programlisting></para>
            </section>
        </section>
        <section xml:id="cloudpipe-per-project-vpns">
            <title>Cloudpipe — Per Project VPNs</title>
            <?dbhtml stop-chunking?>
            <para> Cloudpipe is a method for connecting end users to
                their project instances in VLAN networking mode. </para>
            <para> The support code for cloudpipe implements admin
                commands (via an extension) to automatically create a
                VM for a project that allows users to VPN into the
                private network of their project. Access to this VPN
                is provided through a public port on the network host
                for the project. This allows users to have free access
                to the virtual machines in their project without
                exposing those machines to the public internet. </para>
            <para> The cloudpipe image is basically just a Linux
                instance with openvpn installed. It needs a simple
                script to grab user data from the metadata server, b64
                decode it into a zip file, and run the autorun.sh
                script from inside the zip. The autorun script will
                configure and run openvpn to run using the data from
                nova. </para>
            <para> It is also useful to have a cron script that will
                periodically redownload the metadata and copy the new
                crl. This will keep revoked users from connecting and
                will disconnect any users that are connected with
                revoked certificates when their connection is
                renegotiated (every hour). </para>
            <section xml:id="creating-a-cloudpipe-image">
                
                <title>Creating a Cloudpipe Image</title>
                <para>To make a cloudpipe image: </para>
                <itemizedlist>
                    <listitem>
                        <para>install openvpn on a base ubuntu image.
                        </para>
                    </listitem>
                    <listitem>
                        <para>set up a server.conf.template in
                                <filename>/etc/openvpn/</filename></para>
                    </listitem>
                    <listitem>
                        <para>set up.sh in
                                <filename>/etc/openvpn/</filename>
                        </para>
                    </listitem>
                    <listitem>
                        <para>set down.sh in <filename>/etc/openvpn/
                            </filename></para>
                    </listitem>
                    <listitem>
                        <para>download and run the payload on boot
                            from
                            <filename>/etc/rc.local</filename></para>
                    </listitem>
                    <listitem>
                        <para>setup
                                <filename>/etc/network/interfaces</filename>
                        </para>
                    </listitem>
                    <listitem>
                        <para>upload the image and set the image id in
                            your config file: </para>
                        <programlisting>
vpn_image_id=[uuid from glance]
                        </programlisting>
                    </listitem>
                    <listitem>
                        <para>you should set a few other configuration
                            options to make VPNs work properly: </para>
                        <programlisting>
use_project_ca=True
cnt_vpn_clients=5
force_dhcp_release
                        </programlisting>
                    </listitem>
                </itemizedlist>
                <para> When you use the os-cloudpipe extension (POST
                    v2/{tenant_id}/os-cloudpipe) or the nova client
                        (<userinput>nova cloudpipe-create
                            <replaceable>[project_id]</replaceable></userinput>)
                    to launch a VPN for a user it goes through the
                    following process: </para>
                <orderedlist>
                    <listitem>
                        <para> creates a keypair called
                                   <literal><replaceable>&lt;project_id&gt;</replaceable>-vpn</literal>
                            and saves it in the keys directory </para>
                    </listitem>
                    <listitem>
                        <para> creates a security group
                                   <literal><replaceable>&lt;project_id&gt;</replaceable>-vpn</literal>
                            and opens up 1194 and icmp </para>
                    </listitem>
                    <listitem>
                        <para> creates a cert and private key for the
                            VPN instance and saves it in the
                                   <literal>CA/projects/<replaceable>&lt;project_id&gt;</replaceable>/
                                directory</literal>
                        </para>
                    </listitem>
                    <listitem>
                        <para> zips up the info and puts it b64
                            encoded as user data </para>
                    </listitem>
                    <listitem>
                        <para> launches an
                                <replaceable>[vpn_instance_type]</replaceable>
                            instance with the above settings using the
                            option-specified VPN image</para>
                    </listitem>
                </orderedlist>
            </section>
            <section xml:id="vpn-access">
                <title>VPN Access</title>
                <para> In VLAN networking mode, the second IP in each
                    private network is reserved for the cloudpipe
                    instance. This gives a consistent IP to the
                    instance so that nova-network can create
                    forwarding rules for access from the outside
                    world. The network for each project is given a
                    specific high-numbered port on the public IP of
                    the network host. This port is automatically
                    forwarded to 1194 on the VPN instance. </para>
                <para> If specific high numbered ports do not work for
                    your users, you can always allocate and associate
                    a public IP to the instance, and then change the
                        <literal>vpn_public_ip</literal> and
                        <literal>vpn_public_port</literal> in the
                    database. Rather than using the database directly,
                    you can also use <command>nova-manage vpn change
                            <replaceable>[new_ip]</replaceable>
                        <replaceable>[new_port]</replaceable></command>
                </para>
            </section>
            <section xml:id="certificates-and-revocation">
                <title>Certificates and Revocation</title>
                <para>For certificate management, it is also useful to have a cron script that will
                    periodically download the metadata and copy the new Certificate Revocation List
                    (CRL). This will keep revoked users from connecting and disconnects any users
                    that are connected with revoked certificates when their connection is
                    re-negotiated (every hour). You set the use_project_ca option in nova.conf for
                    cloudpipes to work securely so that each project has its own Certificate
                    Authority (CA).</para>
                <para>If the <literal>use_project_ca config</literal> option is set (required to for
                    cloudpipes to work securely), then each project has its own CA. This CA is used
                    to sign the certificate for the vpn, and is also passed to the user for bundling
                    images. When a certificate is revoked using nova-manage, a new Certificate
                    Revocation List (crl) is generated. As long as cloudpipe has an updated crl, it
                    will block revoked users from connecting to the vpn. </para>
                <para> The userdata for cloudpipe isn't currently
                    updated when certs are revoked, so it is necessary
                    to restart the cloudpipe instance if a user's
                    credentials are revoked. </para>
            </section>
            <section
                xml:id="restarting-and-logging-into-cloudpipe-vpn">
                <title>Restarting and Logging into the Cloudpipe
                    VPN</title>
                <para>You can reboot a cloudpipe vpn through the api
                    if something goes wrong (using <command>nova
                        reboot</command> for example), but if you
                    generate a new crl, you will have to terminate it
                    and start it again using the cloudpipe extension.
                    The cloudpipe instance always gets the first ip in
                    the subnet and if force_dhcp_release is not set it
                    takes some time for the ip to be recovered. If you
                    try to start the new vpn instance too soon, the
                    instance will fail to start because of a
                    "NoMoreAddresses" error. It is therefore
                    recommended to use
                        <literal>force_dhcp_release</literal>.</para>
                <para>The keypair that was used to launch the
                    cloudpipe instance should be in the
                            <filename>keys/<replaceable>&lt;project_id&gt;</replaceable></filename>
                    folder. You can use this key to log into the
                    cloudpipe instance for debugging purposes. If you
                    are running multiple copies of nova-api this key
                    will be on whichever server used the original
                    request. To make debugging easier, you may want to
                    put a common administrative key into the cloudpipe
                    image that you create.</para>
            </section>
        </section>
    </section>
    <section xml:id="enabling-ping-and-ssh-on-vms">
        <title>Enabling Ping and SSH on VMs</title>
        <para>Be sure you enable access to your VMs by using the
                <command>euca-authorize</command> or <command>nova
                secgroup-add-rule</command> command. Below, you will
            find the commands to allow <command>ping</command> and
                <command>ssh</command> to your VMs: </para>
        <note>
            <para>These commands need to be run as root only if the
                credentials used to interact with nova-api have been
                put under <filename>/root/.bashrc</filename>. If the
                EC2 credentials have been put into another user's
                    <filename>.bashrc</filename> file, then, it is
                necessary to run these commands as the user. </para>
        </note>
        <para>Using the nova command-line tool:</para>
        <screen>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0</userinput>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default tcp 22 22 0.0.0.0/0</userinput>
            </screen>
        <para>Using euca2ools:</para>
        <screen>
<prompt>$</prompt> <userinput>euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default</userinput>
<prompt>$</prompt> <userinput>euca-authorize -P tcp -p 22 -s 0.0.0.0/0 default</userinput>
            </screen>

        <para>If you still cannot ping or SSH your instances after
            issuing the <command>nova secgroup-add-rule</command>
            commands, look at the number of <literal>dnsmasq</literal>
            processes that are running. If you have a running
            instance, check to see that TWO <literal>dnsmasq</literal>
            processes are running. If not, perform the following as
            root:</para>
        <screen>
<prompt>#</prompt> <userinput>killall dnsmasq</userinput>
<prompt>#</prompt> <userinput>service nova-network restart</userinput>
           </screen>
    </section>
    <section xml:id="associating-public-ip">
        <title>Configuring Public (Floating) IP Addresses</title>
        <?dbhtml stop-chunking?>
        <section xml:id="Private_and_Public_IP_Addresses">
            <title>Private and Public IP Addresses</title>
            <para>Every virtual instance is automatically assigned a
                private IP address. You may optionally assign public
                IP addresses to instances. OpenStack uses the term
                "floating IP" to refer to an IP address (typically
                public) that can be dynamically added to a running
                virtual instance. OpenStack Compute uses Network
                Address Translation (NAT) to assign floating IPs to
                virtual instances. </para>
            <para>If you plan to use this feature, you must add the
                following to your nova.conf file to specify which
                interface the nova-network service will bind public IP
                addresses to:</para>
            <programlisting>
public_interface=vlan100
        </programlisting>
            <para>Restart the nova-network service if you change
                nova.conf while the service is running.</para>
        </section>
        <section
            xml:id="Creating_a_List_of_Available_Floating_IP_Addresses">
            <title>Creating a List of Available Floating IP
                Addresses</title>
            <para>Nova maintains a list of floating IP addresses that
                are available for assigning to instances. Use the
                    <command>nova-manage floating create</command>
                command to add entries to this list, as root.</para>
            <para>For example:</para>
            <screen>
<prompt>#</prompt> <userinput>nova-manage floating create --ip_range=68.99.26.170/31</userinput>
        </screen>
            <para>The following nova-manage commands apply to floating
                IPs.</para>
            <itemizedlist>
                <listitem>
                    <para><command>nova-manage floating
                        list</command>: List the floating IP addresses
                        in the pool.</para>
                </listitem>
                <listitem>
                    <para><command>nova-manage floating create
                            [cidr]</command>: Create specific floating
                        IPs for either a single address or a
                        subnet.</para>
                </listitem>
                <listitem>
                    <para><command>nova-manage floating delete
                            [cidr]</command>: Remove floating IP
                        addresses using the same parameters as the
                        create command.</para>
                </listitem>
            </itemizedlist>

        </section>
        <section xml:id="Adding_a_Floating_IP_to_an_Instance">
            <title>Adding a Floating IP to an Instance</title>
            <para>Adding a floating IP to an instance is a two step
                process:</para>
            <orderedlist>
                <listitem>
                    <para><command>nova floating-ip-create</command>:
                        Allocate a floating IP address from the list
                        of available addresses.</para>
                </listitem>
                <listitem>
                    <para><command>nova add-floating-ip</command>: Add
                        an allocated floating IP address to a running
                        instance.</para>
                </listitem>
            </orderedlist>

            <para>Here's an example of how to add a floating IP to a
                running instance with an ID of 12</para>
            <screen>
<prompt>$</prompt> <userinput>nova floating-ip-create</userinput>
<computeroutput>
+-----------------+-------------+----------+------+
|        Ip       | Instance Id | Fixed Ip | Pool |
+-----------------+-------------+----------+------+
|    68.99.26.170 | None        | None     |      |
+-----------------+-------------+----------+------+
</computeroutput>
<prompt>$</prompt> <userinput>nova add-floating-ip 12 68.99.26.170</userinput>
        </screen>

            <para>If the instance no longer needs a public address,
                remove the floating IP address from the instance and
                de-allocate the address:</para>
            <screen>
<prompt>$</prompt> <userinput>nova remove-floating-ip 12 68.99.26.170</userinput>
<prompt>$</prompt> <userinput>nova floating-ip-delete 68.99.26.170</userinput>
    </screen>
        </section>

        <section xml:id="Automatically_adding_floating_IPs">
            <title>Automatically adding floating IPs</title>
            <para>The nova-network service can be configured to
                automatically allocate and assign a floating IP
                address to virtual instances when they are launched.
                Add the following line to nova.conf and restart the
                nova-network service</para>

            <programlisting>
auto_assign_floating_ip=True
            </programlisting>
            <para>Note that if this option is enabled and all of the
                floating IP addresses have already been allocated, the
                    <command>nova boot</command> command will fail
                with an error.</para>
        </section>
        <section xml:id="Troubleshooting">
            <title>Troubleshooting</title>
            <para>If you aren't able to reach your instances via the
                floating IP address, make sure the default security
                group allows ICMP (ping) and SSH (port 22), so that
                you can reach the instances:</para>
            <screen>
<prompt>$</prompt> <userinput>nova secgroup-list-rules default</userinput>
<computeroutput>
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+
</computeroutput>
            </screen>
            <para>Ensure the NAT rules have been added to iptables on
                the node that nova-network is running on, as
                root:</para>

            <screen>
<prompt>#</prompt> <userinput>iptables -L -nv</userinput>
<computeroutput>
 -A nova-network-OUTPUT -d 68.99.26.170/32 -j DNAT --to-destination 10.0.0.3
</computeroutput>
<prompt>#</prompt> <userinput>iptables -L -nv -t nat</userinput>
<computeroutput>
-A nova-network-PREROUTING -d 68.99.26.170/32 -j DNAT --to-destination10.0.0.3
-A nova-network-floating-snat -s 10.0.0.3/32 -j SNAT --to-source 68.99.26.170
</computeroutput>
            </screen>

            <para>Check that the public address, in this example
                "68.99.26.170", has been added to your public
                interface: You should see the address in the listing
                when you enter "ip addr" at the command prompt.</para>

            <screen>
<prompt>$</prompt> <userinput>ip addr</userinput>
<computeroutput>
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
link/ether xx:xx:xx:17:4b:c2 brd ff:ff:ff:ff:ff:ff
inet 13.22.194.80/24 brd 13.22.194.255 scope global eth0
inet 68.99.26.170/32 scope global eth0
inet6 fe80::82b:2bf:fe1:4b2/64 scope link
valid_lft forever preferred_lft forever
</computeroutput>
            </screen>

            <para>Note that you cannot SSH to an instance with a
                public IP from within the same server as the routing
                configuration won't allow it. </para>
        </section>
    </section>
    <section xml:id="removing-network-from-project">
        <title>Removing a Network from a Project</title>
        <para>You will find that you cannot remove a network that has
            already been associated to a project by simply deleting
            it. You can disassociate the project from the network with
            a scrub command and the project name as the final
            parameter: </para>
        <screen>
<prompt>$</prompt> <userinput>nova-manage project scrub projectname</userinput>
        </screen>
    </section>
    <section xml:id="using-multi-nics">
        <title>Using multiple interfaces for your instances
            (multinic)</title>
        <?dbhtml stop-chunking?>
        <para> The multi-nic feature allows you to plug more than one
            interface to your instances, making it possible to make
            several use cases available : <itemizedlist>
                <listitem>
                    <para>SSL Configurations (VIPs)</para>
                </listitem>
                <listitem>
                    <para>Services failover/ HA</para>
                </listitem>
                <listitem>
                    <para>Bandwidth Allocation</para>
                </listitem>
                <listitem>
                    <para>Administrative/ Public access to your
                        instances</para>
                </listitem>
            </itemizedlist> Each VIF is representative of a separate
            network with its own IP block. Every network mode
            introduces it's own set of changes regarding the mulitnic
            usage : <figure>
                <title>multinic flat manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="50"
                            fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <figure>
                <title>multinic flatdhcp manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40"
                            fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <figure>
                <title>multinic VLAN manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40"
                            fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
        </para>
        <section xml:id="using-multiple-nics-usage">
            <title>Using the multinic feature</title>
            <para> The first thing to do is to create a new network
                and attach it to your project :
                <screen><prompt>$</prompt> <userinput>nova-manage network create --fixed_range_v4=20.20.0.0/24 --num_networks=1 --network_size=256 --label=test --project=$your-project</userinput></screen>
                Now every time you spawn a new instance, it gets two
                IP addresses from the respective DHCP servers : <screen><prompt>$</prompt> <userinput>nova list</userinput>
<computeroutput>+-----+------------+--------+----------------------------------------+
|  ID |    Name    | Status |                Networks                |
+-----+------------+--------+----------------------------------------+
| 124 | Server 124 | ACTIVE | network2=20.20.0.3; private=20.10.0.14 |
+-----+------------+--------+----------------------------------------+</computeroutput></screen>
                <note>
                    <para>Make sure to power up the second interface
                        on the instance, otherwise that last won't be
                        reacheable via its second IP. Here is an
                        example of how to setup the interfaces within
                        the instance : </para>
                    <para><filename>/etc/network/interfaces</filename>
                        <programlisting># The loopback network interface
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet dhcp                </programlisting></para>
                </note></para>
        </section>
    </section>
    <section xml:id="existing-ha-networking-options">
        <title>Existing High Availability Options for
            Networking</title>
        <para>Adapted from a blog post by<link
                xlink:href="http://unchainyourbrain.com/openstack/13-networking-in-nova">Vish
                Ishaya</link></para>

        <para>As illustrated in the Flat DHCP diagram in Section <link
                xlink:href="#configuring-flat-dhcp-networking">Configuring Flat DHCP
                Networking</link> titled <link linkend="flat-dhcp-diagram">Flat DHCP network, multiple interfaces, multiple servers</link>,
            traffic from the VM to the public internet has to go through the host running nova
            network. DHCP is handled by nova-network as well, listening on the gateway address of
            the fixed_range network. The compute hosts can optionally have their own public IPs, or
            they can use the network host as their gateway. This mode is pretty simple and it works
            in the majority of situations, but it has one major drawback: the network host is a
            single point of failure! If the network host goes down for any reason, it is impossible
            to communicate with the VMs. Here are some options for avoiding the single point of
            failure.</para>
        <simplesect>
            <title>HA Option 1: Multi-host</title>
            <para>To eliminate the network host as a single point of failure, Compute can be
                configured to allow each compute host to do all of the networking jobs for its own
                VMs. Each compute host does NAT, DHCP, and acts as a gateway for all of its own VMs.
                While there is still a single point of failure in this scenario, it is the same
                point of failure that applies to all virtualized systems.</para>

            <para>This setup requires adding an IP on the VM network to each host in the system, and
                it implies a little more overhead on the compute hosts. It is also possible to
                combine this with option 4 (HW Gateway) to remove the need for your compute hosts to
                gateway. In that hybrid version they would no longer gateway for the VMs and their
                responsibilities would only be DHCP and NAT.</para>
            <para>The resulting layout for the new HA networking
                option looks the following diagram:</para>
            <para><figure>
                <title>High Availability Networking Option</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="50"
                            fileref="figures/ha-net.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure></para>
            <para>In contrast with the earlier diagram, all the hosts in the system are running the
                nova-compute, nova-network and nova-api services. Each host does DHCP and does NAT
                for public traffic for the VMs running on that particular host. In this model every
                compute host requires a connection to the public internet and each host is also
                assigned an address from the VM network where it listens for DHCP traffic. The
                nova-api service is needed so that it can act as a metadata server for the
                instances.</para>
            <para>To run in HA mode, each compute host must run the following services:<itemizedlist>
                    <listitem>
                        <para><command>nova-compute</command></para>
                    </listitem>
                    <listitem>
                        <para><command>nova-network</command></para>
                    </listitem>
                    <listitem>
                        <para><command>nova-api</command></para>
                    </listitem>
                </itemizedlist></para>
            <para>The <filename>nova.conf</filename> file on your compute hosts must contain the
                following
                options:<programlisting>multi_host=True
enabled_apis=metadata</programlisting></para>
            <para>If a compute host is also an API endpoint, your <literal>enabled_apis</literal>
                option will need to contain additional values, depending on the API services. For
                example, if it supports compute requests, volume requests, and EC2 compatibility,
                the <filename>nova.conf</filename> file should contain:
                <programlisting>multi_host=True
enabled_apis=ec2,osapi_compute,osapi_volume,metadata</programlisting></para>

            <para>The <literal>multi_host</literal> option must be in place for network creation and
                nova-network must be run on every compute host. These created multi hosts networks
                will send all network related commands to the host that the VM is on. You need to
                set the configuration option <literal>enabled_apis</literal> such that it includes
                    <literal>metadata</literal> in the list of enabled APIs. </para>
        </simplesect>

        <simplesect>
            <title>HA Option 2: Failover</title>
            <para>The folks at NTT labs came up with a ha-linux
                configuration that allows for a 4 second failover to a
                hot backup of the network host. Details on their
                approach can be found in the following post to the
                openstack mailing list: <link
                    xlink:href="https://lists.launchpad.net/openstack/msg02099.html"
                    >https://lists.launchpad.net/openstack/msg02099.html</link></para>
            <para>This solution is definitely an option, although it
                requires a second host that essentially does nothing
                unless there is a failure. Also four seconds can be
                too long for some real-time applications.</para>
            <para>To enable this HA option, your <filename>nova.conf</filename> file must contain
                the following option:<programlisting>send_arp_for_ha=True</programlisting></para>
            <para>See <link xlink:href="https://bugs.launchpad.net/nova/+bug/782364"
                    >https://bugs.launchpad.net/nova/+bug/782364</link> for details on why this
                option is required when configuring for failover.</para>
        </simplesect>
        <simplesect>
            <title>HA Option 3: Multi-nic</title>
            <para>Recently, nova gained support for multi-nic. This
                allows us to bridge a given VM into multiple networks.
                This gives us some more options for high availability.
                It is possible to set up two networks on separate
                vlans (or even separate ethernet devices on the host)
                and give the VMs a NIC and an IP on each network. Each
                of these networks could have its own network host
                acting as the gateway.</para>
            <para>In this case, the VM has two possible routes out. If
                one of them fails, it has the option of using the
                other one. The disadvantage of this approach is it
                offloads management of failure scenarios to the guest.
                The guest needs to be aware of multiple networks and
                have a strategy for switching between them. It also
                doesn't help with floating IPs. One would have to set
                up a floating IP associated with each of the IPs on
                private the private networks to achieve some type of
                redundancy.</para>
        </simplesect>
        <simplesect>
            <title>HA Option 4: Hardware gateway</title>
            <para>The <systemitem class="service">dnsmasq</systemitem> service can be configured to
                use an external gateway instead of acting as the gateway for the VMs. This offloads
                HA to standard switching hardware and it has some strong benefits. Unfortunately,
                the <systemitem class="service">nova-network</systemitem> service is still
                responsible for floating IP natting and DHCP, so some failover strategy needs to be
                employed for those options. To configure for hardware gateway:<orderedlist>
                    <listitem>
                        <para>Create a <systemitem class="service">dnsmasq</systemitem>
                            configuration file (e.g., <filename>/etc/dnsmasq-nova.conf</filename>)
                            that contains the IP address of the external gateway using the following
                            syntax:<programlisting>dhcpoption=3,<replaceable>&lt;ip of gateway></replaceable></programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Edit <filename>/etc/nova/nova.conf</filename> to specify the location
                            of the <systemitem class="service">dnsmasq</systemitem> configuration
                            file:<programlisting>dnsmasq_config_file=/etc/dnsmasq-nova.conf</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Configure the hardware gateway to forward metadata requests to a host
                            that's running the <systemitem class="service">nova-api</systemitem>
                            service with the metadata API enabled.</para>
                        <para>The virtual machine instances access the metadata service at
                                <literal>169.254.169.254</literal> port <literal>80</literal>. The
                            hardware gateway should forward these requests to a host  running the
                                <systemitem class="service">nova-api</systemitem> service on the
                            port specified as the <literal>metadata_host</literal> config option in
                                <filename>/etc/nova/nova.conf</filename>, which defaults to
                                <literal>8775</literal>.</para>
                        <para>Make sure that the list in the <literal>enabled_apis</literal>
                            configuration option <filename>/etc/nova/nova.conf</filename> contains
                                <literal>metadata</literal> in addition to the other APIs. An
                            example that contains the EC2 API, the OpenStack compute API, the
                            OpenStack volume API, and the metadata service would look like:
                            <programlisting>enabled_apis=ec2,osapi_compute,osapi_volume,metadata</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Ensure you have set up routes properly so that the subnet that you use
                            for virtual machines is routable.</para>
                    </listitem>
                </orderedlist></para>
            <warning>
                <para>The hardware gateway option is not available when running in VLAN mode, as it
                    can only be used to specify a single gateway for all instances, and VLAN mode
                    requires a separate gateway for each network.</para>
            </warning>
        </simplesect>

    </section>
</chapter>
