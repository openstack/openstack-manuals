<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_networking">
    <title>Networking</title>
    <para>By understanding the available networking configuration
        options you can design the best configuration for your
        OpenStack Compute instances.</para>

    <section xml:id="networking-options">
        <title>Networking Options</title>
        <para>This section offers a brief overview of each concept in
            networking for Compute. </para>
        <para>In Compute, users organize their cloud resources in projects. A Compute project
            consists of a number of VM instances created by a user. For each VM instance, Compute
            assigns to it a private IP address. (Currently, Compute only supports Linux bridge
            networking that allows the virtual interfaces to connect to the outside network through
            the physical interface.)</para>
        <para>The Network Controller provides virtual networks to
            enable compute servers to interact with each other and
            with the public network.</para>
        <para>Currently, Compute supports three kinds of networks, implemented in three “Network
            Manager” types:<itemizedlist>
                <listitem>
                    <para>Flat Network Manager</para>
                </listitem>
                <listitem>
                    <para>Flat DHCP Network Manager</para>
                </listitem>
                <listitem>
                    <para>VLAN Network Manager</para>
                </listitem>
            </itemizedlist></para> <para>The three kinds of networks can co-exist in a cloud system. However,
            since you can't yet select the type of network for a given project, you cannot configure
            more than one type of network in a given Compute installation.</para>
        <note>
            <para>All of the networking options require network connectivity to be already set up
                between OpenStack physical nodes. OpenStack will not create or configure any network
                interfaces (except bridges and VM virtual interfaces). </para>
            <para>All machines must have a <emphasis role="italic">public</emphasis> and <emphasis
                    role="italic">internal</emphasis> network interface (controlled by the options:
                    <literal>public_interface</literal> for the public interface, and
                    <literal>flat_interface</literal> and <literal>vlan_interface</literal> for the
                internal interface with flat / VLAN managers). </para>
            <para>The internal network interface is used for communication with VMs, it shouldn't
                have an IP address attached to it before OpenStack installation (it serves merely as
                a fabric where the actual endpoints are VMs and dnsmasq). Also, the internal network
                interface must be put in <emphasis role="italic">promiscuous mode</emphasis>,
                because it will have to receive packets whose target MAC address is of the guest VM,
                not of the host.</para>
        </note>
        <para>All the network managers configure the network using <emphasis role="italic">network
                drivers</emphasis>, e.g. the linux L3 driver (<literal>l3.py</literal> and
                <literal>linux_net.py</literal>) which makes use of <literal>iptables</literal>,
                <literal>route</literal> and other network management facilities, and also of
            libvirt's <link xlink:href="http://libvirt.org/formatnwfilter.html">network filtering
                facilities</link>. The driver isn't tied to any particular network manager; all
            network managers use the same driver. The driver usually initializes (creates bridges
            etc.) only when the first VM lands on this host node. </para>
        <para>All network managers operate in either <emphasis role="italic">single-host</emphasis>
            or <emphasis role="italic">multi-host</emphasis> mode. This choice greatly influences
            the network configuration. In single-host mode, there is just 1 instance of
                <literal>nova-network</literal> which is used as a default gateway for VMs and hosts
            a single DHCP server (dnsmasq), whereas in multi-host mode every compute node has its
            own <literal>nova-network</literal>. In any case, all traffic between VMs and the outer
            world flows through <literal>nova-network</literal>. There are pros and cons to both
            modes, read more in <link linkend="existing-ha-networking-options">Existing High
                Availability Options</link>.</para>
        <para>Compute makes a distinction between <emphasis role="italic">fixed IPs</emphasis> and
                <emphasis role="italic">floating IPs</emphasis> for VM instances. Fixed IPs are IP
            addresses that are assigned to an instance on creation and stay the same until the
            instance is explicitly terminated. By contrast, floating IPs are addresses that can be
            dynamically associated with an instance. A floating IP address can be disassociated and
            associated with another instance at any time. A user can reserve a floating IP for their
            project. </para>
        <para>In <emphasis role="bold">Flat Mode</emphasis>, a network administrator specifies a
            subnet. The IP addresses for VM instances are grabbed from the subnet, and then injected
            into the image on launch. Each instance receives a fixed IP address from the pool of
            available addresses. A network administrator must configure the Linux networking bridge
            (typically named <literal>br100</literal>, although this configurable) both on the
            network controller hosting the network and on the cloud controllers hosting the
            instances. All instances of the system are attached to the same bridge, configured
            manually by the network administrator.</para>
        <para>
            <note>
                <para>The configuration injection currently only works on Linux-style systems that
                    keep networking configuration in
                    <filename>/etc/network/interfaces</filename>.</para>
            </note>
        </para>
        <para>In <emphasis role="bold">Flat DHCP Mode</emphasis>, OpenStack starts a DHCP server
            (dnsmasq) to pass out IP addresses to VM instances from the specified subnet in addition
            to manually configuring the networking bridge. IP addresses for VM instances are grabbed
            from a subnet specified by the network administrator. </para>
        <para>Like Flat Mode, all instances are attached to a single bridge on the compute node. In
            addition a DHCP server is running to configure instances (depending on
            single-/multi-host mode, alongside each <literal>nova-network</literal>). In this mode,
            Compute does a bit more configuration in that it attempts to bridge into an ethernet
            device (<literal>flat_interface</literal>, eth0 by default). It will also run and
            configure dnsmasq as a DHCP server listening on this bridge, usually on IP address
            10.0.0.1 (see <link linkend="dnsmasq">DHCP server: dnsmasq</link>). For every instance,
            nova will allocate a fixed IP address and configure dnsmasq with the MAC/IP pair for the
            VM, i.e. dnsmasq doesn't take part in the IP address allocation process, it only hands
            out IPs according to the mapping done by nova. Instances receive their fixed IPs by
            doing a dhcpdiscover. These IPs are <emphasis role="italic">not</emphasis> assigned to
            any of the host's network interfaces, only to the VM's guest-side interface.</para>
        <para>In any setup with flat networking, the host(-s) with nova-network on it is (are)
            responsible for forwarding traffic from the private network configured with the
                <literal>fixed_range</literal> configuration option in
                <filename>nova.conf</filename>. Such host(-s) needs to have <literal>br100</literal>
            configured and physically connected to any other nodes that are hosting VMs. You must
            set the <literal>flat_network_bridge</literal> option or create networks with the bridge
            parameter in order to avoid raising an error. Compute nodes have iptables/ebtables
            entries created per project and instance to protect against IP/MAC address spoofing and
            ARP poisoning. </para>
        <note>
            <para>In single-host Flat DHCP mode you <emphasis role="italic">will</emphasis> be able
                to ping VMs via their fixed IP from the nova-network node, but you <emphasis
                    role="italic">will not</emphasis> be able to ping them from the compute nodes.
                This is expected behavior.</para>
        </note>
        <para><emphasis role="bold">VLAN Network Mode is the default mode</emphasis> for OpenStack
            Compute. In this mode, Compute creates a VLAN and bridge for each project. For multiple
            machine installation, the VLAN Network Mode requires a switch that supports VLAN tagging
            (IEEE 802.1Q). The project gets a range of private IPs that are only accessible from
            inside the VLAN. In order for a user to access the instances in their project, a special
            VPN instance (code named cloudpipe) needs to be created. Compute generates a certificate
            and key for the user to access the VPN and starts the VPN automatically. It provides a
            private network segment for each project's instances that can be accessed via a
            dedicated VPN connection from the Internet. In this mode, each project gets its own
            VLAN, Linux networking bridge, and subnet. </para>
        <para>The subnets are specified by the network administrator,
            and are assigned dynamically to a project when required. A
            DHCP Server is started for each VLAN to pass out IP
            addresses to VM instances from the subnet assigned to the
            project. All instances belonging to one project are
            bridged into the same VLAN for that project. OpenStack
            Compute creates the Linux networking bridges and VLANs
            when required.</para>
        <note>
            <para>With the default Compute settings, once a virtual machine instance is destroyed,
                it can take some time for the IP address associated with the destroyed instance to
                become available for assignment to a new instance.</para>
            <para>The <literal>force_dhcp_release=True</literal> configuration option, when set,
                causes the Compute service to send out a DHCP release packet when it destroys a
                virtual machine instance. The result is that the IP address assigned to the instance
                is immediately released. </para>
            <para>This configuration option applies to both Flat DHCP mode and VLAN Manager
                mode.</para>
            <para>Use of this option requires the <command>dhcp_release</command> program. Verify
                that this program is installed on all hosts running the <systemitem class="service"
                    >nova-compute</systemitem> service before enabling this option. This can be
                checked with the <command>which</command> command, and will return the complete path
                if the program is installed. As
                root:<screen><prompt>#</prompt> <userinput>which dhcp_release</userinput>
<computeroutput>/usr/bin/dhcp_release</computeroutput></screen></para>
        </note>
    </section>
    <section xml:id="dnsmasq">
        <title>DHCP server: dnsmasq</title>
        <para>The Compute service uses <link
                xlink:href="http://www.thekelleys.org.uk/dnsmasq/doc.html"
                >dnsmasq</link> as the DHCP server when running with
            either that Flat DHCP Network Manager or the VLAN Network
            Manager. The <systemitem class="service"
                >nova-network</systemitem> service is responsible for
            starting up dnsmasq processes.</para>
        <para>The behavior of dnsmasq can be customized by creating a
            dnsmasq configuration file. Specify the config file using
            the <literal>dnsmasq_config_file</literal> configuration
            option. For
            example:<programlisting>dnsmasq_config_file=/etc/dnsmasq-nova.conf</programlisting>See
            the <link linkend="existing-ha-networking-options">high
                availability section</link> for an example of how to
            change the behavior of dnsmasq using a dnsmasq
            configuration file. The dnsmasq documentation has a more
            comprehensive <link
                xlink:href="http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq.conf.example"
                >dnsmasq configuration file example</link>.</para>
        <para>Dnsmasq also acts as a caching DNS server for instances.
            You can explicitly specify the DNS server that dnsmasq
            should use by setting the <literal>dns_server</literal>
            configuration option in
                <filename>/etc/nova/nova.conf</filename>. The
            following example would configure dnsmasq to use Google's
            public DNS
            server:<programlisting>dns_server=8.8.8.8</programlisting></para>
        <para>Dnsmasq logging output goes to the syslog (typically
                <filename>/var/log/syslog</filename> or
                <filename>/var/log/messages</filename>, depending on
            Linux distribution). The dnsmasq logging output can be
            useful for troubleshooting if VM instances boot
            successfully but are not reachable over the
            network.</para>
        <para>A network administrator can run <code>nova-manage fixed
                reserve
                --address=<replaceable>x.x.x.x</replaceable></code> to
            specify the starting point IP address (x.x.x.x) to reserve
            with the DHCP server, replacing the
            flat_network_dhcp_start configuration option that was
            available in Diablo. This reservation only affects which
            IP address the VMs start at, not the fixed IP addresses
            that the nova-network service places on the bridges.</para>
    </section>
    <section xml:id="metadata-service">
        <title>Metadata service</title>
        <para>The Compute service uses a special metadata service to enable virtual machine
            instances to retrieve instance-specific data.  Instances access the metadata service at
                <literal>http://169.254.169.254</literal>. For example, instances retrieve the
            public SSH key (identified by keypair name when a user requests a new instance) by
            making a GET request
            to:<programlisting>http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key</programlisting></para>
        <para>Instances also retrieve user data (passed as the <literal>user_data</literal>
            parameter in the API call or by the <literal>--user_data</literal> flag in the
                <command>nova boot</command> command) through the metadata service, by making a GET
            request
            to:<programlisting>http://169.254.169.254/latest/user-data</programlisting></para>
        <para>The Compute metadata service is compatible with the <link
                xlink:href="http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html"
                >Amazon EC2 metadata service</link>; virtual machine images that are designed for
            EC2 will work properly with OpenStack. </para>
        <para>The metadata service is implemented by either the <systemitem class="service"
                >nova-api</systemitem> service or the <systemitem class="service"
                >nova-api-metadata</systemitem> service. (The <systemitem class="service"
                >nova-api-metadata</systemitem> service is generally only used when running in
            multi-host mode, see the section titled <link linkend="existing-ha-networking-options"
                >Existing High Availability Options for Networking</link>  for details). If you are
            running the <systemitem class="service">nova-api</systemitem> service,  you must have
                <literal>metadata</literal> as one of the elements of the list of the
                <literal>enabled_apis</literal> configuration option in
                <filename>/etc/nova/nova.conf</filename>. The default
                <literal>enabled_apis</literal> configuration setting includes the metadata service,
            so you should not need to modify it.</para>
        <para>To allow instances to reach the metadata service, the <systemitem class="service"
                >nova-network</systemitem> service will configure iptables to NAT port
                <literal>80</literal> of the <literal>169.254.169.254</literal> address to the IP
            address specified in <literal>metadata_host</literal> (default
            <literal>$my_ip</literal>, which is the IP address of the <systemitem class="service"
                >nova-network</systemitem> service) and port specified in
                <literal>metadata_port</literal> (default <literal>8775</literal>) in
                <filename>/etc/nova/nova.conf</filename>.
            <warning><para>The <literal>metadata_host</literal> configuration option must be an IP address, not a
                    hostname.</para></warning>
            <note>
                <para>The default Compute service settings assume that the <systemitem
                        class="service">nova-network</systemitem> service and the <systemitem
                        class="service">nova-api</systemitem> service are running on the same host.
                    If this is not the case, you must make the following change in the
                        <filename>/etc/nova/nova.conf</filename> file on the host running the
                        <systemitem class="service">nova-network</systemitem> service:</para>
                <para>Set the <literal>metadata_host</literal> configuration option to the IP
                    address of the host where the <systemitem class="service">nova-api</systemitem>
                    service is running.</para>
            </note></para>
    </section>
    <section xml:id="configuring-networking-on-the-compute-node">
        <title>Configuring Networking on the Compute Node</title>
        <para>To configure the Compute node's networking for the VM
            images, the overall steps are:</para>

        <orderedlist>
            <listitem>
                <para>Set the <literal>network_manager</literal> option in nova.conf.</para>
            </listitem>
            <listitem>
                <para>Use the <code>nova-manage network create label
                        CIDR n n</code> command to create the subnet
                    that the VMs reside on.</para>
            </listitem>
            <listitem>
                <para>Integrate the bridge with your network. </para>
            </listitem>
        </orderedlist>
        <para>By default, Compute uses the VLAN Network Mode. You
            choose the networking mode for your virtual instances in
            the nova.conf file. Here are the three possible options: </para>
        <itemizedlist>
            <listitem>
                <para><literal>--network_manager=nova.network.manager.FlatManager</literal></para>
                <para>Simple, non-VLAN networking</para>
            </listitem>
            <listitem>
                <para><literal>--network_manager=nova.network.manager.FlatDHCPManager</literal></para>
                <para>Flat networking with DHCP, you must set a bridge using the
                        <literal>flat_network_bridge</literal> option</para>
            </listitem>
            <listitem>
                <para><literal>--network_manager=nova.network.manager.VlanManager</literal></para>
                <para>VLAN networking with DHCP. This is the Default
                    if no network manager is defined in nova.conf.
                </para>
            </listitem>
        </itemizedlist>
        <para>When you issue the <literal>nova-manage network create</literal> command, it uses the
            settings from the nova.conf configuration options file. Use the following command to
            create the subnet that your VMs will run on :
            <literallayout class="monospaced"><literal>nova-manage network create private 192.168.0.0/24 1 256</literal></literallayout>
        </para>
        <para>When using the XenAPI compute driver, the OpenStack services run
            in a virtual machine. This means networking is
            significantly different when compared to the networking
            with the libvirt compute driver.
            Before reading how to configure networking using the XenAPI compute
            driver, you may find it useful to read the Citrix article on
            <link xlink:href="http://support.citrix.com/article/CTX117915">
            Understanding XenServer Networking</link> and the section of this
            document that describes
            <link linkend="introduction-to-xen">XenAPI and OpenStack</link>.
        </para>

        <section xml:id="configuring-flat-networking">
            <title>Configuring Flat Networking</title>
            <para>FlatNetworking uses ethernet adapters configured as
                bridges to allow network traffic to transit between
                all the various nodes. This setup can be done with a
                single adapter on the physical host, or multiple. This
                option does not require a switch that does VLAN
                tagging as VLAN networking does, and is a common
                development installation or proof of concept setup.
                When you choose Flat networking, Nova does not manage
                networking at all. Instead, IP addresses are injected
                into the instance via the file system (or passed in
                via a guest agent). Metadata forwarding must be
                configured manually on the gateway if it is required
                within your network. </para>
            <para>To configure flat networking, ensure that your
                nova.conf file contains the following line:</para>
            <para>
                <programlisting>
network_manager=nova.network.manager.FlatManager
                </programlisting>
            </para>
            <note>
                <para>When configuring Flat Networking, failing to enable
                        <literal>flat_injected</literal> can prevent
                    guest VMs from receiving their IP information at
                    boot time.</para>
            </note>

          <section xml:id="libvirt-flat-networking">
          <title>Libvirt Flat Networking</title>
            <para>Compute defaults to a bridge device named ‘br100’
                which is stored in the Nova database, so you can
                change the name of the bridge device by modifying the
                entry in the database. Consult the diagrams for
                additional configuration options.</para>
            <para>In any set up with FlatNetworking (either Flat or
                FlatDHCP), the host with nova-network on it is
                responsible for forwarding traffic from the private
                network configured with the
                    <literal>--fixed_range=</literal> directive in
                nova.conf and the
                    <literal>--flat_network_bridge</literal> setting.
                This host needs to have br100 configured and talking
                to any other nodes that are hosting VMs. With either
                of the Flat Networking options, the default gateway
                for the virtual machines is set to the host which is
                running nova-network. </para>
            <para>Set the compute node's external IP address to be on
                the bridge and add eth0 to that bridge. To do this,
                edit your network interfaces configuration to look
                like the following example: </para>
            <para>
                <programlisting>
# The loopback network interface
auto lo
iface lo inet loopback

# Networking for OpenStack Compute
auto br100

iface br100 inet dhcp
    bridge_ports        eth0
    bridge_stp           off
    bridge_maxwait   0
    bridge_fd            0
 </programlisting>
            </para>
            <para>Next, restart networking to apply the changes:
                    <code>sudo /etc/init.d/networking
                restart</code></para>
            <para>For an all-in-one development setup, this diagram
                represents the network setup.</para>

            <para><figure>
                    <title>Flat network, all-in-one server
                        installation </title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="80"
                                fileref="figures/FlatNetworkSingleInterfaceAllInOne.png"
                            />
                        </imageobject>
                    </mediaobject>
                </figure></para>
            <para>For multiple compute nodes with a single network
                adapter, which you can use for smoke testing or a
                proof of concept, this diagram represents the network
                setup.</para>
            <figure>
                <title>Flat network, single interface, multiple
                    servers</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/FlatNetworkSingleInterface.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <para>For multiple compute nodes with multiple network
                adapters, this diagram represents the network setup.
                You may want to use this setup for separate admin and
                data traffic.</para>
            <figure xml:id="flat-dhcp-diagram">
                <title>Flat network, multiple interfaces, multiple
                    servers</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/FlatNetworkMultInterface.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
          </section>

          <section xml:id="xenapi-flat-networking">
            <title xml:id="xenapi-flat-networking.title">XenAPI Flat Networking</title>
            <para>When using the XenAPI driver, the virtual machines creates
                OpenStack are attached to the XenServer bridge configured in
                the <literal>flat_network_bridge</literal> setting.
                Otherwise, flat networking works in a very similar way with
                both the libvirt driver and the XenAPI driver.
            </para>
          </section>
        </section>

        <section xml:id="configuring-flat-dhcp-networking">
            <title>Configuring Flat DHCP Networking</title>
            <para>With Flat DHCP, the host(-s) running nova-network act as the gateway to the
                virtual nodes. If you're using single-host networking, you can optionally set
                    <literal>network_host</literal> on the <filename>nova.conf</filename> stored on
                the nova-compute node to tell it which host the nova-network is running on so it can
                more efficiently communicate with nova-network. In any setup with flat networking,
                the hosts with nova-network on it are responsible for forwarding traffic from the
                private network configured with the <literal>fixed_range=</literal> directive in
                    <filename>nova.conf</filename> and the <literal>flat_network_bridge</literal>
                flag which you must also set to the name of the bridge (as there is no default). The
                nova-network service will track leases and releases in the database, using dnsmasq's
                    <emphasis role="italic">dhcp-script</emphasis> facility (the script <emphasis
                    role="italic">bin/nova-dhcpbridge</emphasis> is supplied) so it knows if a VM
                instance has stopped properly configuring via DHCP (e.g. when a DHCP lease expires,
                the fixed IP is released from the nova database). Lastly, it sets up iptables rules
                to allow the VMs to communicate with the outside world and contact a special
                metadata server to retrieve information from the cloud.</para>
            <para>Compute hosts in the FlatDHCP model are responsible for bringing up a matching
                bridge and bridging the VM tap devices into the same ethernet device that the
                network host is on. The compute hosts should not have an IP address on the VM
                network, because the bridging puts the VMs and the network host on the same logical
                network. When a VM boots, the VM sends out DHCP packets, and the DHCP server on the
                network host responds with their assigned IP address (remember, the address is
                actually <emphasis role="italic">assigned</emphasis> by nova and put into DHCP
                server's configuration file, the DHCP server merely tells the VM what it is).</para>
            <para>You can read a detailed walk-through of what exactly happens in single-host Flat
                DHCP mode in <link
                    xlink:href="http://www.mirantis.com/blog/openstack-networking-single-host-flatdhcpmanager/"
                    >this blogpost</link>, parts of which are also relevant in other networking
                modes.</para> 
            <para>FlatDHCP doesn't create VLANs, it creates a bridge.
                This bridge works just fine on a single host, but when
                there are multiple hosts, traffic needs a way to get
                out of the bridge onto a physical interface. </para>

          <section xml:id="libvirt-flat-dhcp-networking">
            <title>Libvirt Flat DHCP Networking</title>
            <para>When using the libvirt driver, the setup will look like
            the figure below:</para>
            <figure>
                <title>Flat DHCP network, multiple interfaces,
                    multiple servers with libvirt driver</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="50"
                            fileref="figures/flatdchp-net.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Be careful when setting up
                    <literal>--flat_interface</literal>. If you
                specify an interface that already has an IP it will
                break and if this is the interface you are connecting
                through with SSH, you cannot fix it unless you have
                ipmi/console access. In FlatDHCP mode, the setting for
                    <literal>--network_size</literal> should be number
                of IPs in the entire fixed range. If you are doing a
                /12 in CIDR notation, then this number would be 2^20
                or 1,048,576 IP addresses. That said, it will take a
                very long time for you to create your initial network,
                as an entry for each IP will be created in the
                database. </para>
            <para>If you have an unused interface on your hosts (eg eth2) that
                has connectivity with no IP address, you can simply
                tell FlatDHCP to bridge into the interface by
                specifying
                        <literal>flat_interface=<replaceable>&lt;interface></replaceable></literal>
                in your configuration file. The network host will
                automatically add the gateway ip to this bridge. If this is the case for you, edit
                your nova.conf file to contain the following lines: </para>
            <para>
                <programlisting>
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
network_manager=nova.network.manager.FlatDHCPManager
fixed_range=10.0.0.0/8
flat_network_bridge=br100
flat_interface=eth2
flat_injected=False
public_interface=eth0
                </programlisting>
                You can also add the unused interface to br100 manually and not
                set flat_interface. 
            </para>
            <para>Integrate your network interfaces to match this
                configuration.</para>
          </section>
          <section xml:id="xenapi-flat-dhcp-networking">
            <title xml:id="xenapi-flat-dhcp-networking.title">XenAPI Flat DHCP Networking</title>
            <para>The following figure shows a setup with
            Flat DHCP networking, network HA, and using multiple interfaces.
            For simplicity, the management network (on XenServer
            eth0 and eth2 of the VM running the OpenStack services)
            has been omitted from the figure below.</para>
            <figure xml:id="xenapi-flat-dhcp-diagram">
                <title>Flat DHCP network, multiple interfaces, multiple
                    servers, network HA with XenAPI driver</title>
                <mediaobject><imageobject>
                    <imagedata scale="80" fileref="figures/XenApiFlatDHCPMultInterfaceHA.png" />
                </imageobject></mediaobject>
            </figure>            
            <para>Here is an extract from a <filename>nova.conf</filename>
                file in a system running the above setup:</para>
            <para>
                <programlisting>network_manager=nova.network.manager.FlatDHCPManager
xenapi_vif_driver=nova.virt.xenapi.vif.(XenAPIBridgeDriver or XenAPIOpenVswitchDriver)
flat_interface=eth1
flat_network_bridge=xenbr2
public_interface=eth3
multi_host=True
dhcpbridge_flagfile=/etc/nova/nova.conf
fixed_range=10.0.0.0/24
force_dhcp_release=True
send_arp_for_ha=True
flat_injected=False
firewall_driver=nova.virt.xenapi.firewall.Dom0IptablesFirewallDriver</programlisting>
            </para>
            <para>You should notice that
                <literal>flat_interface</literal>
                and
                <literal>public_interface</literal>
                refer to the network interface on the VM running the OpenStack
                services, not the network interface on the Hypervisor.
            </para>
            <para>Secondly
                <literal>flat_network_bridge</literal>
                refers to the name of XenAPI network that you wish to have
                your instance traffic on, i.e. the network on which the VMs
                will be attached. You can either specify the bridge name,
                such an <literal>xenbr2</literal>, or the name label,
                such as <literal>vmbr</literal>.
                Specifying the name-label is very useful in cases where your
                networks are not uniform across your XenServer hosts.
            </para>
            <para>When you have a limited number of network cards on your
                server, it is possible to use networks isolated using VLANs for
                the public and network traffic. For example, if you have two
                XenServer networks <literal>xapi1</literal> and
                <literal>xapi2</literal> attached on VLAN 102 and 103
                on <literal>eth0</literal>, respectively,
                you could use these for eth1 and eth3 on
                your VM, and pass the appropriate one to
                <literal>flat_network_bridge</literal>.
             </para>
            <para>When using XenServer, it is best to use the firewall driver
                written specifically for XenServer. This pushes the firewall
                rules down to the hypervisor, rather than running them in the
                VM that is running <literal>nova-network</literal>.
            </para>
          </section>
        </section>
        
        <section
            xml:id="outbound-traffic-flow-with-any-flat-networking">
            <title>Outbound Traffic Flow with Any Flat
                Networking</title>
            <para> In any set up with FlatNetworking, the host with nova-network on it is
                responsible for forwarding traffic from the private network configured with the
                    <literal>fixed_range=...</literal> directive in <filename>nova.conf</filename>.
                This host needs to have a bridge interface (e.g., <literal>br100</literal>) configured and talking to any other
                nodes that are hosting VMs. With either of the Flat Networking options, the default
                gateway for the virtual machines is set to the host which is running nova-network. </para>
            <para> When a virtual machine sends traffic out to the
                public networks, it sends it first to its default
                gateway, which is where nova-network is configured. </para>
            <figure>
                <title>Single adaptor hosts, first route</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/SingleInterfaceOutbound_1.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Next, the host on which nova-network is configured
                acts as a router and forwards the traffic out to the
                Internet.</para>
            <figure>
                <title>Single adaptor hosts, second route</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/SingleInterfaceOutbound_2.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <warning>
                <para>If you're using a single interface, then that
                    interface (often eth0) needs to be set into
                    promiscuous mode for the forwarding to happen
                    correctly. This does not appear to be needed if
                    you're running with physical hosts that have and
                    use two interfaces.</para>
            </warning>
        </section>
        <section xml:id="configuring-vlan-networking">
            <?dbhtml stop-chunking?>
            <title>Configuring VLAN Networking</title>
            <para>Compute can be configured so that the virtual machine instances of different
                projects (tenants) are in different subnets, with each subnet having a different
                VLAN tag. This can be useful in networking environments where you have a large IP
                space which is cut up into smaller subnets. The smaller subnets are then trunked
                together at the switch level (dividing layer 3 by layer 2) so that all machines in
                the larger IP space can communicate. The purpose of this is generally to control the
                size of broadcast domains. It can also be useful to provide an additional layer of
                isolation in a multi-tenant environment.</para>
            <note>
                <para>The terms <emphasis role="italic">network</emphasis> and <emphasis
                    role="italic">subnet</emphasis> are often used interchangeably in
                    discussions of VLAN mode. In all cases, we are referring to a range of IP
                    addresses specified by a <emphasis role="italic">subnet </emphasis>(e.g.,
                    <literal>172.16.20.0/24</literal>) that are on the same VLAN (layer 2
                    <emphasis role="italic">network</emphasis>). </para>
            </note>
            <para>Running in VLAN mode is more complex than the other network modes. In particular:<itemizedlist>
                    <listitem>
                        <para>IP forwarding must be enabled</para>
                    </listitem>
                <listitem>
                    <para>The hosts running nova-network and nova-compute must have the
                                <literal>8021q</literal> kernel module loaded</para>
                </listitem>
                    <listitem>
                        <para>Your networking switches must support VLAN tagging</para>
                    </listitem>
                    <listitem>
                        <para>Your networking switches must be configured to enable the specific
                            VLAN tags you specify in your Compute setup</para>
                    </listitem>
                    <listitem>
                        <para>You will need information about your networking setup from your
                            network administrator to configure Compute properly (e.g., netmask,
                            broadcast, gateway, ethernet device, VLAN IDs)</para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>The <literal>network_manager=nova.network.manager.VlanManager</literal> option specifies VLAN
                mode, which happens to be the default networking mode. </para>
            <para>The bridges that are created by the network manager will be attached to the
                interface specified by <literal>vlan_interface</literal>, the example above uses the
                    <literal>eth0</literal> interface, which is the default. </para>
            <para>The <literal>fixed_range</literal> option is a CIDR block which describes the IP
                address space for all of the instances: this space will be divided up into subnets.
                This range is typically a <link
                    xlink:href="https://en.wikipedia.org/wiki/Private_network">private
                    network</link>. The example above uses the private range
                    <literal>172.16.0.0/12</literal>.</para>
            <para>The <literal>network_size</literal> option refers to the default number of IP
                addresses in each network, although this can be overriden at network creation time .
                The example above uses a network size of <literal>256</literal>, whicih corresponds
                to a <literal>/24</literal> network.</para>
            <para>Networks are created with the <command>nova-manage network create</command>
                command. Here is an example of how to create a network consistent with the above
                example configuration options, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network create --label=example-net --fixed_range_v4=172.16.169.0/24 --vlan=169 --bridge=br169 --project_id=a421ae28356b4cc3a25e1429a0b02e98</userinput> </screen></para>
            <para>This creates a network called <literal>example-net</literal> associated with
                tenant <literal>a421ae28356b4cc3a25e1429a0b02e98</literal>. The subnet is
                    <literal>172.16.169.0/24</literal> with a VLAN tag of <literal>169</literal>
                (the VLAN tag does not need to match the third byte of the address, though it is a
                useful convention to remember the association). This will create a bridge interface
                device called <literal>br169</literal> on the host running the nova-network service.
                This device will appear in the output of an <command>ifconfig</command>
                command.</para>
            <para>Each network is associated with one tenant. As in the example above, you may
                (optionally) specify this association at network creation time by using the
                    <literal>--project_id</literal> flag which corresponds to the tenant ID. Use the
                    <command>keystone tenant-list</command> command to list the tenants and
                corresponding IDs that you have already created.</para>
            <para>Instead of manually specifying a VLAN, bridge, and project id, you can create many
                networks at once and have the Compute service automatically associate these networks
                with tenants as needed, as well as automatically generating the VLAN IDs and bridge
                interface names. For example, the following command would create 100 networks, from
                    <literal>172.16.100.0/24</literal> to <literal>172.16.199.0/24</literal>. (This
                assumes the <literal>network_size=256</literal> option has been set at nova.conf,
                though this can also be specified by passing <literal>--network_size=256</literal>
                as a flag to the <command>nova-manage</command>
                command)<screen><prompt>#</prompt> nova-manage network create --num_networks=100 --fixed_range_v4=172.16.100.0/24 </screen></para>
            <para>The <command>nova-manage network create</command> command supports many
                configuration options, which are displayed when called with the
                    <literal>--help</literal> flag:</para>
            <programlisting>Usage: nova-manage network create &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --label=&lt;label>       Label for network (ex: public)
  --fixed_range_v4=&lt;x.x.x.x/yy>
                        IPv4 subnet (ex: 10.0.0.0/8)
  --num_networks=&lt;number>
                        Number of networks to create
  --network_size=&lt;number>
                        Number of IPs per network
  --vlan=&lt;vlan id>      vlan id
  --vpn=VPN_START       vpn start
  --fixed_range_v6=FIXED_RANGE_V6
                        IPv6 subnet (ex: fe80::/64)
  --gateway=GATEWAY     gateway
  --gateway_v6=GATEWAY_V6
                        ipv6 gateway
  --bridge=&lt;bridge>     VIFs on this network are connected to this bridge
  --bridge_interface=&lt;bridge interface>
                        the bridge is connected to this interface
  --multi_host=&lt;'T'|'F'>
                        Multi host
  --dns1=&lt;DNS Address>  First DNS
  --dns2=&lt;DNS Address>  Second DNS
  --uuid=&lt;network uuid>
                        Network UUID
  --fixed_cidr=&lt;x.x.x.x/yy>
                        IPv4 subnet for fixed IPS (ex: 10.20.0.0/16)
  --project_id=&lt;project id>
                        Project id
  --priority=&lt;number>   Network interface priority
                    </programlisting>
            <para>In particular, flags to the <command>nova-mange network create</command> command
                can be used to override settings from <filename>nova.conf</filename>:<variablelist>
                    <varlistentry>
                        <term><literal>--network_size</literal></term>
                        <listitem>
                            <para>Overrides the <literal>network_size</literal> configuration
                                option</para>
                        </listitem>
                    </varlistentry>
                    <varlistentry>
                        <term><literal>--bridge_interface</literal></term>
                        <listitem>
                            <para>Overrides the <literal>vlan_interface</literal> configuration
                                option</para>
                        </listitem>
                    </varlistentry>
                </variablelist></para>
            <para>To view a list of the networks that have been created, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network list</userinput></screen></para>
            <para>To modify an existing network, use the <command>nova-manage network
                    modify</command> command, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network modify --help</userinput>
<computeroutput>Usage: nova-manage network modify &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --fixed_range=&lt;x.x.x.x/yy>
                        Network to modify
  --project=&lt;project name>
                        Project name to associate
  --host=&lt;host>         Host to associate
  --disassociate-project
                        Disassociate Network from Project
  --disassociate-host   Disassociate Host from Project</computeroutput></screen></para>
            <para>To delete a network, use <command>nova-manage network delete</command>, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network delete --help</userinput><computeroutput>
Usage: nova-manage network delete &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --fixed_range=&lt;x.x.x.x/yy>
                        Network to delete
  --uuid=&lt;uuid>         UUID of network to delete</computeroutput></screen></para>
            <para>Note that a network must first be disassociated from a project using the
                    <command>nova-manage network modify</command> command before it can be
                deleted.</para>
            <para>Creating a network will automatically cause the Compute database to populate with
                a list of available fixed IP addresses. You can view the list of fixed IP addresses
                and their associations with active virtual machines by doing, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage fix list</userinput></screen></para>
            <para>If users need to access the instances in their project across a VPN, a special VPN
                instance (code named cloudpipe) needs to be created as described in the section
                titled <link linkend="cloudpipe-per-project-vpns">Cloudpipe — Per Project
                    VPNs</link>.
            </para>

          <section xml:id="libvirt-vlan-networking">
            <title>Libvirt VLAN networking</title>
            <para>To configure your nodes to support VLAN tagging, install the
                    <literal>vlan</literal> package and load the <literal>8021q</literal> kernel
                module, as
                root:<screen><prompt>#</prompt> <userinput>apt-get install vlan</userinput>
<prompt>#</prompt> <userinput>modprobe 8021q </userinput></screen></para>
            <para>To have this kernel module loaded on boot, add the following line to
                    <filename>/etc/modules</filename>:<programlisting>8021q</programlisting></para>
            <para>Here is an example of settings from <filename>/etc/nova/nova.conf</filename> for a
                host configured to run <command>nova-network</command> in VLAN mode</para>
            <programlisting>network_manager=nova.network.manager.VlanManager
vlan_interface=eth0
fixed_range=172.16.0.0/12
network_size=256                </programlisting>
            <para>In certain cases, the network manager may not properly tear down bridges and VLANs
                when it is stopped. If you attempt to restart the network manager and it does not
                start, check the logs for errors indicating that a bridge device already exists. If
                this is the case, you will likely need to tear down the bridge and VLAN devices
                manually. It is also advisable to kill any remaining dnsmasq processes. These
                commands would stop the service, manually tear down the bridge and VLAN from the
                previous example, kill any remaining dnsmasq processes, and start the service up
                again, as root:</para>
            <screen><prompt>#</prompt> <userinput>stop nova-network</userinput>
<prompt>#</prompt> <userinput>vconfig rem vlan169</userinput>
<prompt>#</prompt> <userinput>ip link set br169 down</userinput>
<prompt>#</prompt> <userinput>brctl delbr br169</userinput>
<prompt>#</prompt> <userinput>killall dnsmasq</userinput>
<prompt>#</prompt> <userinput>start nova-network</userinput></screen>
            </section>
            <section xml:id="xenapi-vlan-networking">
                <title>XenAPI VLAN networking</title>
                <para>VLAN networking works quite differently with the XenAPI
                    driver, compared to the libvit driver.
                    The following figure shows how your setup might look:
                </para>
                <figure xml:id="xenapi-vlan-diagram">
                    <title>VLAN network, multiple interfaces, multiple
                        servers, network HA with XenAPI driver</title>
                    <mediaobject><imageobject>
                        <imagedata scale="80" fileref="figures/XenApiVLANMultInterfaceHA.png" />
                    </imageobject></mediaobject>
                </figure>
                <para>Here is an extract from a <filename>nova.conf</filename>
                    file in a system running the above setup:</para>
                <para>
                    <programlisting>network_manager=nova.network.manager.VlanManager
xenapi_vif_driver=nova.virt.xenapi.vif.(XenAPIBridgeDriver or XenAPIOpenVswitchDriver)
vlan_interface=eth1
public_interface=eth3
multi_host=True
force_dhcp_release=True
send_arp_for_ha=True
flat_injected=False
firewall_driver=nova.virt.xenapi.firewall.Dom0IptablesFirewallDriver</programlisting>
                </para>
                <para>You should notice that
                    <literal>vlan_interface</literal>
                    refers to the network interface on the Hypervisor and
                    the network interface on the VM running the OpenStack
                    services.
                    As with before <literal>public_interface</literal>
                    refers to the network interfce on the VM running the
                    OpenStack services.
                </para>
                <para>With VLAN networking and the XenAPI driver, the following
                    things happen when you start a VM:
                    <itemizedlist>
                        <listitem>
                            <para>
                                First the XenServer network is attached to the
                                appropriate physical interface (PIF) and VLAN
                                unless the network already exsists.
                            </para>
                        </listitem>
                        <listitem>
                            <para>
                               When the VM is created, its VIF is attached
                               to the above network.
                            </para>
                        </listitem>
                        <listitem>
                            <para>
                              The 'Openstack domU', i.e. where nova-network is running,
                              acts as a gateway and DHCP for this instance.
                              The DomU does this for multiple VLAN networks,
                              so it has to be attached on a VLAN trunk.
                              For this reason it must have an interface on the
                              parent bridge of the VLAN bridge where VM instances are plugged.
                            </para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>To help understand VLAN networking with the XenAPI
                    further, here are some important things to note:
                  <itemizedlist>
                    <listitem>
                        <para>
                        A physical interface (PIF) identified either
                        by (A) the vlan_interface flag
                        or (B) the bridge_interface column in the networks db
                        table will be used for creating a XenServer VLAN network.
                        The VLAN tag is found in the vlan column, still in the
                        networks table, and by default the first tag is 100.
                        </para>
                    </listitem>
                    <listitem>
                        <para>
                        VIF for VM instances within this network will be
                        plugged in this VLAN network. You won't see the bridge
                        until a VIF is plugged in it.
                        </para>
                    </listitem>
                    <listitem>
                        <para>
                        The 'Openstack domU', i.e. the VM running the nova
                        network node, instead will not be plugged into this network;
                        since it acts as a gateway for multiple VLAN networks,
                        it has to be attached on a VLAN trunk.
                        For this reason it must have an interface on the parent
                        bridge of the VLAN bridge where VM instances are plugged.
                        For example, if <literal>vlan_interface</literal>
                        is eth0 it must be plugged in xenbr1, eth1 --> xenbr1, etc.
                        </para>
                    </listitem>
                    <listitem>
                        <para>
                        Within the Openstack domU, 'ip link' is then used to
                        configure VLAN interfaces on the 'trunk' port. Each of
                        this vlan interfaces is associated with a dnsmasq instance,
                        which will distribute IP addresses to instances.
                        The lease file for dnsmasq is constantly updated by
                        nova-network, thus ensuring VMs get the IP address
                        specified by the layer3 network driver
                        (nova IPAM or Melange).
                        </para>
                    </listitem>
                  </itemizedlist>
                </para>
                <para>With this configuration, VM instances should be able to
                get the IP address assigned to them from the appropriate dnsmasq
                instance, and should be able to communicate without any problem
                with other VMs on the same network and with the their gateway.
                </para>
                <para>The above point (3) probably needs some more explanations.
                With Open vSwitch, we don't really have distinct bridges for
                different VLANs; even if they appear as distinct bridges to linux
                and XenServer, they are actually the same OVS instance, which
                runs a distinct 'fake-bridge' for each VLAN. The 'real' bridge
                is the 'parent' of the fake one.
                You can easily navigate fake and real bridges with ovs-vsctl.
                </para>
                <para>As you can see I am referring to Openvswitch only.
                This is for a specific reason: the fake-parent mechanism
                automatically imply that ports which are not on a fake bridge
                are trunk ports. This does not happen with linux bridge.
                A packet forwarded on a VLAN interfaces does not get back
                in the xenbrX bridge for ethX.
                For this reason, with XenAPI, you must use Open vSwitch when
                running VLAN networking with network HA (i.e. mult-host) enabled.
                On XenServer 6.0 and later, Open vSwitch is the default
                network stack.
                When using VLAN networking with XenAPI and linux bridge,
                the default networking stack on XenServer prior to version 6.0,
                you must run the network node on a VM on a XenServer
                that does not host any nova-compute controlled instances.
                </para>
            </section>
            <section xml:id="vlan-known-issues">
                <title>Known issue with failed DHCP leases in VLAN configuration</title>
                <para>Text in this section was adapted from <link
                        xlink:href="https://lists.launchpad.net/openstack/msg11696.html">an email
                        from Vish Ishaya on the OpenStack mailing list</link>.</para>
                <para>There is an issue with the way Compute uses <link
                        xlink:href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq</link> in
                    VLAN mode. Compute starts up a single copy of dnsmasq for each VLAN on the
                    network host (or on every host in multi_host mode). <link
                        xlink:href="http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2011q3/005233.html"
                        >The problem</link> is in the way that dnsmasq binds to an IP  address and
                    port. Both copies can respond to broadcast packets, but unicast packets can only
                    be answered by one of the copies.</para>
                <para>As a consequence, guests from only one project will get responses to their
                    unicast DHCP renew requests. Unicast projects from guests in other projects get
                    ignored. What happens next is different depending on the guest OS. Linux
                    generally will send a broadcast packet out after the unicast fails, and so the
                    only effect is a small (tens of ms) hiccup while the interface is reconfigured.
                    It can be much worse than that, however. There have been observed cases where
                    Windows just gives up and ends up with a non-configured interface.</para>
                <para>This bug was first noticed by some users of OpenStack who rolled their own
                    fix. In short, on Linux, if you set the <literal>SO_BINDTODEVICE</literal>
                    socket option, it will allow different daemons to share the port and respond to
                    unicast packets, as long as they listen on different interfaces. Simon Kelley,
                    the maintainer of dnsmasq, <link
                        xlink:href="http://thekelleys.org.uk/gitweb/?p=dnsmasq.git;a=commitdiff;h=9380ba70d67db6b69f817d8e318de5ba1e990b12"
                        >has integrated a fix</link> for the issue in dnsmaq version 2.61. </para>
                <para>If upgrading dnsmasq is out of the question, a possible workaround is to
                    minimize lease renewals with something like the following combination of config
                    options.
                    <programlisting># release leases immediately on terminate
force_dhcp_release
# one week lease time
dhcp_lease_time=604800
# two week disassociate timeout
fixed_ip_disassociate_timeout=1209600</programlisting></para>
            </section>
        </section>
        <section xml:id="cloudpipe-per-project-vpns">
            <title>Cloudpipe — Per Project VPNs</title>
            <?dbhtml stop-chunking?>
            <para> Cloudpipe is a method for connecting end users to
                their project instances in VLAN networking mode. </para>
            <para> The support code for cloudpipe implements admin
                commands (via an extension) to automatically create a
                VM for a project that allows users to VPN into the
                private network of their project. Access to this VPN
                is provided through a public port on the network host
                for the project. This allows users to have free access
                to the virtual machines in their project without
                exposing those machines to the public internet. </para>
            <para> The cloudpipe image is basically just a Linux
                instance with openvpn installed. It needs a simple
                script to grab user data from the metadata server, b64
                decode it into a zip file, and run the autorun.sh
                script from inside the zip. The autorun script will
                configure and run openvpn to run using the data from
                nova. </para>
            <para> It is also useful to have a cron script that will
                periodically redownload the metadata and copy the new
                crl. This will keep revoked users from connecting and
                will disconnect any users that are connected with
                revoked certificates when their connection is
                renegotiated (every hour). </para>
            <section xml:id="creating-a-cloudpipe-image">
                
                <title>Creating a Cloudpipe Image</title>
                <para>To make a cloudpipe image: </para>
                <itemizedlist>
                    <listitem>
                        <para>install openvpn on a base ubuntu image.
                        </para>
                    </listitem>
                    <listitem>
                        <para>set up a server.conf.template in
                                <filename>/etc/openvpn/</filename></para>
                    </listitem>
                    <listitem>
                        <para>set up.sh in
                                <filename>/etc/openvpn/</filename>
                        </para>
                    </listitem>
                    <listitem>
                        <para>set down.sh in <filename>/etc/openvpn/
                            </filename></para>
                    </listitem>
                    <listitem>
                        <para>download and run the payload on boot
                            from
                            <filename>/etc/rc.local</filename></para>
                    </listitem>
                    <listitem>
                        <para>setup
                                <filename>/etc/network/interfaces</filename>
                        </para>
                    </listitem>
                    <listitem>
                        <para>upload the image and set the image id in
                            your config file: </para>
                        <programlisting>
vpn_image_id=[uuid from glance]
                        </programlisting>
                    </listitem>
                    <listitem>
                        <para>you should set a few other configuration
                            options to make VPNs work properly: </para>
                        <programlisting>
use_project_ca=True
cnt_vpn_clients=5
force_dhcp_release
                        </programlisting>
                    </listitem>
                </itemizedlist>
                <para> When you use the os-cloudpipe extension (POST
                    v2/{tenant_id}/os-cloudpipe) or the nova client
                        (<userinput>nova cloudpipe-create
                            <replaceable>[project_id]</replaceable></userinput>)
                    to launch a VPN for a user it goes through the
                    following process: </para>
                <orderedlist>
                    <listitem>
                        <para> creates a keypair called
                                   <literal><replaceable>&lt;project_id&gt;</replaceable>-vpn</literal>
                            and saves it in the keys directory </para>
                    </listitem>
                    <listitem>
                        <para> creates a security group
                                   <literal><replaceable>&lt;project_id&gt;</replaceable>-vpn</literal>
                            and opens up 1194 and icmp </para>
                    </listitem>
                    <listitem>
                        <para> creates a cert and private key for the
                            VPN instance and saves it in the
                                   <literal>CA/projects/<replaceable>&lt;project_id&gt;</replaceable>/
                                directory</literal>
                        </para>
                    </listitem>
                    <listitem>
                        <para> zips up the info and puts it b64
                            encoded as user data </para>
                    </listitem>
                    <listitem>
                        <para> launches an
                                <replaceable>[vpn_instance_type]</replaceable>
                            instance with the above settings using the
                            option-specified VPN image</para>
                    </listitem>
                </orderedlist>
            </section>
            <section xml:id="vpn-access">
                <title>VPN Access</title>
                <para> In VLAN networking mode, the second IP in each
                    private network is reserved for the cloudpipe
                    instance. This gives a consistent IP to the
                    instance so that nova-network can create
                    forwarding rules for access from the outside
                    world. The network for each project is given a
                    specific high-numbered port on the public IP of
                    the network host. This port is automatically
                    forwarded to 1194 on the VPN instance. </para>
                <para> If specific high numbered ports do not work for
                    your users, you can always allocate and associate
                    a public IP to the instance, and then change the
                        <literal>vpn_public_ip</literal> and
                        <literal>vpn_public_port</literal> in the
                    database. Rather than using the database directly,
                    you can also use <command>nova-manage vpn change
                            <replaceable>[new_ip]</replaceable>
                        <replaceable>[new_port]</replaceable></command>
                </para>
            </section>
            <section xml:id="certificates-and-revocation">
                <title>Certificates and Revocation</title>
                <para>For certificate management, it is also useful to have a cron script that will
                    periodically download the metadata and copy the new Certificate Revocation List
                    (CRL). This will keep revoked users from connecting and disconnects any users
                    that are connected with revoked certificates when their connection is
                    re-negotiated (every hour). You set the use_project_ca option in nova.conf for
                    cloudpipes to work securely so that each project has its own Certificate
                    Authority (CA).</para>
                <para>If the <literal>use_project_ca config</literal> option is set (required to for
                    cloudpipes to work securely), then each project has its own CA. This CA is used
                    to sign the certificate for the vpn, and is also passed to the user for bundling
                    images. When a certificate is revoked using nova-manage, a new Certificate
                    Revocation List (crl) is generated. As long as cloudpipe has an updated crl, it
                    will block revoked users from connecting to the vpn. </para>
                <para> The userdata for cloudpipe isn't currently
                    updated when certs are revoked, so it is necessary
                    to restart the cloudpipe instance if a user's
                    credentials are revoked. </para>
            </section>
            <section
                xml:id="restarting-and-logging-into-cloudpipe-vpn">
                <title>Restarting and Logging into the Cloudpipe
                    VPN</title>
                <para>You can reboot a cloudpipe vpn through the api
                    if something goes wrong (using <command>nova
                        reboot</command> for example), but if you
                    generate a new crl, you will have to terminate it
                    and start it again using the cloudpipe extension.
                    The cloudpipe instance always gets the first ip in
                    the subnet and if force_dhcp_release is not set it
                    takes some time for the ip to be recovered. If you
                    try to start the new vpn instance too soon, the
                    instance will fail to start because of a
                    "NoMoreAddresses" error. It is therefore
                    recommended to use
                        <literal>force_dhcp_release</literal>.</para>
                <para>The keypair that was used to launch the
                    cloudpipe instance should be in the
                            <filename>keys/<replaceable>&lt;project_id&gt;</replaceable></filename>
                    folder. You can use this key to log into the
                    cloudpipe instance for debugging purposes. If you
                    are running multiple copies of nova-api this key
                    will be on whichever server used the original
                    request. To make debugging easier, you may want to
                    put a common administrative key into the cloudpipe
                    image that you create.</para>
            </section>
        </section>
    </section>
    <section xml:id="enabling-ping-and-ssh-on-vms">
        <title>Enabling Ping and SSH on VMs</title>
        <para>Be sure you enable access to your VMs by using the
                <command>euca-authorize</command> or <command>nova
                secgroup-add-rule</command> command. Below, you will
            find the commands to allow <command>ping</command> and
                <command>ssh</command> to your VMs: </para>
        <note>
            <para>These commands need to be run as root only if the
                credentials used to interact with nova-api have been
                put under <filename>/root/.bashrc</filename>. If the
                EC2 credentials have been put into another user's
                    <filename>.bashrc</filename> file, then, it is
                necessary to run these commands as the user. </para>
        </note>
        <para>Using the nova command-line tool:</para>
        <screen>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default icmp -1 -1 -s 0.0.0.0/0</userinput>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default tcp 22 22 -s 0.0.0.0/0</userinput>
            </screen>
        <para>Using euca2ools:</para>
        <screen>
<prompt>$</prompt> <userinput>euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default</userinput>
<prompt>$</prompt> <userinput>euca-authorize -P tcp -p 22 -s 0.0.0.0/0 default</userinput>
            </screen>

        <para>If you still cannot ping or SSH your instances after
            issuing the <command>nova secgroup-add-rule</command>
            commands, look at the number of <literal>dnsmasq</literal>
            processes that are running. If you have a running
            instance, check to see that TWO <literal>dnsmasq</literal>
            processes are running. If not, perform the following as
            root:</para>
        <screen>
<prompt>#</prompt> <userinput>killall dnsmasq</userinput>
<prompt>#</prompt> <userinput>service nova-network restart</userinput>
           </screen>
    </section>
    <section xml:id="associating-public-ip">
        <title>Configuring Public (Floating) IP Addresses</title>
        <?dbhtml stop-chunking?>
        <section xml:id="Private_and_Public_IP_Addresses">
            <title>Private and Public IP Addresses</title>
            <para>Every virtual instance is automatically assigned a
                private IP address. You may optionally assign public
                IP addresses to instances. OpenStack uses the term
                "floating IP" to refer to an IP address (typically
                public) that can be dynamically added to a running
                virtual instance. OpenStack Compute uses Network
                Address Translation (NAT) to assign floating IPs to
                virtual instances. </para>
            <para>If you plan to use this feature, you must add the
                following to your nova.conf file to specify which
                interface the nova-network service will bind public IP
                addresses to:</para>
            <programlisting>
public_interface=vlan100
        </programlisting>
            <para>Restart the nova-network service if you change
                nova.conf while the service is running.</para>
        </section>
        <section
            xml:id="Creating_a_List_of_Available_Floating_IP_Addresses">
            <title>Creating a List of Available Floating IP
                Addresses</title>
            <para>Nova maintains a list of floating IP addresses that
                are available for assigning to instances. Use the
                    <command>nova-manage floating create</command>
                command to add entries to this list, as root.</para>
            <para>For example:</para>
            <screen>
<prompt>#</prompt> <userinput>nova-manage floating create --ip_range=68.99.26.170/31</userinput>
        </screen>
            <para>The following nova-manage commands apply to floating
                IPs.</para>
            <itemizedlist>
                <listitem>
                    <para><command>nova-manage floating
                        list</command>: List the floating IP addresses
                        in the pool.</para>
                </listitem>
                <listitem>
                    <para><command>nova-manage floating create
                            [cidr]</command>: Create specific floating
                        IPs for either a single address or a
                        subnet.</para>
                </listitem>
                <listitem>
                    <para><command>nova-manage floating delete
                            [cidr]</command>: Remove floating IP
                        addresses using the same parameters as the
                        create command.</para>
                </listitem>
            </itemizedlist>

        </section>
        <section xml:id="Adding_a_Floating_IP_to_an_Instance">
            <title>Adding a Floating IP to an Instance</title>
            <para>Adding a floating IP to an instance is a two step
                process:</para>
            <orderedlist>
                <listitem>
                    <para><command>nova floating-ip-create</command>:
                        Allocate a floating IP address from the list
                        of available addresses.</para>
                </listitem>
                <listitem>
                    <para><command>nova add-floating-ip</command>: Add
                        an allocated floating IP address to a running
                        instance.</para>
                </listitem>
            </orderedlist>

            <para>Here's an example of how to add a floating IP to a
                running instance with an ID of 12</para>
            <screen>
<prompt>$</prompt> <userinput>nova floating-ip-create</userinput>
<computeroutput>
+-----------------+-------------+----------+------+
|        Ip       | Instance Id | Fixed Ip | Pool |
+-----------------+-------------+----------+------+
|    68.99.26.170 | None        | None     |      |
+-----------------+-------------+----------+------+
</computeroutput>
<prompt>$</prompt> <userinput>nova add-floating-ip 12 68.99.26.170</userinput>
        </screen>

            <para>If the instance no longer needs a public address,
                remove the floating IP address from the instance and
                de-allocate the address:</para>
            <screen>
<prompt>$</prompt> <userinput>nova remove-floating-ip 12 68.99.26.170</userinput>
<prompt>$</prompt> <userinput>nova floating-ip-delete 68.99.26.170</userinput>
    </screen>
        </section>

        <section xml:id="Automatically_adding_floating_IPs">
            <title>Automatically adding floating IPs</title>
            <para>The nova-network service can be configured to
                automatically allocate and assign a floating IP
                address to virtual instances when they are launched.
                Add the following line to nova.conf and restart the
                nova-network service</para>

            <programlisting>
auto_assign_floating_ip=True
            </programlisting>
            <para>Note that if this option is enabled and all of the
                floating IP addresses have already been allocated, the
                    <command>nova boot</command> command will fail
                with an error.</para>
        </section>
    </section>
    <section xml:id="removing-network-from-project">
        <title>Removing a Network from a Project</title>
        <para>You will find that you cannot remove a network that has
            already been associated to a project by simply deleting
            it. You can disassociate the project from the network with
            a scrub command and the project name as the final
            parameter: </para>
        <screen>
<prompt>$</prompt> <userinput>nova-manage project scrub projectname</userinput>
        </screen>
    </section>
    <section xml:id="using-multi-nics">
        <title>Using multiple interfaces for your instances
            (multinic)</title>
        <?dbhtml stop-chunking?>
        <para> The multi-nic feature allows you to plug more than one
            interface to your instances, making it possible to make
            several use cases available : <itemizedlist>
                <listitem>
                    <para>SSL Configurations (VIPs)</para>
                </listitem>
                <listitem>
                    <para>Services failover/ HA</para>
                </listitem>
                <listitem>
                    <para>Bandwidth Allocation</para>
                </listitem>
                <listitem>
                    <para>Administrative/ Public access to your
                        instances</para>
                </listitem>
            </itemizedlist> Each VIF is representative of a separate
            network with its own IP block. Every network mode
            introduces it's own set of changes regarding the mulitnic
            usage : <figure>
                <title>multinic flat manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40" fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure>
            <figure>
                <title>multinic flatdhcp manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40" fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure>
            <figure>
                <title>multinic VLAN manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40" fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure>
        </para>
        <section xml:id="using-multiple-nics-usage">
            <title>Using the multinic feature</title>
            <para> In order to use the multinic feature, first create two networks, and attach them
                to your project :
                <screen><prompt>$</prompt> <userinput>nova-manage network create --fixed_range_v4=20.20.0.0/24 --num_networks=1 --network_size=256 --label=first-net --project=$your-project</userinput>
<prompt>$</prompt> <userinput>nova-manage network create --fixed_range_v4=20.20.10.0/24 --num_networks=1 --network_size=256 --label=second-net --project=$your-project</userinput>              </screen>
                Now every time you spawn a new instance, it gets two IP addresses from the
                respective DHCP servers : <screen><prompt>$</prompt> <userinput>nova list</userinput>
<computeroutput>+-----+------------+--------+----------------------------------------+
 |  ID |    Name    | Status |                Networks                |
 +-----+------------+--------+----------------------------------------+
 | 124 | Server 124 | ACTIVE | network2=20.20.0.3; private=20.20.10.14|
 +-----+------------+--------+----------------------------------------+</computeroutput></screen>
                <note>
                    <para>Make sure to power up the second interface on the instance, otherwise that
                        last won't be reacheable via its second IP. Here is an example of how to
                        setup the interfaces within the instance (this is the configuration that
                        needs to be applied inside the image) : </para>
                    <para><filename>/etc/network/interfaces</filename>
                        <programlisting># The loopback network interface
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet dhcp                </programlisting></para>
                </note></para>
            <note>
            <para>If the Virtual Network Service Quantum is installed, it is possible to specify the
                networks to attach to the respective interfaces by using the
                    <literal>--nic</literal> flag when invoking the <literal>nova</literal> command
                :
                <screen><prompt>$</prompt> <userinput>nova boot --image ed8b2a37-5535-4a5f-a615-443513036d71 --flavor 1 --nic net-id= &lt;id of first network&gt;  --nic net-id= &lt;id of first network&gt;  test-vm1</userinput></screen>
            </para>
            </note>
        </section>
    </section>
    <section xml:id="existing-ha-networking-options">
        <title>Existing High Availability Options for
            Networking</title>
        <para>Adapted from a blog post by<link xlink:href="http://unchainyourbrain.com/openstack/13-networking-in-nova">Vish
                Ishaya</link></para>

        <para>As illustrated in the Flat DHCP diagram in Section <link xlink:href="#configuring-flat-dhcp-networking">Configuring Flat DHCP
                Networking</link> titled <link linkend="flat-dhcp-diagram">Flat DHCP network, multiple interfaces, multiple servers</link>,
            traffic from the VM to the public internet has to go through the host running nova
            network. DHCP is handled by nova-network as well, listening on the gateway address of
            the fixed_range network. The compute hosts can optionally have their own public IPs, or
            they can use the network host as their gateway. This mode is pretty simple and it works
            in the majority of situations, but it has one major drawback: the network host is a
            single point of failure! If the network host goes down for any reason, it is impossible
            to communicate with the VMs. Here are some options for avoiding the single point of
            failure.</para>
        <simplesect>
            <title>HA Option 1: Multi-host</title>
            <para>To eliminate the network host as a single point of failure, Compute can be
                configured to allow each compute host to do all of the networking jobs for its own
                VMs. Each compute host does NAT, DHCP, and acts as a gateway for all of its own VMs.
                While there is still a single point of failure in this scenario, it is the same
                point of failure that applies to all virtualized systems.</para>

            <para>This setup requires adding an IP on the VM network to each host in the system, and
                it implies a little more overhead on the compute hosts. It is also possible to
                combine this with option 4 (HW Gateway) to remove the need for your compute hosts to
                gateway. In that hybrid version they would no longer gateway for the VMs and their
                responsibilities would only be DHCP and NAT.</para>
            <para>The resulting layout for the new HA networking
                option looks the following diagram:</para>
            <para><figure>
                <title>High Availability Networking Option</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="50"
                            fileref="figures/ha-net.jpg"/>
                    </imageobject>
                </mediaobject>
            </figure></para>
            <para>In contrast with the earlier diagram, all the hosts in the system are running the
                nova-compute, nova-network and nova-api services. Each host does DHCP and does NAT
                for public traffic for the VMs running on that particular host. In this model every
                compute host requires a connection to the public internet and each host is also
                assigned an address from the VM network where it listens for DHCP traffic. The
                nova-api service is needed so that it can act as a metadata server for the
                instances.</para>
            <para>To run in HA mode, each compute host must run the following services:<itemizedlist>
                    <listitem>
                        <para><command>nova-compute</command></para>
                    </listitem>
                    <listitem>
                        <para><command>nova-network</command></para>
                    </listitem>
                    <listitem>
                        <para><command>nova-api-metadata</command> or
                            <command>nova-api</command></para>
                    </listitem>
                </itemizedlist></para>
            <para>If the compute host is not an API endpoint, use the
                    <command>nova-api-metadata</command> service. The <filename>nova.conf</filename>
                file should contain:<programlisting>multi_host=True</programlisting></para>
            <para>If a compute host is also an API endpoint, use the <command>nova-api</command>
                service. Your <literal>enabled_apis</literal> option will need to contain
                    <literal>metadata</literal>, as well as additional options depending on the API
                services. For example, if it supports compute requests, volume requests, and EC2
                compatibility, the <filename>nova.conf</filename> file should contain:
                <programlisting>multi_host=True
enabled_apis=ec2,osapi_compute,osapi_volume,metadata</programlisting></para>

            <para>The <literal>multi_host</literal> option must be in place for network creation and
                nova-network must be run on every compute host. These created multi hosts networks
                will send all network related commands to the host that the VM is on. You need to
                set the configuration option <literal>enabled_apis</literal> such that it includes
                    <literal>metadata</literal> in the list of enabled APIs. </para>
            <note><para>You must specify the <literal>multi_host</literal> option
            on the command line when creating fixed networks. For example:
            <screen>
              <prompt>#</prompt> <userinput> nova-manage network create --fixed_range_v4=192.168.0.0/24 --num_networks=1 --network_size=256 --multi_host=T --label=test</userinput>
            </screen></para>
            </note>
        </simplesect>

        <simplesect>
            <title>HA Option 2: Failover</title>
            <para>The folks at NTT labs came up with a ha-linux
                configuration that allows for a 4 second failover to a
                hot backup of the network host. Details on their
                approach can be found in the following post to the
                openstack mailing list: <link
                    xlink:href="https://lists.launchpad.net/openstack/msg02099.html"
                    >https://lists.launchpad.net/openstack/msg02099.html</link></para>
            <para>This solution is definitely an option, although it
                requires a second host that essentially does nothing
                unless there is a failure. Also four seconds can be
                too long for some real-time applications.</para>
            <para>To enable this HA option, your <filename>nova.conf</filename> file must contain
                the following option:<programlisting>send_arp_for_ha=True</programlisting></para>
            <para>See <link xlink:href="https://bugs.launchpad.net/nova/+bug/782364"
                    >https://bugs.launchpad.net/nova/+bug/782364</link> for details on why this
                option is required when configuring for failover.</para>
        </simplesect>
        <simplesect>
            <title>HA Option 3: Multi-nic</title>
            <para>Recently, nova gained support for multi-nic. This
                allows us to bridge a given VM into multiple networks.
                This gives us some more options for high availability.
                It is possible to set up two networks on separate
                vlans (or even separate ethernet devices on the host)
                and give the VMs a NIC and an IP on each network. Each
                of these networks could have its own network host
                acting as the gateway.</para>
            <para>In this case, the VM has two possible routes out. If
                one of them fails, it has the option of using the
                other one. The disadvantage of this approach is it
                offloads management of failure scenarios to the guest.
                The guest needs to be aware of multiple networks and
                have a strategy for switching between them. It also
                doesn't help with floating IPs. One would have to set
                up a floating IP associated with each of the IPs on
                private the private networks to achieve some type of
                redundancy.</para>
        </simplesect>
        <simplesect>
            <title>HA Option 4: Hardware gateway</title>
            <para>The <systemitem class="service">dnsmasq</systemitem> service can be configured to
                use an external gateway instead of acting as the gateway for the VMs. This offloads
                HA to standard switching hardware and it has some strong benefits. Unfortunately,
                the <systemitem class="service">nova-network</systemitem> service is still
                responsible for floating IP natting and DHCP, so some failover strategy needs to be
                employed for those options. To configure for hardware gateway:<orderedlist>
                    <listitem>
                        <para>Create a <systemitem class="service">dnsmasq</systemitem>
                            configuration file (e.g., <filename>/etc/dnsmasq-nova.conf</filename>)
                            that contains the IP address of the external gateway. If running in
                            FlatDHCP mode, assuming the IP address of the hardware gateway was
                            172.16.100.1, the file would contain the
                            line:<programlisting>dhcp-option=option:router,<replaceable>172.16.100.1</replaceable></programlisting></para>
                        <para>If running in VLAN mode, a separate router must be specified for each
                            network. The networks are identified by the <literal>--label</literal>
                            argument when calling <command>nova-manage network create</command> to
                            create the networks as documented in the <link
                                linkend="configuring-vlan-networking">Configuring VLAN Networking
                                subsection</link>. Assuming you have three VLANs, that are labeled
                                <literal>red</literal>, <literal>green</literal>, and
                                <literal>blue</literal>, with corresponding hardware routers at
                                <literal>172.16.100.1</literal>, <literal>172.16.101.1</literal> and
                                <literal>172.16.102.1</literal>, the <systemitem class="service"
                                >dnsmasq</systemitem>configuration file  (e.g.,
                                <filename>/etc/dnsmasq-nova.conf</filename>) would contain the
                            following:<programlisting>dhcp-option=tag:'red',option:router,172.16.100.1
dhcp-option=tag:'green',option:router,172.16.101.1
dhcp-option=tag:'blue',option:router,172.16.102.1</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Edit <filename>/etc/nova/nova.conf</filename> to specify the location
                            of the <systemitem class="service">dnsmasq</systemitem> configuration
                            file:<programlisting>dnsmasq_config_file=/etc/dnsmasq-nova.conf</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Configure the hardware gateway to forward metadata requests to a host
                            that's running the <systemitem class="service">nova-api</systemitem>
                            service with the metadata API enabled.</para>
                        <para>The virtual machine instances access the metadata service at
                                <literal>169.254.169.254</literal> port <literal>80</literal>. The
                            hardware gateway should forward these requests to a host  running the
                                <systemitem class="service">nova-api</systemitem> service on the
                            port specified as the <literal>metadata_host</literal> config option in
                                <filename>/etc/nova/nova.conf</filename>, which defaults to
                                <literal>8775</literal>.</para>
                        <para>Make sure that the list in the <literal>enabled_apis</literal>
                            configuration option <filename>/etc/nova/nova.conf</filename> contains
                                <literal>metadata</literal> in addition to the other APIs. An
                            example that contains the EC2 API, the OpenStack compute API, the
                            OpenStack volume API, and the metadata service would look like:
                            <programlisting>enabled_apis=ec2,osapi_compute,osapi_volume,metadata</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Ensure you have set up routes properly so that the subnet that you use
                            for virtual machines is routable.</para>
                    </listitem>
                </orderedlist></para>
        </simplesect>

    </section>
    <section xml:id="network-troubleshooting">
        <title>Troubleshooting Networking</title>
        <simplesect><title>Can't reach floating IPs</title>
        <para>If you aren't able to reach your instances via the
            floating IP address, make sure the default security
            group allows ICMP (ping) and SSH (port 22), so that
            you can reach the instances:</para>
        <screen>
<prompt>$</prompt> <userinput>nova secgroup-list-rules default</userinput>
<computeroutput>
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+
</computeroutput>
            </screen>
        <para>Ensure the NAT rules have been added to iptables on
            the node that nova-network is running on, as
            root:</para>

        <screen>
<prompt>#</prompt> <userinput>iptables -L -nv</userinput>
<computeroutput>
 -A nova-network-OUTPUT -d 68.99.26.170/32 -j DNAT --to-destination 10.0.0.3
</computeroutput>
<prompt>#</prompt> <userinput>iptables -L -nv -t nat</userinput>
<computeroutput>
-A nova-network-PREROUTING -d 68.99.26.170/32 -j DNAT --to-destination10.0.0.3
-A nova-network-floating-snat -s 10.0.0.3/32 -j SNAT --to-source 68.99.26.170
</computeroutput>
            </screen>

        <para>Check that the public address, in this example
            "68.99.26.170", has been added to your public
            interface: You should see the address in the listing
            when you enter "ip addr" at the command prompt.</para>

        <screen>
<prompt>$</prompt> <userinput>ip addr</userinput>
<computeroutput>
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
link/ether xx:xx:xx:17:4b:c2 brd ff:ff:ff:ff:ff:ff
inet 13.22.194.80/24 brd 13.22.194.255 scope global eth0
inet 68.99.26.170/32 scope global eth0
inet6 fe80::82b:2bf:fe1:4b2/64 scope link
valid_lft forever preferred_lft forever
</computeroutput>
            </screen>

        <para>Note that you cannot SSH to an instance with a
            public IP from within the same server as the routing
            configuration won't allow it. </para>
            <para>You can use <command>tcpdump</command> to identify if packets are being routed to
                the inbound interface on the compute host. If the packets are reaching the compute
                hosts but the connection is failing, the issue may be that the packet is being
                dropped by reverse path filtering. Try disabling reverse path filtering on the
                inbound interface. For example, if the inbound interface is <literal>eth2</literal>,
                as
                root:<screen><prompt>#</prompt> <userinput>sysctl -w net.ipv4.conf.<replaceable>eth2</replaceable>.rp_filter=0</userinput></screen></para>
            <para>If this solves your issue, add the following line to
                    <filename>/etc/sysctl.conf</filename> so that the revesrse path filter will be
                disabled the next time the compute host
                reboots:<programlisting>net.ipv4.conf.rp_filter=0</programlisting></para>

        </simplesect>
        <simplesect><title>Disabling firewall</title>
        <para>To help debug networking issues with reaching VMs, you can disable the firewall by
            setting the following option in
            <filename>/etc/nova/nova.conf</filename>:<programlisting>firewall_driver=nova.virt.firewall.NoopFirewallDriver</programlisting></para>
        <para>We strongly recommend you remove the above line to re-enable the firewall once your
            networking issues have been resolved.</para>
        </simplesect>
        <simplesect><title>Packet loss from instances to nova-network server (VLANManager mode)</title>
            <para>If you can SSH to your instances but you find that the network interactions to
                your instance is slow, or if you find that running certain operations are slower
                than they should be (e.g., <command>sudo</command>), then there may be packet loss
                occurring on the connection to the instance.</para>
            <para>Packet loss can be caused by Linux networking configuration settings related to
                bridges. Certain settings can cause packets to be dropped between the VLAN interface
                (e.g., <literal>vlan100</literal>) and the associated bridge interface (e.g.,
                    <literal>br100</literal>) on the host running the nova-network service.</para>
            <para>One way to check if this is the issue in your setup is to open up three terminals
                and run the following commands:</para>
            <para>In the first terminal, on the host running nova-network, use
                    <command>tcpdump</command> to monitor DNS-related traffic (UDP, port 53) on the
                VLAN interface. As
                root:<screen><prompt>#</prompt> <userinput>tcpdump -K -p -i vlan100 -v -vv udp port 53</userinput></screen></para>
            <para>In the second terminal, also on the host running nova-network, use
                    <command>tcpdump</command> to monitor DNS-related traffic on the bridge
                interface. As
                root:<screen><prompt>#</prompt> <userinput>tcpdump -K -p -i br100 -v -vv udp port 53</userinput></screen></para>
            <para>In the third terminal, SSH inside of the instance and generate DNS requests by
                using the <command>nslookup</command>
                command:<screen><prompt>$</prompt> <userinput>nslookup www.google.com</userinput></screen></para>
            <para>The symptoms may be intermittent, so try running <command>nslookup</command>
                multiple times. If the network configuration is correct, the command should return
                immediately each time. If it is not functioning properly, the command will hang for
                several seconds.</para>
            <para>If the <command>nslookup</command> command somteimes hangs, and there are packets
                that appear in the first terminal but not the second, then the problem may be due to
                filtering done on the bridges. Try to disable filtering, as
                root:<screen><prompt>#</prompt> <userinput>sysctl -w net.bridge.bridge-nf-call-arptables=0</userinput>
<prompt>#</prompt> <userinput>sysctl -w net.bridge.bridge-nf-call-iptables=0</userinput>
<prompt>#</prompt> <userinput>sysctl -w net.bridge.bridge-nf-call-ip6tables=0</userinput></screen></para>
            <para>If this solves your issue, add the following line to
                    <filename>/etc/sysctl.conf</filename> so that these changes will take effect the
                next time the host
                reboots:<programlisting>net.bridge.bridge-nf-call-arptables=0
net.bridge.bridge-nf-call-iptables=0
net.bridge.bridge-nf-call-ip6tables=0</programlisting></para>
        </simplesect>
        <simplesect><title>KVM: Network connectivity works initially, then fails</title>
        <para>Some administrators have observed an issue with the KVM hypervisor where instances
                running Ubuntu 12.04 will sometimes lose network connectivity after functioning
                properly for a period of time. Some users have reported success with loading the
                vhost_net kernel module as a workaround for this issue (see <link
                    xlink:href="https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/997978/">bug
                    #997978</link>) . This kernel module may also <link
                    xlink:href="http://www.linux-kvm.org/page/VhostNet">improve network performance
                    on KVM</link>. To load the kernel module, as
                root:<screen><prompt>#</prompt> <userinput>modprobe vhost_net</userinput></screen></para>
            <para>Note that loading the module will have no effect on instances that are already
                running.</para></simplesect>
    </section>

</chapter>
