# Akihiro Motoki <amotoki@gmail.com>, 2016. #zanata
# KATO Tomoyuki <kato.tomoyuki@jp.fujitsu.com>, 2016. #zanata
# Yuta Hono <yuta.hono@ntt.com>, 2016. #zanata
# KATO Tomoyuki <kato.tomoyuki@jp.fujitsu.com>, 2017. #zanata
msgid ""
msgstr ""
"Project-Id-Version: High Availability Guide 15.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-07-27 15:25+0000\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"PO-Revision-Date: 2017-03-22 02:26+0000\n"
"Last-Translator: KATO Tomoyuki <kato.tomoyuki@jp.fujitsu.com>\n"
"Language-Team: Japanese\n"
"Language: ja\n"
"X-Generator: Zanata 3.9.6\n"
"Plural-Forms: nplurals=1; plural=0\n"

msgid "**Cluster Address**: List the IP addresses for each cluster node."
msgstr "**クラスターアドレス**: 各クラスターノードの IP アドレスを表示します。"

msgid "**Cluster Name**: Define an arbitrary name for your cluster."
msgstr "**クラスター名**: 任意のクラスターの名前を定義します。"

msgid ""
"**Corosync configuration file fragment for unicast (``corosync.conf``)**"
msgstr "**ユニキャスト向け Corosync 設定ファイルの断片 (``corosync.conf``)**"

msgid ""
"**Example Corosync configuration file for multicast (``corosync.conf``)**"
msgstr "**マルチキャスト用の Corosync 設定ファイル例 (``corosync.conf``)**"

msgid "**Node Address**: Define the IP address of the cluster node."
msgstr "**ノードアドレス**: クラスターノードの IP アドレスを定義します。"

msgid "**Node Name**: Define the logical name of the cluster node."
msgstr "**ノード名**: クラスターノードの論理名を定義します。"

msgid ""
"**wsrep Provider**: The Galera Replication Plugin serves as the ``wsrep`` "
"provider for Galera Cluster. It is installed on your system as the "
"``libgalera_smm.so`` file. Define the path to this file in your ``my.cnf``:"
msgstr ""
"**wsrep Provider**: Galera Replication Plugin は、Galera Cluster の "
"``wsrep`` プロバイダーとして動作します。お使いのシステムに ``libgalera_smm."
"so`` ファイルとしてインストールされます。このファイルへのパスを ``my.cnf`` に"
"定義します。"

msgid "/etc/neutron/neutron.conf parameters for high availability"
msgstr "高可用性のための /etc/neutron/neutron.conf のパラメーター"

msgid "12 GB"
msgstr "12 GB"

msgid "12+ GB"
msgstr "12+ GB"

msgid "120 GB"
msgstr "120 GB"

msgid "120+ GB"
msgstr "120+ GB"

msgid "2"
msgstr "2"

msgid "2 or more"
msgstr "2 以上"

msgid "4"
msgstr "4"

msgid "8+"
msgstr "8+"

msgid ":doc:`Networking DHCP agent<networking-ha-dhcp>`"
msgstr ":doc:`Networking DHCP エージェント <networking-ha-dhcp>`"

msgid ":doc:`Neutron L3 agent<networking-ha-l3>`"
msgstr ":doc:`Networking L3 エージェント <networking-ha-l3>`"

msgid ""
":ref:`Configure OpenStack services to use RabbitMQ HA queues <rabbitmq-"
"services>`"
msgstr ""
":ref:`Configure OpenStack services to use RabbitMQ HA queues <rabbitmq-"
"services>`"

msgid ":ref:`Configure RabbitMQ for HA queues<rabbitmq-configure>`"
msgstr ":ref:`高可用性 キュー用の RabbitMQ の設定 <rabbitmq-configure>`"

msgid ":ref:`Install RabbitMQ<rabbitmq-install>`"
msgstr ":ref:`RabbitMQ のインストール<rabbitmq-install>`"

msgid ":ref:`corosync-multicast`"
msgstr ":ref:`corosync-multicast`"

msgid ":ref:`corosync-unicast`"
msgstr ":ref:`corosync-unicast`"

msgid ":ref:`corosync-votequorum`"
msgstr ":ref:`corosync-votequorum`"

msgid ":ref:`glance-api-configure`"
msgstr ":ref:`glance-api-configure`"

msgid ":ref:`glance-api-pacemaker`"
msgstr ":ref:`glance-api-pacemaker`"

msgid ":ref:`glance-services`"
msgstr ":ref:`glance-services`"

msgid ":ref:`ha-blockstorage-configure`"
msgstr ":ref:`ha-blockstorage-configure`"

msgid ":ref:`ha-blockstorage-pacemaker`"
msgstr ":ref:`ha-blockstorage-pacemaker`"

msgid ":ref:`ha-blockstorage-services`"
msgstr ":ref:`ha-blockstorage-services`"

msgid ":ref:`ha-sharedfilesystems-configure`"
msgstr ":ref:`ha-sharedfilesystems-configure`"

msgid ":ref:`ha-sharedfilesystems-pacemaker`"
msgstr ":ref:`ha-sharedfilesystems-pacemaker`"

msgid ":ref:`ha-sharedfilesystems-services`"
msgstr ":ref:`ha-sharedfilesystems-services`"

msgid ":ref:`identity-config-identity`"
msgstr ":ref:`identity-config-identity`"

msgid ":ref:`identity-pacemaker`"
msgstr ":ref:`identity-pacemaker`"

msgid ":ref:`identity-services-config`"
msgstr ":ref:`identity-services-config`"

msgid ":ref:`pacemaker-cluster-properties`"
msgstr ":ref:`pacemaker-cluster-properties`"

msgid ":ref:`pacemaker-corosync-setup`"
msgstr ":ref:`pacemaker-corosync-setup`"

msgid ":ref:`pacemaker-corosync-start`"
msgstr ":ref:`pacemaker-corosync-start`"

msgid ":ref:`pacemaker-install`"
msgstr ":ref:`pacemaker-install`"

msgid ":ref:`pacemaker-start`"
msgstr ":ref:`pacemaker-start`"

msgid ""
":term:`Advanced Message Queuing Protocol (AMQP)` provides OpenStack internal "
"stateful communication service."
msgstr ""
":term:`Advanced Message Queuing Protocol (AMQP)` は、OpenStack 内部のステート"
"フルな通信サービスを提供します。"

msgid ":term:`active/active configuration`"
msgstr ":term:`アクティブ/アクティブ設定 <active/active configuration>`"

msgid ":term:`active/passive configuration`"
msgstr ":term:`アクティブ/パッシブ設定 <active/passive configuration>`"

msgid ""
"A crucial aspect of high availability is the elimination of single points of "
"failure (SPOFs). A SPOF is an individual piece of equipment or software that "
"causes system downtime or data loss if it fails. In order to eliminate "
"SPOFs, check that mechanisms exist for redundancy of:"
msgstr ""
"高可用性の重要な側面は、単一障害点 (SPOF) を減らすことです。SPOF は、障害が発"
"生した場合にシステム停止やデータ損失を引き起こす、設備やソフトウェアの個々の"
"部品です。SPOF を削減するために、以下の冗長性に対するメカニズムを確認します。"

msgid "A minimum of three hosts"
msgstr "最小 3 ノード"

msgid ""
"A sample votequorum service configuration in the :file:`corosync.conf` file "
"is:"
msgstr ":file:`corosync.conf` ファイルの votequorum サービス設定例:"

msgid ""
"A service that provides a response after your request and then requires no "
"further attention. To make a stateless service highly available, you need to "
"provide redundant instances and load balance them. OpenStack services that "
"are stateless include ``nova-api``, ``nova-conductor``, ``glance-api``, "
"``keystone-api``, ``neutron-api``, and ``nova-scheduler``."
msgstr ""
"リクエストに応答して、その後さらなる注意を必要としないサービス。ステートレス"
"なサービスを高可用化するために、複数のインスタンスを配備して、負荷分散する必"
"要があります。ステートレスな OpenStack サービスに ``nova-api``、``nova-"
"conductor``、``glance-api``、``keystone-api``、``neutron-api``、``nova-"
"scheduler`` があります。"

msgid ""
"A service where subsequent requests to the service depend on the results of "
"the first request. Stateful services are more difficult to manage because a "
"single action typically involves more than one request. Providing additional "
"instances and load balancing does not solve the problem. For example, if the "
"horizon user interface reset itself every time you went to a new page, it "
"would not be very useful. OpenStack services that are stateful include the "
"OpenStack database and message queue. Making stateful services highly "
"available can depend on whether you choose an active/passive or active/"
"active configuration."
msgstr ""
"最初のリクエストの結果に応じて、後続のリクエストがあるサービス。ステートフル"
"サービスは、あるアクションが一般的に複数のリクエストに影響するため、管理する"
"ことが難しいです。追加インスタンスを配備して負荷分散するだけでは、問題を解決"
"できません。例えば、horizon ユーザーインターフェースが、新しいページを開くた"
"びに毎回リセットされると、ほとんど役に立たないでしょう。ステートフルな "
"OpenStack サービスには、OpenStack のデータベース、メッセージキューがありま"
"す。ステートレスなサービスの高可用化には、アクティブ/パッシブまたはアクティ"
"ブ/アクティブな設定のどちらを選択するかに依存する可能性があります。"

msgid ""
"A shared implementation and calculation of `quorum <https://en.wikipedia.org/"
"wiki/Quorum_(Distributed_Systems)>`_"
msgstr ""
"`クォーラム <https://en.wikipedia.org/wiki/Quorum_(Distributed_Systems)>`_ の"
"共有実装と計算"

msgid ""
"A single application does not have sufficient context to know the difference "
"between failure of a machine and failure of the application on a machine. "
"The usual practice is to assume the machine is dead and continue working, "
"however this is highly risky. A rogue process or machine could still be "
"responding to requests and generally causing havoc. The safer approach is to "
"make use of remotely accessible power switches and/or network switches and "
"SAN controllers to fence (isolate) the machine before continuing."
msgstr ""
"単一アプリケーションは、マシンの障害とマシン上のアプリケーション障害の違いを"
"十分に理解できません。一般的なプラクティスは、マシンが停止したと仮定して、動"
"作し続けることです。しかしながら、これは非常にリスクがあります。はぐれたプロ"
"セスやマシンがリクエストに応答し続け、一般的に大破壊を引き起こし続ける可能性"
"があります。より安全なアプローチは、継続する前にマシンをフェンス (隔離) する"
"ために、リモートアクセス可能な電源スイッチ、ネットワークスイッチ、SAN コント"
"ローラーを使用することです。"

msgid ""
"A typical active/active installation for a stateful service includes "
"redundant services, with all instances having an identical state. In other "
"words, updates to one instance of a database update all other instances. "
"This way a request to one instance is the same as a request to any other. A "
"load balancer manages the traffic to these systems, ensuring that "
"operational systems always handle the request."
msgstr ""
"一般的にステートレスサービスをアクティブ / アクティブにインストールすること"
"は、すべてのインスタンスが同じ状態を持つ冗長なサービスになることを含みます。"
"別の言い方をすると、あるインスタンスのデータベースの更新は、他のすべてのイン"
"スタンスも更新されます。このように、あるインスタンスへのリクエストは、他への"
"リクエストと同じです。ロードバランサーがこれらのシステムのトラフィックを管理"
"し、利用可能なシステムが常にリクエストを確実に処理します。"

msgid ""
"A typical active/passive installation for a stateful service maintains a "
"replacement resource that can be brought online when required. Requests are "
"handled using a :term:`virtual IP address (VIP)` that facilitates returning "
"to service with minimal reconfiguration. A separate application (such as "
"Pacemaker or Corosync) monitors these services, bringing the backup online "
"as necessary."
msgstr ""
"一般的にステートレスサービスをアクティブ / パッシブにインストールすると、必要"
"に応じてオンラインにできる置換リソースを維持します。リクエストは、サービスの"
"最小限の再設定により返す機能を持つ :term:`仮想 IP アドレス <virtual IP "
"address (VIP)>` を使用して処理されます。 独立したアプリケーション (Pacemaker "
"や Corosync など) がこれらのサービスを監視し、必要に応じてバックアップ側をオ"
"ンラインにします。"

msgid "API isolation"
msgstr "API 分離"

msgid "Abstract"
msgstr "概要"

msgid ""
"Access to Memcached is not handled by HAProxy because replicated access is "
"currently in an experimental state. Instead, OpenStack services must be "
"supplied with the full list of hosts running Memcached."
msgstr ""
"重複アクセスは現在実験的な位置づけのため、Memcached へのアクセスは HAproxy を"
"利用しません。代わりに、OpenStack のサービスは Memcached を実行しているホスト"
"をすべて指定する必要があります。"

msgid ""
"Access to RabbitMQ is not normally handled by HAProxy. Instead, consumers "
"must be supplied with the full list of hosts running RabbitMQ with "
"``rabbit_hosts`` and turn on the ``rabbit_ha_queues`` option. For more "
"information, read the `core issue <http://people.redhat.com/jeckersb/private/"
"vip-failover-tcp-persist.html>`_. For more detail, read the `history and "
"solution <http://john.eckersberg.com/improving-ha-failures-with-tcp-timeouts."
"html>`_."
msgstr ""
"RabbitMQ へのアクセスは、通常 HAproxy により取り扱われません。利用者は代わり"
"に、 ``rabbit_hosts`` を用いて RabbitMQ を実行しているホストの一覧を指定し"
"て、 ``rabbit_ha_queues`` オプションを有効化する必要があります。詳細は `core "
"issue <http://people.redhat.com/jeckersb/private/vip-failover-tcp-persist."
"html>`_ と `history and solution <http://john.eckersberg.com/improving-ha-"
"failures-with-tcp-timeouts.html>`_ を参照してください。"

msgid "Active/passive versus active/active"
msgstr "アクティブ/パッシブとアクティブ/アクティブ"

msgid "Add Block Storage API resource to Pacemaker"
msgstr "Block Storage API リソースの Pacemaker への追加"

msgid ""
"Add HAProxy to the cluster and ensure the VIPs can only run on machines "
"where HAProxy is active:"
msgstr ""
"HAProxy をクラスターに追加して、仮想 IP が HAProxy の動作しているマシンにおい"
"てのみ動作できることを確認します。"

msgid "Add OpenStack Identity resource to Pacemaker"
msgstr "OpenStack Identity リソースの Pacemaker への追加"

msgid "Add OpenStack Image API resource to Pacemaker"
msgstr "OpenStack Image API リソースの Pacemaker への追加"

msgid "Add Shared File Systems API resource to Pacemaker"
msgstr "Shared File Systems API リソースの Pacemaker への追加"

msgid ""
"Add the Pacemaker configuration for the OpenStack Identity resource by "
"running the following command to connect to the Pacemaker cluster:"
msgstr ""
"ここで以下のコマンドを使用して、Pacemaker クラスターに接続することにより、"
"OpenStack Identity リソース向けに Pacemaker の設定を追加します。"

msgid ""
"Add the Pacemaker configuration for the OpenStack Image API resource. Use "
"the following command to connect to the Pacemaker cluster:"
msgstr ""
"ここで OpenStack Image API リソース向けに Pacemaker の設定を追加します。以下"
"のコマンドを使用して、Pacemaker クラスターに接続します。"

msgid ""
"Add the Pacemaker configuration for the Shared File Systems API resource. "
"Connect to the Pacemaker cluster with the following command:"
msgstr ""
"Shared File Systems API リソース用の Pacemaker 設定を追加します。以下のコマン"
"ドを使用して Pacemaker クラスターに接続します。"

msgid "Add the following cluster resources:"
msgstr "以下のクラスターリソースを追加します。"

msgid "Additional parameters"
msgstr "追加パラメーター"

msgid ""
"After installing the Corosync package, you must create the :file:`/etc/"
"corosync/corosync.conf` configuration file."
msgstr ""
"Corosync パッケージのインストール後、 :file:`/etc/corosync/corosync.conf` 設"
"定ファイルを作成する必要があります。"

msgid ""
"After the ``corosync`` service have been started and you have verified that "
"the cluster is communicating properly, you can start :command:`pacemakerd`, "
"the Pacemaker master control process. Choose one from the following four "
"ways to start it:"
msgstr ""
"``corosync`` サービスが起動して、クラスターが正常に通信していることを確認した"
"後、Pacemaker のマスター制御プロセス :command:`pacemakerd` を起動できます。以"
"下の 4 通りの方法からどれかを選択してください。"

msgid ""
"After the ``pacemaker`` service has started, Pacemaker creates a default "
"empty cluster configuration with no resources. Use the :command:`crm_mon` "
"utility to observe the status of ``pacemaker``:"
msgstr ""
"``pacemaker`` サービスの起動後、Pacemaker がリソースを持たないデフォルトの空"
"クラスターを作成します。 :command:`crm_mon` ユーティリティーを使用して、"
"``pacemaker`` の状態を確認します。"

msgid "After you make these changes, commit the updated configuration."
msgstr "これらの変更実行後、更新した設定を反映します。"

msgid ""
"After you set up your Pacemaker cluster, set a few basic cluster properties:"
msgstr ""
"Pacemaker クラスターのセットアップ後、いくつかの基本的なクラスターのプロパ"
"ティーを設定します。"

msgid "All routers are highly available by default."
msgstr "すべてのルーターは、デフォルトで高可用性になっています。"

msgid ""
"Almost all services in this stack benefit from being proxied. Using a proxy "
"server provides the following capabilities:"
msgstr ""
"このスタックのほぼすべてのサービスは、プロキシーする恩恵を受けられます。プロ"
"キシーサーバを使用することにより、以下の機能が提供されます。"

msgid ""
"Alternatively, if the database server is running, use the "
"``wsrep_last_committed`` status variable:"
msgstr ""
"代わりに、データベースサーバーが動作している場合、 ``wsrep_last_committed`` "
"状態変数を使用します。"

msgid ""
"Alternatively, instead of using systemd agents, download and install the OCF "
"resource agent:"
msgstr ""
"または、systemd エージェントを使用する代わりに、OCF リソースエージェントをダ"
"ウンロードしてインストールします。"

msgid ""
"Alternatively, make modifications using the ``firewall-cmd`` utility for "
"FirewallD that is available on many Linux distributions:"
msgstr ""
"代わりに、多くの Linux ディストリビューションにおいて利用できる FirewallD 向"
"けの ``firewall-cmd`` ユーティリティーを使用して変更することもできます。"

msgid ""
"Alternatively, you can use a commercial load balancer, which is hardware or "
"software. We recommend a hardware load balancer as it generally has good "
"performance."
msgstr ""
"代わりに、ハードウェアやソフトウェアの商用ロードバランサーを使用することもで"
"きます。ハードウェアロードバランサーは一般的に高性能なので、推奨されます。"

msgid "Alternatively:"
msgstr "他の"

msgid ""
"An AMQP (Advanced Message Queuing Protocol) compliant message bus is "
"required for most OpenStack components in order to coordinate the execution "
"of jobs entered into the system."
msgstr ""
"AMQP (Advanced Message Queuing Protocol) 互換メッセージバスが、システム内の"
"ジョブ実行を調整するために、ほとんどの OpenStack コンポーネントに必要となりま"
"す。"

msgid "An OpenStack environment includes multiple data pools for the VMs:"
msgstr "OpenStack 環境は、仮想マシン向けの複数のデータプールがあります。"

msgid ""
"And the quorum could also have been set to three, just as a configuration "
"example."
msgstr "また、クォーラムが、設定例にあるように 3 つに設定されているでしょう。"

msgid "AppArmor"
msgstr "AppArmor"

msgid "AppArmor now permits Galera Cluster to operate."
msgstr "AppArmor により Galera Cluster の動作を許可されます。"

msgid "Appendix"
msgstr "付録"

msgid ""
"Application Armor is a kernel module for improving security on Linux "
"operating systems. It is developed by Canonical and commonly used on Ubuntu-"
"based distributions. In the context of Galera Cluster, systems with AppArmor "
"may block the database service from operating normally."
msgstr ""
"Application Armor は、Linux オペレーティングシステムにおいてセキュリティーを"
"向上するためのカーネルモジュールです。Canonical により開発され、一般的に "
"Ubuntu 系のディストリビューションにおいて使用されています。Galera Cluster の"
"観点では、AppArmor を有効化したシステムは、データベースサービスが正常に動作す"
"ることを妨げる可能性があります。"

msgid "Applications and automatic service migration"
msgstr "アプリケーションおよびサービスの自動的なマイグレーション"

msgid ""
"As another option to make RabbitMQ highly available, RabbitMQ contains the "
"OCF scripts for the Pacemaker cluster resource agents since version 3.5.7. "
"It provides the active/active RabbitMQ cluster with mirrored queues. For "
"more information, see `Auto-configuration of a cluster with a Pacemaker "
"<https://www.rabbitmq.com/pacemaker.html>`_."
msgstr ""
"RabbitMQ を高可用化する別の選択肢として、RabbitMQ バージョン 3.5.7 以降、"
"Pacemaker クラスターリソースエージェント向けの OCF スクリプトが含まれます。ア"
"クティブ/アクティブ RabbitMQ クラスターにミラーキューを提供します。詳細は "
"`Auto-configuration of a cluster with a Pacemaker <https://www.rabbitmq.com/"
"pacemaker.html>`_ を参照してください。"

msgid ""
"As of September 2016, the OpenStack High Availability community is designing "
"and developing an official and unified way to provide high availability for "
"instances. We are developing automatic recovery from failures of hardware or "
"hypervisor-related software on the compute node, or other failures that "
"could prevent instances from functioning correctly, such as, issues with a "
"cinder volume I/O path."
msgstr ""
"2016年9月時点、OpenStack High Availability コミュニティーは、インスタンスの高"
"可用性を提供するために、公式な統一された方法を設定および開発しています。ハー"
"ドウェアやハイパーバイザー関連ソフトウェアの障害、cinder ボリュームの I/O パ"
"スに関する問題のように、インスタンスが正常に動作しないような他の障害から自動"
"的に復旧する方法を開発しています。"

msgid ""
"At its core, a cluster is a distributed finite state machine capable of co-"
"ordinating the startup and recovery of inter-related services across a set "
"of machines."
msgstr ""
"クラスターは、その中心において、複数のセットのマシン間で関連するサービスのス"
"タートアップとリカバリーを調整する機能を持つ、分散有限状態マシンです。"

msgid "Automated recovery of failed instances"
msgstr "障害インスタンスの自動復旧"

msgid "Awareness of instances on other machines"
msgstr "他のマシンにあるインスタンスの把握"

msgid "Awareness of other applications in the stack"
msgstr "スタックにある他のアプリケーションの認識"

msgid ""
"Bear in mind, leaving SELinux in permissive mode is not a good security "
"practice. Over the longer term, you need to develop a security policy for "
"Galera Cluster and then switch SELinux back into enforcing mode."
msgstr ""
"SELinux を permissive モードにすることは、良いセキュリティー慣行ではないこと"
"を覚えておいてください。長い間、Galera Cluster のセキュリティーポリシーを開発"
"して、SELinux を enforcing モードに切り替える必要があります。"

msgid ""
"Before beginning, ensure you have read the `OpenStack Identity service "
"getting started documentation <https://docs.openstack.org/admin-guide/common/"
"get-started-identity.html>`_."
msgstr ""
"進める前に `OpenStack Identity サービスの概要 <https://docs.openstack.org/"
"admin-guide/common/get-started-identity.html>`_ をきちんと読んでください。"

msgid ""
"Before following this guide to configure the highly available OpenStack "
"cluster, ensure the IP ``10.0.0.11`` and hostname ``controller`` are not in "
"use."
msgstr ""
"このガイドを読み進める前に、高可用性 OpenStack クラスターが IP アドレス "
"``10.0.0.11`` とホスト名 ``controller`` を使わないよう設定してください。"

msgid ""
"Before you launch Galera Cluster, you need to configure the server and the "
"database to operate as part of the cluster."
msgstr ""
"Galera クラスターを起動する前に、クラスターの一部として動作するよう、サーバー"
"とデータベースを設定する必要があります。"

msgid ""
"Both the central and the compute agent can run in an HA deployment. This "
"means that multiple instances of these services can run in parallel with "
"workload partitioning among these running instances."
msgstr ""
"中央エージェントとコンピュートエージェントの両方は、高可用性で動作できます。"
"これらのサービスの複数のインスタンスが、これらを実行しているインスタンス間で"
"並行して負荷分散できることを意味します。"

msgid ""
"Both use a cluster manager, such as Pacemaker or Veritas, to orchestrate the "
"actions of the various services across a set of machines. Because we are "
"focused on FOSS, we refer to these as Pacemaker architectures."
msgstr ""
"どちらも Pacemaker や Veritas のようなクラスターマネージャーを使用して、複数"
"のマシンにまたがるさまざまなサービスの動作を協調させます。私たちは FOSS に注"
"力しているため、Pacemaker のアーキテクチャーを参照します。"

msgid ""
"By default, STONITH is enabled in Pacemaker, but STONITH mechanisms (to "
"shutdown a node via IPMI or ssh) are not configured. In this case Pacemaker "
"will refuse to start any resources. For production cluster it is recommended "
"to configure appropriate STONITH mechanisms. But for demo or testing "
"purposes STONITH can be disabled completely as follows:"
msgstr ""
"デフォルトでは、STONITH は Pacemaker で有効化されていますが、Pacemaker メカニ"
"ズム (IPMI や SSH 経由のノードのシャットダウン) は設定されていません。この場"
"合、Pacemaker はリソースの開始をすべて拒否します。本番環境のクラスターは、適"
"切な STONITH メカニズムを設定することが推奨されます。デモ目的やテスト目的の場"
"合、STONITH は以下のとおり完全に無効化できます。"

msgid ""
"By default, ``controller1`` handles the caching service. If the host goes "
"down, ``controller2`` or ``controller3`` will complete the service."
msgstr ""
"デフォルトで ``controller1`` がキャッシュサービスを処理します。そのホストが停"
"止している場合、 ``controller2`` または ``controller3`` がサービスを実施しま"
"す。"

msgid ""
"By default, cluster nodes do not start as part of a Primary Component. In "
"the Primary Component, replication and state transfers bring all databases "
"to the same state."
msgstr ""
"クラスターノードは、デフォルトで Primary Component の一部として起動しません。"
"Primary Component において、レプリケーションと状態転送により、すべてのデータ"
"ベースが同じ状態になります。"

msgid ""
"By sending all API access through the proxy, you can clearly identify "
"service interdependencies. You can also move them to locations other than "
"``localhost`` to increase capacity if the need arises."
msgstr ""
"すべての API アクセスをプロキシー経由で送信することにより、サービスの相互依存"
"関係を明確に識別できます。キャパシティーを必要に応じて増やすために、それらを "
"``localhost`` から別の場所に移動できます。"

msgid "Ceph"
msgstr "Ceph"

msgid ""
"Ceph RBD provides object replication capabilities by storing Block Storage "
"volumes as Ceph RBD objects. Ceph RBD ensures that each replica of an object "
"is stored on a different node. This means that your volumes are protected "
"against hard drive and node failures, or even the failure of the data center "
"itself."
msgstr ""
"Ceph RBD は、Ceph RBD オブジェクトとして Block Storage のボリュームを保存する"
"ことにより、オブジェクトレプリケーション機能を提供します。オブジェクトの各レ"
"プリカが別々のノードに保存されることを保証します。このことは、お使いのボ"
"リュームがハードディスクやノードの障害時、データセンター自体の障害時にも保護"
"されることを意味します。"

msgid ""
"Certain services running on the underlying operating system of your "
"OpenStack database may block Galera Cluster from normal operation or prevent "
"``mysqld`` from achieving network connectivity with the cluster."
msgstr ""
"OpenStack データベースのベースとなるオペレーティングシステムで動作している特"
"定のサービスは、Galera Cluster が通常の動作をブロックしたり、``mysqld`` がク"
"ラスターとのネットワーク接続を妨害したりする可能性があります。"

msgid "Change the number of expected votes for a cluster to be quorate"
msgstr "クラスターが定数になるために期待されるボート数を変更します"

msgid "Change the number of votes assigned to a node"
msgstr "ノードに割り当てられたボート数を変更します"

msgid ""
"Cinder provides Block-Storage-as-a-Service suitable for performance "
"sensitive scenarios such as databases, expandable file systems, or providing "
"a server with access to raw block level storage."
msgstr ""
"Cinder は、データベースなどの性能を必要とするシナリオ、拡張可能なファイルシス"
"テム、ローブロックレベルストレージにアクセスするサーバーに適するサービスとし"
"て Block-Storage-as-a-Service を提供します。"

msgid "Clusters and quorums"
msgstr "クラスターとクォーラム"

msgid ""
"Clusters with an even number of hosts suffer from similar issues. A single "
"network failure could easily cause a N:N split where neither side retains a "
"majority. For this reason, we recommend an odd number of cluster members "
"when scaling up."
msgstr ""
"偶数のホストを持つクラスターは、同じような問題に苦しみます。単一のネットワー"
"ク障害により、どちらの側も多数派になれない N:N 分断を簡単に引き起こす可能性が"
"あります。この理由により、スケールアップするとき、奇数個のクラスターメンバー"
"を推奨します。"

msgid "Collapsed"
msgstr "Collapsed"

msgid ""
"Commit your configuration changes by entering the following command from "
"the :command:`crm configure` menu:"
msgstr ""
":command:`crm configure` メニューから以下のコマンドを実行して、設定の変更を反"
"映します。"

msgid ""
"Commit your configuration changes from the :command:`crm configure` menu "
"with the following command:"
msgstr ""
":command:`crm configure` メニューから以下のコマンドを入力して、設定の変更をコ"
"ミットします。"

msgid "Common deployment architectures"
msgstr "一般的な配備のアーキテクチャー"

msgid "Configuration"
msgstr "設定"

msgid "Configuration tips"
msgstr "設定のヒント"

msgid "Configure Block Storage API service"
msgstr "Block Storage API サービスの設定"

msgid "Configure NTP"
msgstr "NTP の設定"

msgid "Configure OpenStack Identity service"
msgstr "OpenStack Identity Service の設定"

msgid "Configure OpenStack Image service API"
msgstr "OpenStack Image サービス API の設定"

msgid "Configure OpenStack services to use HA Shared File Systems API"
msgstr ""
"高可用性 Shared File Systems API を使用するための OpenStack サービスの設定"

msgid "Configure OpenStack services to use Rabbit HA queues"
msgstr "RabbitMQ HA キューを使用するための OpenStack サービスの設定"

msgid ""
"Configure OpenStack services to use the highly available Block Storage API"
msgstr "高可用性 Block Storage API を使用するための OpenStack サービスの設定"

msgid ""
"Configure OpenStack services to use the highly available OpenStack Identity"
msgstr "高可用性 OpenStack Identity を使用するための OpenStack サービスの設定"

msgid ""
"Configure OpenStack services to use the highly available OpenStack Image API"
msgstr ""
"高可用性 OpenStack Image Service API を使用するための OpenStack サービスの設"
"定"

msgid "Configure RabbitMQ for HA queues"
msgstr "高可用性 キュー用の RabbitMQ の設定"

msgid "Configure Shared File Systems API service"
msgstr "Shared File Systems API サービスの設定"

msgid "Configure the OpenStack components to use at least two RabbitMQ nodes."
msgstr ""
"2 つ以上の RabbitMQ ノードを使用するよう、OpenStack のコンポーネントを設定し"
"ます。"

msgid "Configure the VIP"
msgstr "仮想 IP の設定"

msgid ""
"Configure the kernel parameter to allow non-local IP binding. This allows "
"running HAProxy instances to bind to a VIP for failover. Add following line "
"to ``/etc/sysctl.conf``:"
msgstr ""
"ローカル IP 以外のバインドを許可するために、カーネルパラメーターを設定しま"
"す。これにより、動作中の HAProxy インスタンスがフェイルオーバー用の仮想 IP を"
"バインドできるようになります。"

msgid "Configuring Block Storage to listen on the VIP address"
msgstr "Block Storage がその仮想 IP アドレスをリッスンする設定"

msgid "Configuring HAProxy"
msgstr "HAProxy の設定"

msgid "Configuring InnoDB"
msgstr "InnoDB の設定"

msgid "Configuring OpenStack services to use this IP address"
msgstr "OpenStack のサービスがこの IP アドレスを使用する設定"

msgid ""
"Configuring RAID on the hard drives that implement storage protects your "
"data against a hard drive failure. If the node itself fails, data may be "
"lost. In particular, all volumes stored on an LVM node can be lost."
msgstr ""
"ストレージを実装するハードディスクに RAID を設定することにより、ハードディス"
"ク障害からデータを保護します。ノード自体が故障した場合、データが失われるかも"
"しれません。とくに、LVM ノードに保存されている全ボリュームは失われる可能性が"
"あります。"

msgid "Configuring high availability for instances"
msgstr "インスタンスの高可用性の設定"

msgid "Configuring mysqld"
msgstr "mysqld の設定"

msgid "Configuring storage"
msgstr "ストレージの設定"

msgid "Configuring the basic environment"
msgstr "基本環境の設定"

msgid "Configuring the compute node"
msgstr "コンピュートノードの設定"

msgid "Configuring the controller"
msgstr "コントローラーの設定"

msgid "Configuring the networking services"
msgstr "ネットワークサービスの設定"

msgid "Configuring the server"
msgstr "サーバーの設定"

msgid "Configuring the shared services"
msgstr "共有サービスの設定"

msgid "Configuring wsrep replication"
msgstr "wsrep レプリケーションの設定"

msgid ""
"Connect an additional quorum device to allow small clusters remain quorate "
"during node outages"
msgstr ""
"追加のクォーラムデバイスを接続して、小規模なクラスターがノード障害時にクォー"
"ラムを取得できるようにします。"

msgid ""
"Consider that, while exchanges and bindings survive the loss of individual "
"nodes, queues and their messages do not because a queue and its contents are "
"located on one node. If we lose this node, we also lose the queue."
msgstr ""
"エクスチェンジとバインドは個々のノード障害に耐えられますが、キューとそのメッ"
"セージは、あるノードに置かれるため、失われることを考慮してください。このノー"
"ドを失うとき、キューも失われます。"

msgid "Contents"
msgstr "内容"

msgid ""
"Corosync can be configured to work with either multicast or unicast IP "
"addresses or to use the votequorum library."
msgstr ""
"Corosync を動作させるための設定としては、マルチキャスト IP アドレスを使う、ユ"
"ニキャスト IP アドレスを使う、 votequorum ライブラリーを使う、の選択肢があり"
"ます。"

msgid ""
"Corosync is started as a regular system service. Depending on your "
"distribution, it may ship with an LSB init script, an upstart job, or a "
"Systemd unit file."
msgstr ""
"Corosync は通常のシステムサービスとして起動します。お使いのディストリビュー"
"ションに応じて、LSB init スクリプト、upstart ジョブ、systemd ユニットファイル"
"を同梱しているかもしれません。"

msgid ""
"Create a configuration file for ``clustercheck`` at ``/etc/sysconfig/"
"clustercheck``:"
msgstr ""
"``clustercheck`` の設定ファイルを ``/etc/sysconfig/clustercheck`` に作成しま"
"す。"

msgid ""
"Create a configuration file for the HAProxy monitor service, at ``/etc/"
"xinetd.d/galera-monitor``:"
msgstr ""
"HAProxy モニターサービスの設定ファイルを ``/etc/xinetd.d/galera-monitor`` に"
"作成します。"

msgid ""
"Create a symbolic link for the database server in the ``disable`` directory:"
msgstr ""
"``disable`` ディレクトリーにデータベースサーバーへのシンボリックリンクを作成"
"します。"

msgid ""
"Create and name the cluster. Then, start it and enable all components to "
"auto-start at boot time:"
msgstr ""
"クラスターを作成して、名前を付けます。そして、それを起動して、すべてのコン"
"ポーネントが起動時に自動起動するようにします。"

msgid "Create the Block Storage API endpoint with this IP."
msgstr "この IP を用いて Block Storage API エンドポイントを作成します。"

msgid "Create the OpenStack Identity Endpoint with this IP address."
msgstr ""
"この IP アドレスを用いて OpenStack Identity エンドポイントを作成します。"

msgid "Current upstream work"
msgstr "アップストリームの現在の取り組み"

msgid ""
"Data integrity through fencing (a non-responsive process does not imply it "
"is not doing anything)"
msgstr ""
"フェンシングによるデータ完全性 (応答なしプロセスが何もしていないことを意味し"
"ます)"

msgid "Data loss: Accidental deletion or destruction of data."
msgstr "データ損失: 意図しないデータの削除や破損。"

msgid "Database (Galera Cluster) for high availability"
msgstr "データベース (Galera クラスター) の高可用性"

msgid "Database configuration"
msgstr "データベース設定"

msgid "Database hosts with Galera Cluster installed"
msgstr "Galera Cluster をインストールしたデータベースホスト"

msgid ""
"Define the InnoDB memory buffer pool size. The default value is 128 MB, but "
"to compensate for Galera Cluster's additional memory usage, scale your usual "
"value back by 5%:"
msgstr ""
"InnoDB メモリーバッファープールサイズを定義します。デフォルト値は 128 MB です"
"が、Galera Cluster の追加メモリー使用状況に対して補うために、通常の値を 5% ま"
"でスケールさせてください。"

msgid "Deployment flavors"
msgstr "デプロイフレーバー"

msgid "Deployment strategies"
msgstr "デプロイ戦略"

msgid "Description"
msgstr "説明"

msgid ""
"Do not change this value. Other modes may cause ``INSERT`` statements on "
"tables with auto-increment columns to fail as well as unresolved deadlocks "
"that leave the system unresponsive."
msgstr ""
"この値を変更してはいけません。他のモジュールが、自動インクリメントの列を用い"
"てテーブルに ``INSERT`` ステートメントを発行するかもしれません。これは、シス"
"テムが応答不可になる解決不能なデッドロックに陥ります。"

msgid "Download the resource agent to your system:"
msgstr "まず、お使いのシステムにリソースエージェントをダウンロードします。"

msgid ""
"Each configured interface must have a unique ``ringnumber``, starting with 0."
msgstr ""
"設定済みの各インターフェースは、0 から始まる一意な ``ringnumber`` を持つ必要"
"があります。"

msgid "Each instance has its own IP address:"
msgstr "各インスタンスは、自身の IP アドレスを持ちます。"

msgid ""
"Each instance of HAProxy configures its front end to accept connections only "
"to the virtual IP (VIP) address. The HAProxy back end (termination point) is "
"a list of all the IP addresses of instances for load balancing."
msgstr ""
"HAProxy の各インスタンスは、仮想 IP アドレスへの接続のみを受け付けるよう、そ"
"のフロントエンドを設定します。HAProxy のバックエンド (接続先) は、負荷分散さ"
"れるインスタンスの IP アドレスの一覧です。"

msgid ""
"Each service also has a backup but manages both the main and redundant "
"systems concurrently. This way, if there is a failure, the user is unlikely "
"to notice. The backup system is already online and takes on increased load "
"while the main system is fixed and brought back online."
msgstr ""
"各サービスはバックアップも持ちますが、メインと冗長システムを同時に管理しま"
"す。このように、ユーザーが気が付かない障害が発生した場合、バックアップシステ"
"ムはすでにオンラインであり、メインシステムが復旧され、オンラインになるまでの"
"間は負荷が高くなります。"

msgid ""
"Edit the :file:`/etc/glance/glance-api.conf` file to configure the OpenStack "
"Image service:"
msgstr ""
":file:`/etc/glance/glance-api.conf` ファイルを編集して、OpenStack Image サー"
"ビスを設定します。"

msgid "Edit the :file:`/etc/manila/manila.conf` file:"
msgstr "`/etc/manila/manila.conf` ファイルを編集します。"

msgid ""
"Edit the :file:`keystone.conf` file to change the values of the :manpage:"
"`bind(2)` parameters:"
msgstr ""
":file:`keystone.conf` ファイルを編集して、 :manpage:`bind(2)` パラメーターの"
"値を変更します。"

msgid ""
"Edit the ``/etc/cinder/cinder.conf`` file. For example, on a RHEL-based "
"system:"
msgstr ""
"``/etc/cinder/cinder.conf`` ファイルを編集します。たとえば、RHEL 系システムの"
"場合:"

msgid "Enhanced failure detection"
msgstr "高度な障害検出"

msgid ""
"Ensure that the InnoDB locking mode for generating auto-increment values is "
"set to ``2``, which is the interleaved locking mode:"
msgstr ""
"自動インクリメント値を生成するための InnoDB ロックモードがをきちんと``2`` に"
"設定してください。これは、インターリーブ・ロックモードです。"

msgid ""
"Ensure that the InnoDB log buffer is written to file once per second, rather "
"than on each commit, to improve performance:"
msgstr ""
"パフォーマンスを改善するために、InnoDB ログバッファーが、コミットごとではな"
"く、1 秒ごとにファイルに書き込むことを確認します。"

msgid ""
"Ensure that the binary log format is set to use row-level replication, as "
"opposed to statement-level replication:"
msgstr ""
"バイナリーログ形式が、ステートメントレベルのレプリケーションではなく、行レベ"
"ルのレプリケーションに設定されていることを確認してください。"

msgid ""
"Ensure that the database server is not bound only to the localhost: "
"``127.0.0.1``. Also, do not bind it to ``0.0.0.0``. Binding to the localhost "
"or ``0.0.0.0`` makes ``mySQL`` bind to all IP addresses on the machine, "
"including the virtual IP address causing ``HAProxy`` not to start. Instead, "
"bind to the management IP address of the controller node to enable access by "
"other nodes through the management network:"
msgstr ""
"データベースサーバーが localhost: ``127.0.0.1`` のみにバインドされていないこ"
"とを確認してください。また、``0.0.0.0`` にバインドしないでください。"
"localhost や ``0.0.0.0`` にバインドすることにより、MySQL がマシンのすべての "
"IP アドレスにバインドされます。これは仮想 IP アドレスを含み、``HAProxy`` が起"
"動しなくなります。代わりに、コントローラーノードの管理 IP アドレスにバインド"
"して、管理ネットワーク経由で他のノードによりアクセスできるようにします。"

msgid "Ensure that the default storage engine is set to InnoDB:"
msgstr "デフォルトのストレージエンジンをきちんと InnoDB に設定してください。"

msgid ""
"Ensure your HAProxy installation is not a single point of failure, it is "
"advisable to have multiple HAProxy instances running."
msgstr ""
"HAProxy が単一障害点にならないようにします。複数の HAProxy インスタンスを実行"
"することが推奨されます。"

msgid ""
"Ephemeral storage is allocated for an instance and is deleted when the "
"instance is deleted. The Compute service manages ephemeral storage and by "
"default, Compute stores ephemeral drives as files on local disks on the "
"compute node. As an alternative, you can use Ceph RBD as the storage back "
"end for ephemeral storage."
msgstr ""
"一時ストレージは、インスタンスのために割り当てられ、インスタンスの削除時に削"
"除されます。Compute サービスが一時ストレージを管理します。Compute はデフォル"
"トで、コンピュートノードのローカルディスクにファイルとして一時ディスクを保存"
"します。代わりに、一時ストレージのストレージバックエンドとして Ceph RBD を使"
"用できます。"

msgid ""
"Even a distributed or replicated application that is able to survive "
"failures on one or more machines can benefit from a cluster manager because "
"a cluster manager has the following capabilities:"
msgstr ""
"いくつかのマシンの障害に耐えられる分散アプリケーションやレプリケーションで"
"も、クラスターマネージャーが以下の機能を持つので、クラスターマネージャーによ"
"る恩恵があります。"

msgid "Existing solutions"
msgstr "既存のソリューション"

msgid "Facility services such as power, air conditioning, and fire protection"
msgstr "電源、空調、防火などに関する設備"

msgid "Firewall"
msgstr "ファイアウォール"

msgid ""
"For Liberty, you can not have the standalone network nodes. The Networking "
"services are run on the controller nodes. In this guide, the term `network "
"nodes` is used for convenience."
msgstr ""
"Liberty の場合、独立したネットワークノードを一般的に持ちません。Networking "
"サービスはコントローラーノードにおいて実行されます。このガイドでは、便宜上"
"「ネットワークノード」という言葉を使用します。"

msgid ""
"For OpenStack Compute, (if your OpenStack Identity service IP address is "
"10.0.0.11) use the following configuration in the :file:`api-paste.ini` file:"
msgstr ""
"OpenStack Compute の場合 (OpenStack Identity サービスの IP アドレスが "
"10.0.0.11 の場合)、以下の設定を :file:`api-paste.ini` ファイルに使用します。"

msgid "For RHEL, Fedora, or CentOS:"
msgstr "RHEL、Fedora、CentOS の場合:"

msgid ""
"For Red Hat Enterprise Linux and Red Hat-based Linux distributions, the "
"following process uses Systemd unit files."
msgstr ""
"Red Hat Enterprise Linux および Red Hat 系の Linux ディストリビューションの場"
"合、以下のプロセスが Systemd ユニットファイルを使用します。"

msgid ""
"For SLES 12, the packages are signed by GPG key 893A90DAD85F9316. You should "
"verify the fingerprint of the imported GPG key before using it."
msgstr ""
"SLES 12 の場合、パッケージは GPG キー 893A90DAD85F9316 により署名されていま"
"す。使用する前に、インポートした GPG キーのフィンガープリントを検証すべきで"
"す。"

msgid "For SLES 12:"
msgstr "SLES 12 の場合:"

msgid ""
"For UDPU, every node that should be a member of the membership must be "
"specified."
msgstr ""
"UDPUでは、全てのノードがメンバーシップメンバーを指定しなければなりません。"

msgid ""
"For Ubuntu 16.04.1: Create a configuration file for ``clustercheck`` at ``/"
"etc/default/clustercheck``."
msgstr ""
"Ubuntu 16.04.1 の場合: ``clustercheck`` の設定ファイルを ``/etc/default/"
"clustercheck`` に作成します。"

msgid "For Ubuntu or Debian:"
msgstr "Ubuntu、Debian の場合:"

msgid ""
"For Ubuntu, you should also enable the Corosync service in the ``/etc/"
"default/corosync`` configuration file."
msgstr ""
"Ubuntu の場合、 ``/etc/default/corosync`` 設定ファイルにおいて Corosync サー"
"ビスも有効化すべきです。"

msgid ""
"For `Fedora <https://fedoraproject.org/wiki/How_to_edit_iptables_rules>`_"
msgstr ""
"`Fedora <https://fedoraproject.org/wiki/How_to_edit_iptables_rules>`_ の場合"

msgid "For ``crmsh``:"
msgstr "``crmsh`` の場合:"

msgid "For ``pcs``:"
msgstr "``pcs`` の場合:"

msgid ""
"For a complete list of the available parameters, run the ``SHOW VARIABLES`` "
"command from within the database client:"
msgstr ""
"利用できるパラメーターの一覧は、データベースクライアントから ``SHOW "
"VARIABLES`` コマンドを実行してください。"

msgid ""
"For backward compatibility and supporting existing deployments, the central "
"agent configuration supports using different configuration files. This is "
"for groups of service instances that are running in parallel. For enabling "
"this configuration, set a value for the ``partitioning_group_prefix`` option "
"in the `polling section <https://docs.openstack.org/ocata/config-reference/"
"telemetry/telemetry-config-options.html>`_ in the OpenStack Configuration "
"Reference."
msgstr ""
"既存の環境の後方互換性とサポートのために、中央エージェントの設定は、別の設定"
"ファイルを使用することをサポートします。これは並列で実行しているサービスイン"
"スタンスのグループのためです。この設定を有効化するために、OpenStack "
"Configuration Reference の `polling section <https://docs.openstack.org/"
"ocata/config-reference/telemetry/telemetry-config-options.html>`_  にある "
"``partitioning_group_prefix`` オプションの値を設定します。"

msgid ""
"For demonstrations and studying, you can set up a test environment on "
"virtual machines (VMs). This has the following benefits:"
msgstr ""
"デモや学習の場合、仮想マシンにテスト環境をセットアップできます。これには以下"
"の利点があります。"

msgid ""
"For detailed instructions about installing HAProxy on your nodes, see the "
"HAProxy `official documentation <http://www.haproxy.org/#docs>`_."
msgstr ""
"お使いのノードに HAProxy をインストールする方法の詳細は HAProxy `公式ドキュメ"
"ント <http://www.haproxy.org/#docs>`_ を参照してください。"

msgid ""
"For documentation about these parameters, ``wsrep`` provider option, and "
"status variables available in Galera Cluster, see the Galera cluster "
"`Reference <http://galeracluster.com/documentation-webpages/reference."
"html>`_."
msgstr ""
"Galera Cluster において利用できる、これらのパラメーター、``wsrep`` プロバイ"
"ダーオプション、状態変数のドキュメントは、Galera クラスターの`リファレンス "
"<http://galeracluster.com/documentation-webpages/reference.html>`_ を参照して"
"ください。"

msgid ""
"For each sub-group of the central agent pool with the same "
"``partitioning_group_prefix``, a disjoint subset of meters must be polled to "
"avoid samples being missing or duplicated. The list of meters to poll can be "
"set in the :file:`/etc/ceilometer/pipeline.yaml` configuration file. For "
"more information about pipelines see the `Data processing and pipelines "
"<https://docs.openstack.org/admin-guide/telemetry-data-pipelines.html>`_ "
"section."
msgstr ""
"同じ ``partitioning_group_prefix`` を持つ中央エージェントプールの各サブグルー"
"プに対して、サンプルの損失や重複を避けるために、互いに関わらないメーターのサ"
"ブセットが取得される必要があります。取得されるメーターの一覧は :file:`/etc/"
"ceilometer/pipeline.yaml` 設定ファイルに設定できます。パイプラインの詳細は "
"`Data processing and pipelines <https://docs.openstack.org/admin-guide/"
"telemetry-data-pipelines.html>`_ のセクションを参照してください。"

msgid ""
"For environments that do not support multicast, Corosync should be "
"configured for unicast. An example fragment of the :file:`corosync.conf` "
"file for unicastis is shown below:"
msgstr ""
"マルチキャストをサポートしていない場合、Corosync はユニキャストで設定すべきで"
"す。ユニキャスト向け :file:`corosync.conf` ファイルの設定例を以下に示します。"

msgid ""
"For example, if your OpenStack Image API service IP address is 10.0.0.11 (as "
"in the configuration explained here), you would use the following "
"configuration in your :file:`nova.conf` file:"
msgstr ""
"例えば、OpenStack Image API サービスの IP アドレスが (ここで説明されている設"
"定のように) 10.0.0.11 ならば、以下の設定を :file:`nova.conf` ファイルに使用し"
"ます。"

msgid ""
"For example, in a seven-node cluster, the quorum should be set to "
"``floor(7/2) + 1 == 4``. If quorum is four and four nodes fail "
"simultaneously, the cluster itself would fail, whereas it would continue to "
"function, if no more than three nodes fail. If split to partitions of three "
"and four nodes respectively, the quorum of four nodes would continue to "
"operate the majority partition and stop or fence the minority one (depending "
"on the no-quorum-policy cluster configuration)."
msgstr ""
"たとえば、7 ノードクラスターにおいて、クォーラムは ``floor(7/2) + 1 == 4`` に"
"設定されるべきです。クォーラムが 4 で、4 ノードが同時に停止した場合、クラス"
"ター自身が停止するでしょう。一方、3 ノード以下の停止の場合、動作し続けられる"
"でしょう。それぞれ 3 ノードと 4 ノードに分割された場合、4 ノードのクラスター"
"のクォーラムが多数派のパーティションを動作し続け、(no-quorum-policy クラス"
"ター設定に応じて) 少数派を停止またはフェンスするでしょう。"

msgid ""
"For example, you may enter ``edit p_ip_glance-api`` from the :command:`crm "
"configure` menu and edit the resource to match your preferred virtual IP "
"address."
msgstr ""
"例えば、お好みの仮想 IP アドレスに一致させるために、:command:`crm configure` "
"メニューから ``edit  p_ip_glance-api`` と入力し、リソースを編集できます。"

msgid ""
"For example, you may enter ``edit p_ip_keystone`` from the :command:`crm "
"configure` menu and edit the resource to match your preferred virtual IP "
"address."
msgstr ""
"例えば、お好みの仮想 IP アドレスに一致させるために、:command:`crm configure` "
"メニューから ``edit  p_ip_keystone`` と入力し、リソースを編集できます。"

msgid ""
"For example, you may enter ``edit p_ip_manila-api`` from the :command:`crm "
"configure` menu and edit the resource to match your preferred virtual IP "
"address."
msgstr ""
"例えば、お好みの仮想 IP アドレスに一致させるために、:command:`crm configure` "
"メニューから ``edit  p_ip_manila-api`` と入力し、リソースを編集できます。"

msgid ""
"For firewall configurations, Corosync communicates over UDP only, and uses "
"``mcastport`` (for receives) and ``mcastport - 1`` (for sends)."
msgstr ""
"ファイアウォール設定に向け、Corosync は UDP のみで通信して、 ``mcastport`` "
"(受信用) と ``mcastport - 1`` (送信用) を使用します。"

msgid ""
"For information about the required configuration options to set in the :file:"
"`ceilometer.conf`, see the `coordination section <https://docs.openstack.org/"
"ocata/config-reference/telemetry.html>`_ in the OpenStack Configuration "
"Reference."
msgstr ""
":file:`ceilometer.conf` 設定ファイルに設定する必要があるオプションの詳細は、"
"OpenStack Configuration Reference の `coordination section <https://docs."
"openstack.org/ocata/config-reference/telemetry.html>`_  を参照してください。"

msgid ""
"For more information about configuring storage back ends for the different "
"storage options, see `Manage volumes <https://docs.openstack.org/admin-guide/"
"blockstorage-manage-volumes.html>`_ in the OpenStack Administrator Guide."
msgstr ""
"さまざまなストレージの選択肢に対して、ストレージバックエンドを設定する方法の"
"詳細は、OpenStack Administrator Guide の `Manage volumes <https://docs."
"openstack.org/admin-guide/blockstorage-manage-volumes.html>`_ を参照してくだ"
"さい。"

msgid ""
"For more information on configuring SELinux to work with Galera Cluster, see "
"the `SELinux Documentation <http://galeracluster.com/documentation-webpages/"
"selinux.html>`_"
msgstr ""
"Galera Cluster と動作する SELinux を設定する方法の詳細は `SELinux ドキュメン"
"ト <http://galeracluster.com/documentation-webpages/selinux.html>`_ を参照し"
"てください。"

msgid ""
"For more information on firewalls, see `firewalls and default ports <https://"
"docs.openstack.org/admin-guide/firewalls-default-ports.html>`_ in OpenStack "
"Administrator Guide."
msgstr ""
"ファイアウォールの詳細は、OpenStack Administrator Guide の `Firewalls and "
"default ports <https://docs.openstack.org/admin-guide/firewalls-default-"
"ports.html>`_ を参照してください。"

msgid ""
"For more information, see the official installation manual for the "
"distribution:"
msgstr ""
"詳細はディストリビューションの公式インストールガイドを参照してください。"

msgid "For openSUSE:"
msgstr "openSUSE の場合:"

msgid "For servers that use ``systemd``, run the following command:"
msgstr "``systemd`` を使用するサーバーの場合、以下のコマンドを実行します。"

msgid "For servers that use ``systemd``, run the following commands:"
msgstr "``systemd`` を使用するサーバーの場合、以下のコマンドを実行します。"

msgid ""
"For these reasons, we highly recommend the use of a cluster manager like "
"`Pacemaker <http://clusterlabs.org>`_."
msgstr ""
"これらの理由のため、`Pacemaker <http://clusterlabs.org>`_ のようなクラスター"
"マネージャーを使用することを強く推奨します。"

msgid ""
"For this reason, each cluster in a high availability environment should have "
"an odd number of nodes and the quorum is defined as more than a half of the "
"nodes. If multiple nodes fail so that the cluster size falls below the "
"quorum value, the cluster itself fails."
msgstr ""
"この理由により、高可用性環境における各クラスターは、奇数個のコードを持つべき"
"であり、クォーラムがノードの過半数に定義されます。クラスター数がクォーラム値"
"を下回るよう、複数のノードが停止した場合、クラスター自身が停止します。"

msgid ""
"Galera Cluster configuration parameters all have the ``wsrep_`` prefix. You "
"must define the following parameters for each cluster node in your OpenStack "
"database."
msgstr ""
"Galera Cluster の設定パラメーターは、すべて ``wsrep_`` プレフィックスを持ちま"
"す。OpenStack データベースにおいて、各クラスターノード向けに以下のパラメー"
"ターを定義する必要があります。"

msgid ""
"Galera Cluster does not support non-transactional storage engines and "
"requires that you use InnoDB by default. There are some additional "
"parameters that you must define to avoid conflicts."
msgstr ""
"Galera Cluster は、トランザクション未対応ストレージエンジンをサポートしませ"
"ん。デフォルトでは InnoDB を使用する必要があります。競合を避けるために定義す"
"る必要のある追加パラメーターがいくつかあります。"

msgid ""
"Galera Cluster requires that you open the following ports to network traffic:"
msgstr ""
"Galera Cluster は、ネットワーク通信のために以下のポートを開く必要があります。"

msgid "Galera can be configured using one of the following strategies:"
msgstr "Galera は、以下の方法のどれかにより設定できます。"

msgid "Galera runs behind HAProxy:"
msgstr "Galera は HAProxy の後ろで動作します。"

msgid ""
"Galera synchronous replication guarantees a zero slave lag. The failover "
"procedure completes once HAProxy detects that the active back end has gone "
"down and switches to the backup one, which is then marked as ``UP``. If no "
"back ends are ``UP``, the failover procedure finishes only when the Galera "
"Cluster has been successfully reassembled. The SLA is normally no more than "
"5 minutes."
msgstr ""
"Galera の同期レプリケーションは、スレーブのラグがないことを保証します。フェイ"
"ルオーバー手順は、アクティブなバックエンドがダウンしたことを HAProxy が検知す"
"ると、バックアップに切り替え、``UP`` 状態になります。バックエンドが ``UP`` に"
"ならない場合、Galera クラスターが再び正常に再構成された場合のみ、フェイルオー"
"バー手順が完了します。SLA は、通常 5 分以内です。"

msgid ""
"Generally, we use round-robin to distribute load amongst instances of active/"
"active services. Alternatively, Galera uses ``stack-table`` options to "
"ensure that incoming connection to virtual IP (VIP) are directed to only one "
"of the available back ends. This helps avoid lock contention and prevent "
"deadlocks, although Galera can run active/active. Used in combination with "
"the ``httpchk`` option, this ensure only nodes that are in sync with their "
"peers are allowed to handle requests."
msgstr ""
"一般的に、アクティブ/アクティブなサービスのインスタンス間で負荷を分散するため"
"に、ラウンドロビンを使用します。代わりに、Galera は ``stack-table`` オプショ"
"ンを使用して、仮想 IP (VIP) への接続コネクションが利用可能なバックエンドの 1 "
"つのみにリダイレクトされることを確実にします。これにより、Galera がアクティ"
"ブ/アクティブで動作できますが、ロック競争を避け、デッドロックを防ぎます。"
"``httpchk`` オプションを組み合わせて使用すると、ピアー間で同期されているノー"
"ドのみがリクエストを処理できることを保証します。"

msgid "HAProxy"
msgstr "HAProxy"

msgid ""
"HAProxy load balances incoming requests and exposes just one IP address for "
"all the clients."
msgstr ""
"HAProxy は、受信リクエストを負荷分散して、すべてのクライアントに 1 つの IP ア"
"ドレスを公開します。"

msgid ""
"HAProxy provides a fast and reliable HTTP reverse proxy and load balancer "
"for TCP or HTTP applications. It is particularly suited for web crawling "
"under very high loads while needing persistence or Layer 7 processing. It "
"realistically supports tens of thousands of connections with recent hardware."
msgstr ""
"HAProxy は、TCP や HTTP ベースのアプリケーションに、高速かつ高信頼な HTTP リ"
"バースプロキシーとロードバランサーを提供します。とくに、永続性や L7 処理を必"
"要とする、非常に高負荷な Web サイトに適しています。最近のハードウェアを用いる"
"と、数千の接続を現実的にサポートします。"

msgid "Hardware considerations for high availability"
msgstr "高可用性のためのハードウェア考慮事項"

msgid "Hardware setup"
msgstr "ハードウェアのセットアップ"

msgid ""
"High availability is implemented with redundant hardware running redundant "
"instances of each service. If one piece of hardware running one instance of "
"a service fails, the system can then failover to use another instance of a "
"service that is running on hardware that did not fail."
msgstr ""
"高可用性は、各サービスの冗長インスタンスを実行する、冗長ハードウェアを用いて"
"実装されます。あるサービスのインスタンスの 1 つを実行しているハードウェアの部"
"品が故障した場合、システムはフェイルオーバーして、故障していないハードウェア"
"で動作している別のサービスインスタンスを使用します。"

msgid ""
"High availability is not for every user. It presents some challenges. High "
"availability may be too complex for databases or systems with large amounts "
"of data. Replication can slow large systems down. Different setups have "
"different prerequisites. Read the guidelines for each setup."
msgstr ""
"高可用性はあらゆるユーザー向けではありません。いくつかの挑戦を妨害します。高"
"可用性は、大量のデータを持つデータベースやシステムをあまりに複雑にする可能性"
"があります。レプリケーションは大規模システムをスローダウンさせる可能性があり"
"ます。異なるセットアップには、異なる事前要件があります。各セットアップのガイ"
"ドラインを参照してください。"

msgid "High availability is turned off as the default in OpenStack setups."
msgstr "高可用性は、デフォルトの OpenStack セットアップで無効化されています。"

msgid "High availability systems seek to minimize the following issues:"
msgstr "高可用性システムは、以下の問題を最小化することを目指しています。"

msgid ""
"High availability systems typically achieve an uptime percentage of 99.99% "
"or more, which roughly equates to less than an hour of cumulative downtime "
"per year. In order to achieve this, high availability systems should keep "
"recovery times after a failure to about one to two minutes, sometimes "
"significantly less."
msgstr ""
"高可用性システムは、一般的に 99.99% 以上の稼働率を達成します。おそよ年間 1 時"
"間未満の停止時間になります。高可用性システムは、これを実現するために、障害発"
"生後の復旧時間を 1 ～ 2 分以内に、ときにはさらに短く抑えるべきです。"

msgid "Highly available Block Storage API"
msgstr "高可用性 Block Storage API"

msgid "Highly available Identity API"
msgstr "高可用性 Identity API"

msgid "Highly available Image API"
msgstr "高可用性 Image API"

msgid "Highly available Shared File Systems API"
msgstr "高可用性 Shared File Systems API"

msgid "Highly available Telemetry"
msgstr "高可用性 Telemetry"

msgid "How long to back-off for between retries when connecting to RabbitMQ:"
msgstr "RabbitMQ に接続するとき再試行するまでにバックオフする間隔:"

msgid ""
"However, running an OpenStack environment on VMs degrades the performance of "
"your instances, particularly if your hypervisor or processor lacks support "
"for hardware acceleration of nested VMs."
msgstr ""
"しかしながら、仮想マシン上で OpenStack 環境を実行すると、インスタンスの性能が"
"悪くなります。とくに、ハイパーバイザーとプロセッサーが nested 仮想マシンの"
"ハードウェア支援機能をサポートしない場合は顕著です。"

msgid ""
"If the Block Storage service runs on the same nodes as the other services, "
"then it is advisable to also include:"
msgstr ""
"Block Storage サービスが他のサービスと同じノードで実行している場合、以下も含"
"めることを推奨します。"

msgid ""
"If the Identity service will be sending ceilometer notifications and your "
"message bus is configured for high availability, you will need to ensure "
"that the Identity service is correctly configured to use it. For details on "
"how to configure the Identity service for this kind of deployment, see :doc:"
"`shared-messaging`."
msgstr ""
"Identity サービスが ceilometer の通知を送信して、メッセージバスが高可用性のた"
"めに設定されている場合、Identity サービスがきちんとそれを使用するよう設定する"
"必要があります。この種の導入向けに Identity サービスを設定する方法の詳細は、:"
"doc:`shared-messaging` を参照してください。"

msgid ""
"If the ``broadcast`` parameter is set to ``yes``, the broadcast address is "
"used for communication. If this option is set, the ``mcastaddr`` parameter "
"should not be set."
msgstr ""
"``broadcast`` パラメーターが ``yes`` に設定されている場合、ブロードキャストア"
"ドレスが通信に使用されます。このオプションが設定されている場合、"
"``mcastaddr`` パラメーターは設定すべきではありません。"

msgid ""
"If the cluster is working, you can create usernames and passwords for the "
"queues."
msgstr ""
"クラスターが動作していると、キューのユーザー名とパスワードを作成できます。"

msgid ""
"If you are using Corosync version 2 on Ubuntu 14.04, remove or comment out "
"lines under the service stanza. These stanzas enable Pacemaker to start up. "
"Another potential problem is the boot and shutdown order of Corosync and "
"Pacemaker. To force Pacemaker to start after Corosync and stop before "
"Corosync, fix the start and kill symlinks manually:"
msgstr ""
"Ubuntu 14.04 において Corosync バージョン 2 を使用している場合、サービスの節"
"の下にある行を削除するかコメントアウトします。これらの節により、Pacemaker が"
"起動できます。別の潜在的な問題は、Corosync と Pacemaker の起動と停止の順番で"
"す。必ず Pacemaker が Corosync の後に起動して、Corosync  の前に停止させるため"
"に、start と kill のシンボリックリンクを手動で修正します。"

msgid ""
"If you are using Corosync version 2, use the :command:`corosync-cmapctl` "
"utility instead of :command:`corosync-objctl`; it is a direct replacement."
msgstr ""
"Corosync バージョン 2 を使用している場合、 :command:`corosync-objctl` の代わ"
"りに :command:`corosync-cmapctl` ユーティリティーを使用します。これは、そのま"
"ま置き換えられます。"

msgid ""
"If you are using both private and public IP addresses, create two virtual IP "
"addresses and define the endpoint. For example:"
msgstr ""
"プライベート IP とパブリック IP の両方を使用する場合、2 つの仮想 IP アドレス"
"を作成し、次のようにエンドポイントを定義します。"

msgid ""
"If you are using both private and public IP addresses, create two virtual "
"IPs and define your endpoint. For example:"
msgstr ""
"プライベート IP アドレスとパブリック IP アドレスの両方を使用する場合、2 つの"
"仮想 IP アドレスを作成し、次のようにエンドポイントを定義します。"

msgid ""
"If you are using both private and public IP addresses, you should create two "
"virtual IPs and define your endpoints like this:"
msgstr ""
"プライベート IP アドレスとパブリック IP アドレスの両方を使用する場合、2 つの"
"仮想 IP アドレスを作成し、次のようにエンドポイントを定義すべきです。"

msgid ""
"If you are using the Block Storage service OCF agent, some settings will be "
"filled in for you, resulting in a shorter configuration file:"
msgstr ""
"Block Storage サービス OCF エージェントを使用している場合、いくつかの設定は入"
"力されていて、設定ファイルを短くできます。"

msgid ""
"If you are using the horizon Dashboard, edit the :file:`local_settings.py` "
"file to include the following:"
msgstr ""
"Dashboard を使用している場合、以下の内容を含めた :file:`local_settings.py` "
"ファイルを編集します。"

msgid ""
"If you change the configuration from an old set-up that did not use HA "
"queues, restart the service:"
msgstr ""
"HA キューを使用していない古いセットアップから設定を変更した場合、サービスを再"
"起動します。"

msgid ""
"If you use HAProxy as a load-balancing client to provide access to the "
"Galera Cluster, as described in the :doc:`controller-ha-haproxy`, you can "
"use the ``clustercheck`` utility to improve health checks."
msgstr ""
":doc:`controller-ha-haproxy` に記載されているとおり、Galera Cluster へのクラ"
"イアントアクセスを負荷分散するために、HAProxy を使用している場合、 "
"``clustercheck`` ユーティリティーを使用して、より良くヘルスチェックできます。"

msgid ""
"In Corosync, configurations use redundant networking (with more than one "
"interface). This means you must select a Redundant Ring Protocol (RRP) mode "
"other than none. We recommend ``active`` as the RRP mode."
msgstr ""
"Corosync において、設定は (複数のインターフェースを用いた) 冗長ネットワークを"
"使用します。これは ``none`` ではなく、Redundant Ring Protocol (RRP) を選択す"
"る必要があることを意味します。``active`` が RRP の推奨モードです。"

msgid ""
"In Red Hat Enterprise Linux or CentOS environments, this is a recommended "
"path to perform configuration. For more information, see the `RHEL docs "
"<https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/"
"html/High_Availability_Add-On_Reference/ch-clusteradmin-HAAR.html#s1-"
"clustercreate-HAAR>`_."
msgstr ""
"Red Hat Enterprise Linux や CentOS 環境の場合、設定するための推奨パスがありま"
"す。詳細は `RHEL docs <https://access.redhat.com/documentation/en-US/"
"Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Reference/ch-"
"clusteradmin-HAAR.html#s1-clustercreate-HAAR>`_ を参照してください。"

msgid ""
"In a collapsed configuration, there is a single cluster of 3 or more nodes "
"on which every component is running."
msgstr ""
"この折りたたまれた設定では、すべてのコンポーネントが動作する、3 つ以上のノー"
"ドを持つシングルクラスターがあります。"

msgid ""
"In addition to Galera Cluster, you can also achieve high availability "
"through other database options, such as PostgreSQL, which has its own "
"replication system."
msgstr ""
"Galera Cluster 以外に、独自のレプリケーションシステムを持つ PostgreSQL など、"
"他のデータベースにより高可用性を実現することもできます。"

msgid ""
"In general, we can divide all the OpenStack components into three categories:"
msgstr ""
"一般的に、すべての OpenStack コンポーネントは 3 つのカテゴリーに分割できま"
"す。"

msgid ""
"In the Galera Cluster, the Primary Component is the cluster of database "
"servers that replicate into each other. In the event that a cluster node "
"loses connectivity with the Primary Component, it defaults into a non-"
"operational state, to avoid creating or serving inconsistent data."
msgstr ""
"Galera Cluster では、Primary Component が、お互いにレプリケーションするデータ"
"ベースサーバーのクラスターです。クラスターノードが Primary Component との接続"
"性を失った場合、不整合なデータの作成や処理を避けるために、デフォルトで非稼働"
"状態になります。"

msgid ""
"In the event that a component fails and a back-up system must take on its "
"load, most high availability systems will replace the failed component as "
"quickly as possible to maintain necessary redundancy. This way time spent in "
"a degraded protection state is minimized."
msgstr ""
"コンポーネントが故障して、バックアップシステムがその負荷を引き継ぐ場合、多く"
"の高可用性システムは、十分な冗長性を維持するために、できる限り早く故障したコ"
"ンポーネントを置き換えます。この方法は、デグレードされた保護状態を最小化する"
"ことに時間を使います。"

msgid ""
"In the event that you need to restart any cluster node, you can do so. When "
"the database server comes back it, it establishes connectivity with the "
"Primary Component and updates itself to any changes it may have missed while "
"down."
msgstr ""
"クラスターノードをどれか再起動する必要がある場合、実行できます。データベース"
"サーバーが戻ってきたとき、Primary Component との接続を確立して、停止中に失っ"
"た変更をすべて自身に適用します。"

msgid ""
"In theory, you can run the Block Storage service as active/active. However, "
"because of sufficient concerns, we recommend running the volume component as "
"active/passive only."
msgstr ""
"理論的には、Block Storage サービスをアクティブ/アクティブとして実行できます。"
"しかしながら、いくつかの課題のため、ボリュームコンポーネントをアクティブ/パッ"
"シブのみとして実行することが推奨されます。"

msgid ""
"In this configuration, each service runs in a dedicated cluster of 3 or more "
"nodes."
msgstr ""
"この設定では、各サービスが 3 以上のノードの専用クラスターで動作します。"

msgid ""
"Individual cluster nodes can stop and be restarted without issue. When a "
"database loses its connection or restarts, the Galera Cluster brings it back "
"into sync once it reestablishes connection with the Primary Component. In "
"the event that you need to restart the entire cluster, identify the most "
"advanced cluster node and initialize the Primary Component on that node."
msgstr ""
"各クラスターノードは、問題なく停止したり再起動したりできます。データベースが"
"接続を失ったり、再起動したりしたとき、Primary Component と再接続されると、"
"Galera Cluster は同期状態に戻ります。クラスター全体を再起動する必要があると"
"き、最も高度なクラスターノードを識別し、そのノードの Primary Component を初期"
"化します。"

msgid ""
"Initialize the Primary Component on one cluster node. For servers that use "
"``init``, run the following command:"
msgstr ""
"1 つのクラスターノードにおいて Primary Component を初期化します。``init`` を"
"使用するサーバーの場合、以下のコマンドを実行します。"

msgid "Initializing the cluster"
msgstr "クラスターの初期化"

msgid "Install RabbitMQ"
msgstr "RabbitMQ のインストール"

msgid "Install packages"
msgstr "パッケージのインストール"

msgid "Installing Memcached"
msgstr "Memcached のインストール"

msgid "Installing the operating system"
msgstr "オペレーティングシステムのインストール"

msgid "Introduction to OpenStack high availability"
msgstr "OpenStack 高可用性の概要"

msgid ""
"It is also possible to follow a segregated approach for one or more "
"components that are expected to be a bottleneck and use a collapsed approach "
"for the remainder."
msgstr ""
"1 つ以上のコンポーネントに対して、別々のアプローチをとることができますが、ボ"
"トルネックになり、思い出すことが難しいアプローチを使用する可能性があります。"

msgid ""
"It is possible to add controllers to such an environment to convert it into "
"a truly highly available environment."
msgstr ""
"コントローラーをそのような環境に追加して、それを信頼できる高可用性環境に変え"
"られます。"

msgid ""
"It is possible to deploy three different flavors of the Pacemaker "
"architecture. The two extremes are ``Collapsed`` (where every component runs "
"on every node) and ``Segregated`` (where every component runs in its own 3+ "
"node cluster)."
msgstr ""
"3 種類の Pacemaker アーキテクチャーを導入できます。2 つの極端なものは、"
"``Collapsed`` (すべてのコンポーネントがすべてのノードで動作する) と "
"``Segregated`` (すべてのコンポーネントが自身の 3+ ノードクラスターで動作す"
"る) です。"

msgid ""
"It is storage and application-agnostic, and in no way specific to OpenStack."
msgstr ""
"ストレージとアプリケーションから独立していて、OpenStack 特有の方法はありませ"
"ん。"

msgid ""
"It is very important that all members of the system share the same view of "
"who their peers are and whether or not they are in the majority. Failure to "
"do this leads very quickly to an internal `split-brain <https://en.wikipedia."
"org/wiki/Split-brain_(computing)>`_ state. This is where different parts of "
"the system are pulling in different and incompatible directions."
msgstr ""
"システムのすべてのメンバーが、誰がメンバーであるか、それらが多数派かどうかに"
"ついて、同じビューを共有することが非常に重要です。これを行う障害は、かなりす"
"ぐに `スプリットブレイン <https://en.wikipedia.org/wiki/Split-"
"brain_(computing)>`_ 状態を引き起こします。これは、システムの別々の部分が、"
"別々な互換性のない方向を引き込むことです。"

msgid "List the nodes known to the quorum service"
msgstr "クォーラムサービスが把握しているノードの一覧表示"

msgid "Load distribution"
msgstr "負荷分散"

msgid ""
"Log in to the database client and grant the ``clustercheck`` user "
"``PROCESS`` privileges:"
msgstr ""
"データベースクライアントにログインして、``clustercheck`` ユーザーに "
"``PROCESS`` 権限を与えます。"

msgid ""
"Maintains a redundant instance that can be brought online when the active "
"service fails. For example, OpenStack writes to the main database while "
"maintaining a disaster recovery database that can be brought online if the "
"main database fails."
msgstr ""
"動作中のサービスが停止したとき、オンラインにできる冗長インスタンスを維持しま"
"す。例えば、メインのデータベースが故障したとき、オンラインになる災害対策デー"
"タベースを維持する限り、OpenStack はメインのデータベースに書き込みます。"

msgid "Make sure `pcs` is running and configured to start at boot time:"
msgstr ""
"`pcs` が実行中で、ブート時に起動するよう設定されていることを確認してくださ"
"い。"

msgid ""
"Make sure to save the changes once you are done. This will vary depending on "
"your distribution:"
msgstr ""
"完了後、きちんと変更を保存してください。これは、お使いのディストリビューショ"
"ンにより異なります。"

msgid ""
"Making the Block Storage (cinder) API service highly available in active/"
"active mode involves:"
msgstr ""
"Block Storage (cinder) API サービスのアクティブ/アクティブモードでの高可用性"
"は、以下が関係します。"

msgid ""
"Making the Block Storage API service highly available in active/passive mode "
"involves:"
msgstr ""
"Block Storage API サービスのアクティブ/パッシブモードでの高可用性は、以下が関"
"係します。"

msgid ""
"Making the OpenStack Identity service highly available in active and passive "
"mode involves:"
msgstr ""
"OpenStack Identity Service をアクティブ・パッシブモードで高可用性にすること"
"は、次のことが関連します。"

msgid ""
"Making the RabbitMQ service highly available involves the following steps:"
msgstr "RabbitMQ サービスを高可用性にすることは、以下の手順が関連します。"

msgid ""
"Making the Shared File Systems (manila) API service highly available in "
"active/passive mode involves:"
msgstr ""
"Shared File Systems (manila) API サービスのアクティブ/パッシブモードでの高可"
"用性は、以下が関係します。"

msgid "Management"
msgstr "マネジメント"

msgid ""
"Managing the Block Storage API daemon with the Pacemaker cluster manager"
msgstr ""
"Pacemaker クラスターマネージャーを用いた Block Storge API デーモンの管理"

msgid ""
"Many services can act in an active/active capacity, however, they usually "
"require an external mechanism for distributing requests to one of the "
"available instances. The proxy server can serve this role."
msgstr ""
"ほとんどのサービスがアクティブ/アクティブ機能で動作できます。しかしながら、通"
"常は分散されたリクエストが利用できるインスタンスのどれかになる外部機能が必要"
"になります。プロキシーサーバーはこの役割になれます。"

msgid "Maximum number of network nodes to use for the HA router."
msgstr "HA ルーターのために使用するネットワークノードの最大数"

msgid ""
"Maximum retries with trying to connect to RabbitMQ (infinite by default):"
msgstr "RabbitMQ に接続を試行する最大回数 (デフォルトで無制限):"

msgid "Memcached"
msgstr "Memcached"

msgid ""
"Memcached is a general-purpose distributed memory caching system. It is used "
"to speed up dynamic database-driven websites by caching data and objects in "
"RAM to reduce the number of times an external data source must be read."
msgstr ""
"Memcached は汎用の分散メモリーキャッシュシステムです。データやオブジェクトを"
"メモリーにキャッシュすることにより、外部データソースの読み込み回数を減らし、"
"データベースを利用した動的 Web サイトを高速化するために使用されます。"

msgid ""
"Memcached is a memory cache demon that can be used by most OpenStack "
"services to store ephemeral data, such as tokens."
msgstr ""
"Memcached は、ほとんどの OpenStack サービスがトークンなどの一時的なデータを保"
"存するために使用できる、メモリーキャッシュのデーモンです。"

msgid ""
"Memcached uses a timeout value, which should always be set to a value that "
"is higher than the heartbeat value set for Telemetry."
msgstr ""
"Memcached は、タイムアウト値を使用します。これは、Telemetry 向けに設定された"
"ハートビート値よりも大きい値を常に設定されるべきです。"

msgid "Memory"
msgstr "メモリー"

msgid ""
"Memory caching is managed by `oslo.cache <http://specs.openstack.org/"
"openstack/oslo-specs/specs/kilo/oslo-cache-using-dogpile.html>`_. This "
"ensures consistency across all projects when using multiple Memcached "
"servers. The following is an example configuration with three hosts:"
msgstr ""
"メモリーキャッシュは `oslo.cache <http://specs.openstack.org/openstack/oslo-"
"specs/specs/kilo/oslo-cache-using-dogpile.html>`_ により管理されます。これに"
"より、複数の Memcached サーバーの使用時に全プロジェクト間で一貫性を保証できま"
"す。以下の例は 3 ノードの設定例です。"

msgid "Messaging service for high availability"
msgstr "メッセージサービスの高可用性"

msgid ""
"Minimum number of network nodes to use for the HA router. A new router can "
"be created only if this number of network nodes are available."
msgstr ""
"HA ルーターのために使用するネットワークノードの最小数。この数だけのネットワー"
"クノードを利用できる場合のみ、新規ルーターを作成できます。"

msgid ""
"Mirrored queues in RabbitMQ improve the availability of service since it is "
"resilient to failures."
msgstr ""
"RabbitMQ のキューミラーは、障害耐性があるので、サービスの可用性を改善します。"

msgid "Mixed"
msgstr "Mixed"

msgid "MongoDB"
msgstr "MongoDB"

msgid "More information is available in the RabbitMQ documentation:"
msgstr "詳細は RabbitMQ のドキュメントにあります。"

msgid ""
"Most OpenStack services can use Memcached to store ephemeral data such as "
"tokens. Although Memcached does not support typical forms of redundancy such "
"as clustering, OpenStack services can use almost any number of instances by "
"configuring multiple hostnames or IP addresses."
msgstr ""
"ほとんどの OpenStack サービスは、トークンなどの一時データを保存するために "
"Memcached を使用できます。Memcached はクラスターなどの一般的な形式の冗長化を"
"サポートしませんが、OpenStack サービスは複数のホスト名や IP アドレスを設定す"
"ることにより、ほぼ任意の数のインスタンスを使用できます。"

msgid ""
"Most distributions ship an example configuration file (:file:`corosync.conf."
"example`) as part of the documentation bundled with the Corosync package. An "
"example Corosync configuration file is shown below:"
msgstr ""
"ほとんどのディストリビューションは、Corosync パッケージに同梱されているドキュ"
"メントの一部として、サンプル設定ファイル (:file:`corosync.conf.example`) を同"
"梱しています。"

msgid ""
"Most high availability systems fail in the event of multiple independent "
"(non-consequential) failures. In this case, most implementations favor "
"protecting data over maintaining availability."
msgstr ""
"多くの高可用性システムは、複数の独立した (不連続な) 障害が発生すると停止しま"
"す。この場合、多くのシステムは可用性の維持よりデータを保護することを優先しま"
"す。"

msgid ""
"Most high availability systems guarantee protection against system downtime "
"and data loss only in the event of a single failure. However, they are also "
"expected to protect against cascading failures, where a single failure "
"deteriorates into a series of consequential failures. Many service providers "
"guarantee a :term:`Service Level Agreement (SLA)` including uptime "
"percentage of computing service, which is calculated based on the available "
"time and system downtime excluding planned outage time."
msgstr ""
"多くの高可用性システムは、単一障害事象のみにおいて、システム停止時間やデータ"
"損失に対する保護を保証します。しかしながら、単一障害が一連の障害を悪化させて"
"いく、段階的な障害に対しても保護されることが期待されます。多くのサービスプロ"
"バイダーは、コンピューティングサービスの稼働率などの :term:`Service Level "
"Agreement (SLA)` を保証します。それは、計画停止を除くシステム停止時間と稼働時"
"間に基づいて計算されます。"

msgid ""
"Multicast groups (``mcastaddr``) must not be reused across cluster "
"boundaries. No two distinct clusters should ever use the same multicast "
"group. Be sure to select multicast addresses compliant with `RFC 2365, "
"\"Administratively Scoped IP Multicast\" <http://www.ietf.org/rfc/rfc2365."
"txt>`_."
msgstr ""
"マルチキャストグループ (``mcastaddr``) は、クラスターの境界を越えて再利用でき"
"ません。2 つの独立したクラスターは、同じマルチキャストグループを使用すべきで"
"はありません。選択したマルチキャストアドレス をきちんと`RFC 2365, "
"\"Administratively Scoped IP Multicast\" <http://www.ietf.org/rfc/rfc2365."
"txt>`_ に準拠させてください。"

msgid ""
"MySQL databases, including MariaDB and Percona XtraDB, manage their "
"configurations using a ``my.cnf`` file, which is typically located in the ``/"
"etc`` directory. Configuration options available in these databases are also "
"available in Galera Cluster, with some restrictions and several additions."
msgstr ""
"MariaDB や Percona XtraDB を含む、MySQL は ``my.cnf`` ファイルを使用して設定"
"を管理します。一般的に ``/etc`` ディレクトリーにあります。これらのデータベー"
"スにおいて利用できる設定オプションは、Galera Cluster においても利用できます。"
"いくつかの制約や追加があります。"

msgid "NIC"
msgstr "NIC"

msgid "Network components, such as switches and routers"
msgstr "スイッチやルーターなどのネットワークの構成要素"

msgid "Networking L2 agent"
msgstr "Neutron L2 エージェント"

msgid "No firewalls between the hosts"
msgstr "ホスト間のファイアウォールなし"

msgid "Node type"
msgstr "ノード種別"

msgid "Note the following about the recommended interface configuration:"
msgstr "インターフェースの推奨設定に関する注意事項がいくつかあります。"

msgid "Note the following:"
msgstr "以下に注意してください。"

msgid ""
"Older versions of some distributions, which do not have an up-to-date policy "
"for securing Galera, may also require SELinux to be more relaxed about "
"database access and actions:"
msgstr ""
"いくつかのディストリビューションの古いバージョンは、Galera をセキュア化するた"
"めの最新ポリシーを提供していません。データベースへのアクセスと操作のために "
"SELinux をもう少しゆるく設定する必要があるかもしれません。"

msgid "On CentOS, RHEL, openSUSE, and SLES:"
msgstr "CentOS、RHEL、openSUSE、SLES の場合:"

msgid ""
"On RHEL-based systems, create resources for cinder's systemd agents and "
"create constraints to enforce startup/shutdown ordering:"
msgstr ""
"RHEL 系のシステムでは、cinder の systemd エージェント向けリソースを作成して、"
"起動と停止の順番を強制する制約を作成します。"

msgid ""
"On ``3306``, Galera Cluster uses TCP for database client connections and "
"State Snapshot Transfers methods that require the client, (that is, "
"``mysqldump``)."
msgstr ""
"``3306`` では、Galera Cluster がデータベースクライアント接続のために TCP を使"
"用します。また、クライアント 、つまり ``mysqldump`` を必要とする State "
"Snapshot Transfers メソッドを使用します。"

msgid ""
"On ``4444``, Galera Cluster uses TCP for all other State Snapshot Transfer "
"methods."
msgstr ""
"``4444`` では、Galera Cluster が他のすべての State Snapshot Transfer メソッド"
"のために TCP を使用します。"

msgid ""
"On ``4567``, Galera Cluster uses TCP for replication traffic. Multicast "
"replication uses both TCP and UDP on this port."
msgstr ""
"``4567`` では、Galera Cluster が複製通信のために TCP を使用します。マルチキャ"
"ストレプリケーションは、このポートで TCP と UDP を使用します。"

msgid "On ``4568``, Galera Cluster uses TCP for Incremental State Transfers."
msgstr ""
"``4568`` では、Galera Cluster が Incremental State Transfers のために TCP を"
"使用します。"

msgid ""
"On any host that is meant to be part of a Pacemaker cluster, establish "
"cluster communications through the Corosync messaging layer. This involves "
"installing the following packages (and their dependencies, which your "
"package manager usually installs automatically):"
msgstr ""
"Pacemaker クラスターに参加させる各ホストで、まず Corosync メッセージレイヤー"
"でクラスター通信を確立します。これには、以下のパッケージをインストールする必"
"要があります (依存パッケージも含みます。依存パッケージは通常パッケージマネー"
"ジャーにより自動的にインストールされます)。"

msgid ""
"On each target node, verify the correct owner, group, and permissions of the "
"file :file:`erlang.cookie`:"
msgstr ""
"各ターゲットノードにおいて、 :file:`erlang.cookie` の所有者、所有グループ、"
"パーミッションが正しいことを確認します。"

msgid ""
"On the infrastructure layer, the SLA is the time for which RabbitMQ cluster "
"reassembles. Several cases are possible. The Mnesia keeper node is the "
"master of the corresponding Pacemaker resource for RabbitMQ. When it fails, "
"the result is a full AMQP cluster downtime interval. Normally, its SLA is no "
"more than several minutes. Failure of another node that is a slave of the "
"corresponding Pacemaker resource for RabbitMQ results in no AMQP cluster "
"downtime at all."
msgstr ""
"インフラ層では、SLA は RabbitMQ クラスターが再構成されるまでの時間です。いく"
"つかの場合では実現できます。Mnesia keeper ノードは、対応する RabbitMQ 用 "
"Pacemaker リソースのマスターです。停止したとき、結果として AMQP クラスターの"
"停止時間になります。通常、その SLA は、数分間より長くなることはありません。対"
"応する RabbitMQ 用 Pacemaker リソースのスレーブになっている、他のノードの停止"
"により AMQP クラスターが停止することはありません。"

msgid ""
"Once completed, commit your configuration changes by entering :command:"
"`commit` from the :command:`crm configure` menu. Pacemaker then starts the "
"Block Storage API service and its dependent resources on one of your nodes."
msgstr ""
"これらの手順の完了後、:command:`crm configure` メニューから :command:"
"`commit` と入力し、設定の変更をコミットします。Pacemaker は Block Storage "
"API サービスおよび依存するリソースを同じノードに起動します。"

msgid ""
"Once created, synchronize the :file:`corosync.conf` file (and the :file:"
"`authkey` file if the secauth option is enabled) across all cluster nodes."
msgstr ""
"作成され同期された後、 :file:`corosync.conf` ファイル (および、secauth オプ"
"ションが有効化されている場合、 :file:`authkey` ファイル) が、すべてのクラス"
"ターノードにわたり同期されます。"

msgid ""
"Once the database server starts, check the cluster status using the "
"``wsrep_cluster_size`` status variable. From the database client, run the "
"following command:"
msgstr ""
"データベースサーバーが起動すると、``wsrep_cluster_size`` 状態変数を使用して、"
"クラスター状態を確認します。データベースクライアントから、以下のコマンドを実"
"行します。"

msgid ""
"One physical server can support multiple nodes, each of which supports "
"almost any number of network interfaces."
msgstr ""
"1 台の物理サーバーで複数のノードを構築できます。各ノードは複数のネットワーク"
"インターフェースを持てます。"

msgid ""
"Only one instance for the central and compute agent service(s) is able to "
"run and function correctly if the ``backend_url`` option is not set."
msgstr ""
"``backend_url`` オプションが設定されていない場合、中央エージェントとコン"
"ピュートエージェントのサービスのインスタンスが 、1 つだけ動作できて正しく機能"
"します。"

msgid ""
"OpenStack APIs: APIs that are HTTP(s) stateless services written in python, "
"easy to duplicate and mostly easy to load balance."
msgstr ""
"OpenStack API: これらは HTTP のステートレスサービスです。Python で書かれてい"
"て、簡単に冗長化でき、かなり簡単に負荷分散できます。"

msgid "OpenStack Block Storage"
msgstr "OpenStack Block Storage"

msgid "OpenStack Compute"
msgstr "OpenStack Compute"

msgid "OpenStack High Availability Guide"
msgstr "OpenStack 高可用性ガイド"

msgid "OpenStack Networking"
msgstr "OpenStack Networking"

msgid ""
"OpenStack currently meets such availability requirements for its own "
"infrastructure services, meaning that an uptime of 99.99% is feasible for "
"the OpenStack infrastructure proper. However, OpenStack does not guarantee "
"99.99% availability for individual guest instances."
msgstr ""
"OpenStack 自体のインフラストラクチャーは、現在その可用性要件を満たせます。つ"
"まり、適切な OpenStack インフラストラクチャーの 99.99% の稼働率が実現可能で"
"す。しかしながら、OpenStack は個々のゲストインスタンスの可用性 99.99% を保証"
"できません。"

msgid ""
"OpenStack does not require a significant amount of resources and the "
"following minimum requirements should support a proof-of-concept high "
"availability environment with core services and several instances:"
msgstr ""
"OpenStack は膨大なリソースを必要としません。以下の最小要件は、コアサービスと"
"いくつかのインスタンスを動かす検証 (POC) 環境には対応できることでしょう。"

msgid ""
"OpenStack is a set of services exposed to the end users as HTTP(s) APIs. "
"Additionally, for your own internal usage, OpenStack requires an SQL "
"database server and AMQP broker. The physical servers, where all the "
"components are running, are called controllers. This modular OpenStack "
"architecture allows you to duplicate all the components and run them on "
"different controllers. By making all the components redundant, it is "
"possible to make OpenStack highly available."
msgstr ""
"OpenStack は、HTTP(s) API としてエンドユーザーに公開されるサービス群です。さ"
"らに、その内部利用のために、OpenStack は SQL データベースサーバーと AMQP ブ"
"ローカーを必要とします。すべてのコンポーネントが動作している、物理サーバーは"
"よくコントローラーと呼ばれます。このモジュール型の OpenStack アーキテクチャー"
"により、すべてのコンポーネントを複製して、それらを別々のコントローラーで実行"
"できます。すべてのコンポーネントを冗長にすることにより、OpenStack の高可用性"
"を実現できます。"

msgid "OpenStack network nodes contain:"
msgstr "OpenStack ネットワークノードでは、以下のものが動作します。"

msgid ""
"OpenStack services are configured with the list of these IP addresses so "
"they can select one of the addresses from those available."
msgstr ""
"OpenStack サービスは、利用できるものから 1 つを選択できるよう、これらの IP ア"
"ドレスの一覧を用いて設定されます。"

msgid ""
"OpenStack supports a single-controller high availability mode that is "
"managed by the services that manage highly available environments but is not "
"actually highly available because no redundant controllers are configured to "
"use for failover. This environment can be used for study and demonstration "
"but is not appropriate for a production environment."
msgstr ""
"OpenStack は、シングルコントローラーの高可用性モードをサポートします。これ"
"は、高可用性環境を管理するソフトウェアにより、サービスが管理されますが、コン"
"トローラーがフェイルオーバーのために冗長化設定されていないため、実際には高可"
"用性ではありません。この環境は、学習やデモのために使用できますが、本番環境と"
"しては適していません。"

msgid "Overview of highly available controllers"
msgstr "高可用性コントローラーの概要"

msgid "Pacemaker cluster stack"
msgstr "Pacemaker クラスタースタック"

msgid ""
"Pacemaker does not inherently understand the applications it manages. "
"Instead, it relies on resource agents (RAs) that are scripts that "
"encapsulate the knowledge of how to start, stop, and check the health of "
"each application managed by the cluster."
msgstr ""
"Pacemaker は、管理するアプリケーションを本質的に理解してません。代わりに、リ"
"ソースエージェント (RA) に依存します。これは、クラスターにより管理される各ア"
"プリケーションの起動、停止、ヘルスチェック方法に関する知識を隠蔽するスクリプ"
"トです。"

msgid ""
"Pacemaker now starts the OpenStack Identity service and its dependent "
"resources on all of your nodes."
msgstr ""
"Pacemaker は OpenStack Identity API サービスおよび依存するリソースをすべての"
"ノードに起動します。"

msgid ""
"Pacemaker now starts the Shared File Systems API service and its dependent "
"resources on one of your nodes."
msgstr ""
"Pacemaker は Shared File Systems API サービスおよび依存するリソースを同じノー"
"ドに起動します。"

msgid ""
"Pacemaker ships with a large set of OCF agents (such as those managing MySQL "
"databases, virtual IP addresses, and RabbitMQ), but can also use any agents "
"already installed on your system and can be extended with your own (see the "
"`developer guide <http://www.linux-ha.org/doc/dev-guides/ra-dev-guide."
"html>`_)."
msgstr ""
"Pacemaker は、(MySQL データベース、仮想 IP アドレス、RabbitMQ などの) OCF "
"エージェントをたくさん同梱していますが、お使いのシステムにインストールした任"
"意のエージェントも使用できます。また、自身で拡張することもできます "
"(`developer guide <http://www.linux-ha.org/doc/dev-guides/ra-dev-guide."
"html>`_ 参照)。"

msgid ""
"Pacemaker then starts the OpenStack Image API service and its dependent "
"resources on one of your nodes."
msgstr ""
"Pacemaker は OpenStack Image API サービスおよび依存するリソースを同じノードに"
"起動します。"

msgid ""
"Pacemaker uses an event-driven approach to cluster state processing. The "
"``cluster-recheck-interval`` parameter (which defaults to 15 minutes) "
"defines the interval at which certain Pacemaker actions occur. It is usually "
"prudent to reduce this to a shorter interval, such as 5 or 3 minutes."
msgstr ""
"Pacemaker は、クラスターの状態を処理するために、イベントドリブンのアプローチ"
"を使用します。 ``cluster-recheck-interval`` パラメーター (デフォルトは 15 "
"分) が、ある Pacemaker のアクションが発生する間隔を定義します。通常、5 分や "
"3 分など、より短い間隔に減らすことは慎重になるべきです。"

msgid "Parameter"
msgstr "パラメーター"

msgid ""
"Persistent block storage can survive instance termination and can also be "
"moved across instances like any external storage device. Cinder also has "
"volume snapshots capability for backing up the volumes."
msgstr ""
"永続ブロックストレージは、インスタンス終了後に残存して、任意の外部ストレージ"
"デバイスのようにインスタンスを越えて移動できます。Cinder は、ボリュームをバッ"
"クアップするために、ボリュームスナップショット機能も持ちます。"

msgid ""
"Persistent storage exists outside all instances. Two types of persistent "
"storage are provided:"
msgstr ""
"永続ストレージは、すべてのインスタンスの外部にあります。2 種類の永続ストレー"
"ジが提供されます。"

msgid "Possible options are:"
msgstr "利用できるオプションは次のとおりです。"

msgid "Prerequisites"
msgstr "前提条件"

msgid "Processor Cores"
msgstr "プロセッサーのコア"

msgid ""
"Production servers should run (at least) three RabbitMQ servers for testing "
"and demonstration purposes, however it is possible to run only two servers. "
"In this section, we configure two nodes, called ``rabbit1`` and ``rabbit2``. "
"To build a broker, ensure that all nodes have the same Erlang cookie file."
msgstr ""
"本番サーバーは、(少なくとも) 3 つの RabbitMQ サーバーを実行すべきです。しかし"
"ながらテストやデモの目的の場合、サーバーを 2 つだけ実行することもできます。こ"
"のセクションでは、``rabbit1`` と ``rabbit2`` という 2 つのノードを設定しま"
"す。ブローカーを構築するために、すべてのノードがきちんと同じ Erlang クッキー"
"ファイルを持ちます。"

msgid "Proxy server"
msgstr "プロキシーサーバー"

msgid "Query the quorum status"
msgstr "クォーラム状態を問い合わせます"

msgid ""
"Quorum becomes important when a failure causes the cluster to split in two "
"or more partitions. In this situation, you want the majority members of the "
"system to ensure the minority are truly dead (through fencing) and continue "
"to host resources. For a two-node cluster, no side has the majority and you "
"can end up in a situation where both sides fence each other, or both sides "
"are running the same services. This can lead to data corruption."
msgstr ""
"障害がクラスターを 2 つ以上のパーティションに分割した場合、クォーラムは重要に"
"なります。この状況では、システムの多数派のメンバーが、少数派を確実に (フェン"
"ス経由で) 停止させ、ホストリソースを継続することを確実にしたいでしょう。2 "
"ノードクラスターの場合、多数派になる側がなく、両方がお互いをフェンスする状"
"況、または両方が同じサービスを実行する状況になる可能性があります。これはデー"
"タ破損を引き起こします。"

msgid "RAID drives"
msgstr "RAID ドライブ"

msgid "RabbitMQ"
msgstr "RabbitMQ"

msgid ""
"RabbitMQ nodes fail over on the application and the infrastructure layers."
msgstr ""
"RabbitMQ ノードは、アプリケーションとインフラ層の両方においてフェイルオーバー"
"します。"

msgid "Receive notifications of quorum state changes"
msgstr "クォーラムの状態変更の通知を受け付けます"

msgid "Recommended for testing."
msgstr "テスト向けの推奨。"

msgid "Recommended solution by the Tooz project."
msgstr "Tooz プロジェクトによる推奨ソリューション。"

msgid "Red Hat"
msgstr "Red Hat"

msgid "Redundancy and failover"
msgstr "冗長性とフェールオーバー"

msgid ""
"Regardless of which flavor you choose, we recommend that clusters contain at "
"least three nodes so that you can take advantage of `quorum <quorum_>`_."
msgstr ""
"選択するフレーバーに関わらず、クラスターは `quorum <quorum_>`_ の利点を得るた"
"めに、少なくとも 3 ノードを持つことを推奨します。"

msgid ""
"Replace ``CINDER_DBPASS`` with the password you chose for the Block Storage "
"database."
msgstr ""
"``CINDER_DBPASS`` を Block Storage データベース用に選択したパスワードで置き換"
"えます。"

msgid ""
"Replace ``CINDER_DBPASS`` with the password you chose for the Block Storage "
"database. Replace ``CINDER_PASS`` with the password you chose for the "
"``cinder`` user in the Identity service."
msgstr ""
"``CINDER_DBPASS`` を Block Storage サービス用に選択したパスワードで置き換えま"
"す。``CINDER_PASS`` を Identity サービスで ``cinder`` ユーザー用に選択したパ"
"スワードで置き換えます。"

msgid ""
"Replace the IP addresses given here with comma-separated list of each "
"OpenStack database in your cluster."
msgstr ""
"ここで指定された IP アドレスを、お使いのクラスターにある OpenStack の各データ"
"ベースのコンマ区切りリストに置き換えます。"

msgid ""
"Restart AppArmor. For servers that use ``init``, run the following command:"
msgstr ""
"AppArmor を再起動します。``init`` を使用するサーバーの場合、以下のコマンドを"
"実行します。"

msgid "Restart the HAProxy service."
msgstr "HAProxy サービスを再起動します。"

msgid "Restart the host or, to make changes work immediately, invoke:"
msgstr ""
"すぐに変更を反映するため、ホストを再起動します。または、以下を実行します。"

msgid "Restarting the cluster"
msgstr "クラスターの再起動"

msgid "Retry connecting with RabbitMQ:"
msgstr "RabbitMQ の接続を再試行します。"

msgid "Run Networking DHCP agent"
msgstr "Networking DHCP エージェントの実行"

msgid "Run Networking L3 agent"
msgstr "Networking L3 エージェントの実行"

msgid "Run the following commands on each node except the first one:"
msgstr "1 番目のノード以外の各ノードで以下のコマンドを実行します。"

msgid ""
"Run the following commands to download the OpenStack Identity resource to "
"Pacemaker:"
msgstr ""
"以下のコマンドを実行して、OpenStack Identity のリソースを Pacemaker にダウン"
"ロードします。"

msgid "SELinux"
msgstr "SELinux"

msgid "SELinux and AppArmor set to permit access to ``mysqld``"
msgstr "``mysqld`` へのアクセス許可を設定した SELinux や AppArmor"

msgid "SUSE"
msgstr "SUSE"

msgid ""
"SUSE Enterprise Linux and SUSE-based distributions, such as openSUSE, use a "
"set of OCF agents for controlling OpenStack services."
msgstr ""
"SUSE Enterprise Linux、openSUSE などの SUSE 系ディストリビューションは、"
"OpenStack のサービスを制御するために OCF エージェント群を使用します。"

msgid ""
"Security-Enhanced Linux is a kernel module for improving security on Linux "
"operating systems. It is commonly enabled and configured by default on Red "
"Hat-based distributions. In the context of Galera Cluster, systems with "
"SELinux may block the database service, keep it from starting, or prevent it "
"from establishing network connections with the cluster."
msgstr ""
"Security-Enhanced Linux は、Linux オペレーティングシステムにおいてセキュリ"
"ティーを向上させるためのカーネルモジュールです。Red Hat 系のディストリビュー"
"ションでは、一般的にデフォルトで有効化され、設定されています。Galera Cluster "
"の観点では、SELinux を有効化したシステムは、データベースサービスをブロックす"
"るかもしれません。また、クラスターを起動しても、ネットワーク接続を確立できな"
"いかもしれません。"

msgid "Segregated"
msgstr "Segregated"

msgid ""
"Services like RabbitMQ and Galera have complicated boot-up sequences that "
"require co-ordination, and often serialization, of startup operations across "
"all machines in the cluster. This is especially true after a site-wide "
"failure or shutdown where you must first determine the last machine to be "
"active."
msgstr ""
"RabbitMQ や Galera などのサービスは、複雑な起動順番を持ちます。クラスター内の"
"全マシンに渡り、起動処理の協調動作を必要とし、しばしば順番に実行する必要があ"
"ります。とくに、サイト全体の障害後、最後にアクティブにするマシンを判断する必"
"要のあるシャットダウンのときに当てはまります。"

msgid "Set a password for hacluster user on each host:"
msgstr "各ホストにおいて hacluster ユーザーのパスワードを設定します。"

msgid "Set automatic L3 agent failover for routers"
msgstr "ルーター向け L3 エージェントの自動フェイルオーバーの設定"

msgid "Set basic cluster properties"
msgstr "基本的なクラスターのプロパティの設定"

msgid "Set up Corosync with multicast"
msgstr "マルチキャストを使う場合の Corosync の設定"

msgid "Set up Corosync with unicast"
msgstr "ユニキャストを使う場合の Corosync の設定"

msgid "Set up Corosync with votequorum library"
msgstr "votequorum ライブラリーを使う場合の Corosync の設定"

msgid "Set up the cluster with `crmsh`"
msgstr "`crmsh` を用いたクラスターのセットアップ"

msgid "Set up the cluster with pcs"
msgstr "pcs を用いたセットアップ"

msgid ""
"Setting ``last_man_standing`` to 1 enables the Last Man Standing (LMS) "
"feature. By default, it is disabled (set to 0). If a cluster is on the "
"quorum edge (``expected_votes:`` set to 7; ``online nodes:`` set to 4) for "
"longer than the time specified for the ``last_man_standing_window`` "
"parameter, the cluster can recalculate quorum and continue operating even if "
"the next node will be lost. This logic is repeated until the number of "
"online nodes in the cluster reaches 2. In order to allow the cluster to step "
"down from 2 members to only 1, the ``auto_tie_breaker`` parameter needs to "
"be set. We do not recommended this for production environments."
msgstr ""
"``last_man_standing`` を 1 に設定することにより、Last Man Standing (LMS) 機能"
"を有効化できます。デフォルトで、無効化されています (0 に設定)。クラスターが、"
"``last_man_standing_window`` パラメーターに指定した時間より長く、クォーラム"
"エッジ (``expected_votes:`` が 7 に設定、 ``online nodes:`` が 4 に設定) にあ"
"る場合、クラスターはクォーラムを再計算して、次のノードが失われても動作を継続"
"します。この論理は、クラスターのオンラインノードが 2 になるまで繰り返されま"
"す。クラスターが 2 つのメンバーから 1 つだけに減ることを許可するために、 "
"``auto_tie_breaker`` パラメーターを設定する必要があります。これは本番環境では"
"推奨されません。"

msgid ""
"Setting the ``pe-warn-series-max``, ``pe-input-series-max``, and ``pe-error-"
"series-max`` parameters to 1000 instructs Pacemaker to keep a longer history "
"of the inputs processed and errors and warnings generated by its Policy "
"Engine. This history is useful if you need to troubleshoot the cluster."
msgstr ""
"パラメーター ``pe-warn-series-max``, ``pe-input-series-max``, ``pe-error-"
"series-max`` を 1000 に設定することにより、Pacemaker が処理した入力履歴、ポリ"
"シーエンジンにより生成されたログと警告を保持するよう指定できます。この履歴"
"は、クラスターのトラブルシューティングを必要とする場合に役立ちます。"

msgid ""
"Setting this parameter to ``1`` or ``2`` can improve performance, but it "
"introduces certain dangers. Operating system failures can erase the last "
"second of transactions. While you can recover this data from another node, "
"if the cluster goes down at the same time (in the event of a data center "
"power outage), you lose this data permanently."
msgstr ""
"このパラメーターを ``1`` か ``2`` に設定することにより、性能を改善できます"
"が、ある種の危険性があります。オペレーティングシステムの障害が、最後の数秒の"
"トランザクションを消去する可能性があります。このデータを他のノードから復旧す"
"ることもできますが、クラスターが同時に停止した場合 (データセンターの電源障害"
"時)、このデータを完全に失います。"

msgid "Simplified process for adding/removing of nodes"
msgstr "ノードの追加と削除を簡単化したプロセス"

msgid ""
"Since all API access is directed to the proxy, adding or removing nodes has "
"no impact on the configuration of other services. This can be very useful in "
"upgrade scenarios where an entirely new set of machines can be configured "
"and tested in isolation before telling the proxy to direct traffic there "
"instead."
msgstr ""
"すべての API アクセスがプロキシーに向けられているので、ノードの追加や削除は、"
"他のサービスの設定に影響を与えません。これにより、プロキシーが新しいマシン群"
"に通信を向ける前に、それらを独立した環境において設定してテストする、アップグ"
"レードシナリオにおいて非常に役立ちます。"

msgid ""
"Since the cluster is a single administrative domain, it is acceptable to use "
"the same password on all nodes."
msgstr ""
"クラスターは単一の管理ドメインなので、一般的にすべてのノードで同じパスワード"
"を使用できます。"

msgid "Single-controller high availability mode"
msgstr "シングルコントローラーの高可用性モード"

msgid ""
"Specifying ``corosync_votequorum`` enables the votequorum library. This is "
"the only required option."
msgstr ""
"``corosync_votequorum`` を指定することにより、votequorum ライブラリーを有効化"
"します。これは唯一の必須オプションです。"

msgid "Start Corosync"
msgstr "Corosync の開始"

msgid "Start Pacemaker"
msgstr "Pacemaker の開始"

msgid "Start ``corosync`` with systemd unit file:"
msgstr "systemd ユニットファイルを用いた ``corosync`` の起動:"

msgid "Start ``corosync`` with the LSB init script:"
msgstr "LSBinit スクリプトを用いた ``corosync`` の起動:"

msgid "Start ``corosync`` with upstart:"
msgstr "upstart を用いた ``corosync`` の起動:"

msgid "Start ``pacemaker`` with the LSB init script:"
msgstr "LSBinit スクリプトを用いた ``pacemaker`` の起動:"

msgid "Start ``pacemaker`` with the systemd unit file:"
msgstr "systemd ユニットファイルを用いた ``pacemaker`` の起動:"

msgid "Start ``pacemaker`` with upstart:"
msgstr "upstart を用いた ``pacemaker`` の起動:"

msgid ""
"Start the ``xinetd`` daemon for ``clustercheck``. For servers that use "
"``init``, run the following commands:"
msgstr ""
"``clustercheck`` の ``xinetd`` デーモンを起動します。 ``init`` を使用するサー"
"バーの場合、以下のコマンドを実行します。"

msgid ""
"Start the database server on all other cluster nodes. For servers that use "
"``init``, run the following command:"
msgstr ""
"すべての他のクラスターノードにおいてデータベースサーバーを起動します。"
"``init`` を使用するサーバーに対して、以下のコマンドを実行します。"

msgid ""
"Start the message queue service on all nodes and configure it to start when "
"the system boots. On Ubuntu, it is configured by default."
msgstr ""
"すべてのノードにおいてメッセージキューサービスを起動し、システム起動時に起動"
"するよう設定します。Ubuntu の場合、デフォルトで設定されます。"

msgid "Stateful service"
msgstr "ステートフルサービス"

msgid ""
"Stateful services can be configured as active/passive or active/active, "
"which are defined as follows:"
msgstr ""
"ステートフルサービスは、アクティブ/パッシブまたはアクティブ/アクティブとして"
"設定できます。これらは以下のように定義されます。"

msgid "Stateless service"
msgstr "ステートレスサービス"

msgid "Stateless versus stateful services"
msgstr "ステートレスサービスとステートフルサービス"

msgid ""
"Stop RabbitMQ and copy the cookie from the first node to each of the other "
"node(s):"
msgstr ""
"RabbitMQ を停止して、1 番目のノードから他のノードにクッキーをコピーします。"

msgid "Storage"
msgstr "ストレージ"

msgid "Storage back end"
msgstr "ストレージバックエンド"

msgid "Storage components"
msgstr "ストレージ構成要素"

msgid ""
"System downtime: Occurs when a user-facing service is unavailable beyond a "
"specified maximum amount of time."
msgstr ""
"システム停止時間: 指定された最大時間を超えて、ユーザーサービスが利用不可能に"
"なること。"

msgid "Telemetry"
msgstr "Telemetry"

msgid "Telemetry polling agent"
msgstr "Telemetry ポーリングエージェント"

msgid ""
"The :command:`crm configure` command supports batch input. Copy and paste "
"the lines in the next step into your live Pacemaker configuration and then "
"make changes as required."
msgstr ""
":command:`crm configure` はバッチ入力をサポートします。そのため、現在の "
"pacemaker 設定の中に上の行をコピー・ペーストし、適宜変更を反映できます。"

msgid ""
"The :command:`crm configure` supports batch input. Copy and paste the lines "
"in the next step into your live Pacemaker configuration and then make "
"changes as required."
msgstr ""
":command:`crm configure` はバッチ入力をサポートします。そのため、現在の "
"pacemaker 設定の中に上の行をコピー・ペーストし、適宜変更を反映できます。"

msgid ""
"The :command:`crm configure` supports batch input. You may have to copy and "
"paste the above lines into your live Pacemaker configuration, and then make "
"changes as required."
msgstr ""
":command:`crm configure` はバッチ入力をサポートします。そのため、現在の "
"Pacemaker 設定の中に上をコピー・ペーストし、適宜変更を反映できます。"

msgid ""
"The Block Storage service (cinder) that can use LVM or Ceph RBD as the "
"storage back end."
msgstr ""
"ストレージバックエンドとして LVM や Ceph RBD を使用できる Block Storage サー"
"ビス (cinder)。"

msgid ""
"The Galera cluster configuration directive ``backup`` indicates that two of "
"the three controllers are standby nodes. This ensures that only one node "
"services write requests because OpenStack support for multi-node writes is "
"not yet production-ready."
msgstr ""
"この Galera cluster の設定ディレクティブ ``backup`` は、3 つのコントローラー"
"の内 2 つがスタンバイノードであることを意味します。"

msgid ""
"The Image service (glance) that can use the Object Storage service (swift) "
"or Ceph RBD as the storage back end."
msgstr ""
"ストレージバックエンドとして Object Storage サービス (swift) や Ceph RBD を使"
"用できる Image サービス (glance)。"

msgid ""
"The L2 agent cannot be distributed and highly available. Instead, it must be "
"installed on each data forwarding node to control the virtual network driver "
"such as Open vSwitch or Linux Bridge. One L2 agent runs per node and "
"controls its virtual interfaces."
msgstr ""
"分散させることはできず、高可用構成にはできません。その代わり、 L2 エージェン"
"トを各データ転送ノードにインストールして、 Open vSwitch や Linux ブリッジなど"
"の仮想ネットワークドライバーを制御します。ノードあたり 1 つの L2 エージェント"
"が動作し、そのノードの仮想インターフェースの制御を行います。"

msgid ""
"The Memcached client implements hashing to balance objects among the "
"instances. Failure of an instance impacts only a percentage of the objects "
"and the client automatically removes it from the list of instances. The SLA "
"is several minutes."
msgstr ""
"Memcached クライアントは、インスタンス間でオブジェクトを分散するハッシュ機能"
"を持ちます。インスタンスの障害は、オブジェクトの使用率のみに影響します。クラ"
"イアントは、インスタンスの一覧から自動的に削除されます。SLA は数分です。"

msgid ""
"The Memcached client implements hashing to balance objects among the "
"instances. Failure of an instance only impacts a percentage of the objects, "
"and the client automatically removes it from the list of instances."
msgstr ""
"Memcached クライアントは、インスタンス間でオブジェクトを分散するハッシュ機能"
"を持ちます。インスタンスの障害は、オブジェクトの使用率のみに影響します。クラ"
"イアントは、インスタンスの一覧から自動的に削除されます。"

msgid ""
"The Networking (neutron) service L3 agent is scalable, due to the scheduler "
"that supports Virtual Router Redundancy Protocol (VRRP) to distribute "
"virtual routers across multiple nodes. For more information about the VRRP "
"and keepalived, see `Linux bridge: High availability using VRRP <https://"
"docs.openstack.org/newton/networking-guide/config-dvr-ha-snat.html>`_ and "
"`Open vSwitch: High availability using VRRP <https://docs.openstack.org/"
"newton/networking-guide/deploy-ovs-ha-vrrp.html>`_."
msgstr ""
"Networking (neutron) サービス L3 エージェントは、スケーラブルです。複数のノー"
"ドにわたり仮想ルーターを分散するために、スケジューラーが Virtual Router "
"Redundancy Protocol (VRRP) をサポートするためです。設定済みのルーターを高可用"
"化するために、 :file:`/etc/neutron/neutron.conf` ファイルを編集して、以下の値"
"を設定します。VRRP と keepalived の詳細は、`Linux bridge: High availability "
"using VRRP <https://docs.openstack.org/newton/networking-guide/config-dvr-ha-"
"snat.html>`_ および `Open vSwitch: High availability using VRRP <https://"
"docs.openstack.org/newton/networking-guide/deploy-ovs-ha-vrrp.html>`_ を参照"
"してください。"

msgid ""
"The OpenStack HA community holds `weekly IRC meetings <https://wiki."
"openstack.org/wiki/Meetings/HATeamMeeting>`_ to discuss a range of topics "
"relating to HA in OpenStack. Everyone interested is encouraged to attend. "
"The `logs of all previous meetings <http://eavesdrop.openstack.org/meetings/"
"ha/>`_ are available to read."
msgstr ""
"OpenStack HA コミュニティーは、OpenStack における HA に関連する各種話題を議論"
"するために、`毎週 IRC ミーティング <https://wiki.openstack.org/wiki/Meetings/"
"HATeamMeeting>`_ を開催しています。興味のある誰でも参加することが推奨されま"
"す。`これまでのミーティングのログ <http://eavesdrop.openstack.org/meetings/"
"ha/>`_ を参照できます。"

msgid ""
"The OpenStack Image service offers a service for discovering, registering, "
"and retrieving virtual machine images. To make the OpenStack Image API "
"service highly available in active/passive mode, you must:"
msgstr ""
"OpenStack Image サービスは、仮想マシンイメージを検索、登録、取得するための"
"サービスを提供します。OpenStack Image API サービスをアクティブ/パッシブモード"
"で高可用性にするために、以下が必要になります。"

msgid ""
"The OpenStack Installation Tutorials and Guides also include a list of the "
"services that use passwords with important notes about using them."
msgstr ""
"OpenStack インストールチュートリアル・インストールガイドは、パスワードを使用"
"するサービスの一覧、それらを使用する上の重要な注意点もまとめてあります。"

msgid ""
"The OpenStack Networking (neutron) service has a scheduler that lets you run "
"multiple agents across nodes. The DHCP agent can be natively highly "
"available."
msgstr ""
"OpenStack Networking (neutron) サービスには、ノードにまたがって複数のエージェ"
"ントを実行できるスケジューラーがあります。"

msgid "The Pacemaker architecture"
msgstr "Pacemaker アーキテクチャー"

msgid ""
"The Pacemaker service also requires an additional configuration file ``/etc/"
"corosync/uidgid.d/pacemaker`` to be created with the following content:"
msgstr ""
"Pacemaker サービスは、以下の内容で作成された、追加の設定ファイル ``/etc/"
"corosync/uidgid.d/pacemaker`` も必要とします。"

msgid ""
"The SQL relational database server provides stateful type consumed by other "
"components. Supported databases are MySQL, MariaDB, and PostgreSQL. Making "
"the SQL database redundant is complex."
msgstr ""
"SQL リレーショナルデータベースサーバーは、他のコンポーネントにより利用される"
"ステートフルな状態を提供します。サポートされるデータベースは、MySQL、"
"MariaDB、PostgreSQL です。SQL データベースを冗長化することは複雑です。"

msgid ""
"The Telemetry API service configuration does not have the ``option httpchk`` "
"directive as it cannot process this check properly."
msgstr ""
"Telemetry API サービスの設定は、このチェックを正常に実行できないため、 "
"``option httpchk`` ディレクティブがありません。"

msgid ""
"The Telemetry polling agent can be configured to partition its polling "
"workload between multiple agents. This enables high availability (HA)."
msgstr ""
"Telemetry ポーリングエージェントは、複数のエージェント間でポーリングする負荷"
"を分割するよう設定できます。これにより、高可用性 (HA) を有効化できます。"

msgid ""
"The `Telemetry service <https://docs.openstack.org/admin-guide/common/get-"
"started-telemetry.html>`_ provides a data collection service and an alarming "
"service."
msgstr ""
"`Telemetry サービス <https://docs.openstack.org/admin-guide/common/get-"
"started-telemetry.html>`_ は、データ収集サービスとアラームサービスを提供しま"
"す。"

msgid ""
"The `Tooz <https://pypi.python.org/pypi/tooz>`_ library provides the "
"coordination within the groups of service instances. It provides an API "
"above several back ends that can be used for building distributed "
"applications."
msgstr ""
"`Tooz <https://pypi.python.org/pypi/tooz>`_ ライブラリーは、サービスインスタ"
"ンスのグループ内に条件を提供します。分散アプリケーションを構築するために使用"
"できる、いくつかのバックエンドに上の API を提供します。"

msgid ""
"The ``-p`` option is used to give the password on command line and makes it "
"easier to script."
msgstr ""
"``-p`` オプションは、コマンドラインにおいてパスワードを指定して、スクリプト化"
"しやすくするために使用されます。"

msgid ""
"The ``admin_bind_host`` parameter lets you use a private network for admin "
"access."
msgstr ""
"``admin_bind_host`` パラメーターにより、管理アクセスのためのプライベートネッ"
"トワークを使用できます。"

msgid ""
"The ``bindnetaddr`` is the network address of the interfaces to bind to. The "
"example uses two network addresses of /24 IPv4 subnets."
msgstr ""
"``bindnetaddr`` は、バインドするインターフェースのネットワークアドレスです。"
"この例は、2 つの /24 IPv4 サブネットを使用します。"

msgid ""
"The ``token`` value specifies the time, in milliseconds, during which the "
"Corosync token is expected to be transmitted around the ring. When this "
"timeout expires, the token is declared lost, and after "
"``token_retransmits_before_loss_const lost`` tokens, the non-responding "
"processor (cluster node) is declared dead. ``token × "
"token_retransmits_before_loss_const`` is the maximum time a node is allowed "
"to not respond to cluster messages before being considered dead. The default "
"for token is 1000 milliseconds (1 second), with 4 allowed retransmits. These "
"defaults are intended to minimize failover times, but can cause frequent "
"false alarms and unintended failovers in case of short network "
"interruptions. The values used here are safer, albeit with slightly extended "
"failover times."
msgstr ""
"``token`` の値は、Corosync トークンがリング内を転送されることが予想される時間"
"をミリ秒単位で指定します。このタイムアウトを過ぎると、トークンが失われます。 "
"``token_retransmits_before_loss_const lost`` トークンの後、応答しないプロセッ"
"サー (クラスターノード) が停止していると宣言されます。  ``token × "
"token_retransmits_before_loss_const`` は、ノードが停止とみなされるまでに、ク"
"ラスターメッセージに応答しないことが許される最大時間です。トークン向けのデ"
"フォルトは、1000 ミリ秒 (1 秒)、4 回の再送許可です。これらのデフォルト値は、"
"フェイルオーバー時間を最小化することを意図していますが、頻繁な誤検知と短い"
"ネットワーク中断による意図しないフェイルオーバーを引き起こす可能性がありま"
"す。ここで使用される値は、フェイルオーバー時間がわずかに長くなりますが、より"
"安全です。"

msgid ""
"The ``transport`` directive controls the transport mechanism. To avoid the "
"use of multicast entirely, specify the ``udpu`` unicast transport parameter. "
"This requires specifying the list of members in the ``nodelist`` directive. "
"This potentially makes up the membership before deployment. The default is "
"``udp``. The transport type can also be set to ``udpu`` or ``iba``."
msgstr ""
"``transport`` ディレクティブは使用するトランスポートメカニズムを制御します。 "
"マルチキャストを完全に無効にするためには、``udpu`` ユニキャストトランスポート"
"パラメーターを指定します。``nodelist`` ディレクティブにメンバー一覧を指定する"
"必要があります。展開する前にメンバーシップを構成することができます。デフォル"
"トは ``udp`` です。トランスポート形式は ``udpu`` や ``iba`` に設定することも"
"できます。"

msgid ""
"The application layer is controlled by the ``oslo.messaging`` configuration "
"options for multiple AMQP hosts. If the AMQP node fails, the application "
"reconnects to the next one configured within the specified reconnect "
"interval. The specified reconnect interval constitutes its SLA."
msgstr ""
"アプリケーション層は、複数 AMQP ホスト向けの ``oslo.messaging`` 設定オプショ"
"ンにより制御されます。AMQP ノードが故障したとき、アプリケーションが、指定され"
"た再接続間隔で、設定された次のノードに再接続します。"

msgid ""
"The architectural challenges of instance HA and several currently existing "
"solutions were presented in `a talk at the Austin summit <https://www."
"openstack.org/videos/video/high-availability-for-pets-and-hypervisors-state-"
"of-the-nation>`_, for which `slides are also available <http://aspiers."
"github.io/openstack-summit-2016-austin-compute-ha/>`_."
msgstr ""
"インスタンス HA のアーキテクチャー的な考慮事項と既存のソリューションは、"
"`Austin summit の講演 <https://www.openstack.org/videos/video/high-"
"availability-for-pets-and-hypervisors-state-of-the-nation>`_ にあります。ま"
"た `スライド <http://aspiers.github.io/openstack-summit-2016-austin-compute-"
"ha/>`_ も参照できます。"

msgid ""
"The architectures differ in the sets of services managed by the cluster."
msgstr ""
"アーキテクチャーは、クラスターにより管理されるサービス群により異なります。"

msgid ""
"The availability check of the instances is provided by heartbeat messages. "
"When the connection with an instance is lost, the workload will be "
"reassigned within the remaining instances in the next polling cycle."
msgstr ""
"インスタンスの死活監視は、ハートビートメッセージによって提供されます。インス"
"タンスとの接続が失われた時、次のポーリングサイクルにて、ワークロードは、残っ"
"たインスタンスの中で再割り当てが行われます。"

msgid ""
"The benefits of this approach are the physical isolation between components "
"and the ability to add capacity to specific components."
msgstr ""
"この方法の利点は、コンポーネント間の物理的な隔離、特定のコンポーネントへの"
"キャパシティーの追加です。"

msgid ""
"The cloud controller runs on the management network and must talk to all "
"other services."
msgstr ""
"クラウドコントローラーは、管理ネットワークで動作し、他のすべてのサービスと通"
"信できる必要があります。"

msgid ""
"The cluster is fully operational with ``expected_votes`` set to 7 nodes "
"(each node has 1 vote), quorum: 4. If a list of nodes is specified as "
"``nodelist``, the ``expected_votes`` value is ignored."
msgstr ""
"このクラスターは、7 ノード (各ノードが 1 つの投票権を持つ)、クォーラム 4 つに"
"設定した ``expected_votes`` で完全に動作します。ノードの一覧は ``nodelist`` "
"に指定された場合、 ``expected_votes`` の値は無視されます。"

msgid ""
"The code for three of these solutions can be found online at the following "
"links:"
msgstr ""
"これら 3 つのソリューションのコードは、以下のリンクからオンライン参照できま"
"す。"

msgid ""
"The command :command:`crm configure` supports batch input, copy and paste "
"the lines above into your live Pacemaker configuration and then make changes "
"as required. For example, you may enter ``edit p_ip_cinder-api`` from the :"
"command:`crm configure` menu and edit the resource to match your preferred "
"virtual IP address."
msgstr ""
":command:`crm configure` コマンドはバッチ入力をサポートします。現在の "
"Pacemaker 設定の中に上の行をコピー・ペーストし、適宜変更を反映できます。例え"
"ば、お好みの仮想 IP アドレスに一致させるために、:command:`crm configure` メ"
"ニューから ``edit  p_ip_cinder-api`` と入力し、リソースを編集できます。"

msgid ""
"The commands for installing RabbitMQ are specific to the Linux distribution "
"you are using."
msgstr ""
"RabbitMQ のインストールコマンドは、使用している Linux ディストリビューション"
"により異なります。"

msgid ""
"The correct path to ``libgalera_smm.so`` given to the ``wsrep_provider`` "
"parameter"
msgstr ""
"``wsrep_provider`` パラメーターに指定された ``libgalera_smm.so`` への適切なパ"
"ス"

msgid ""
"The default node type is a disc node. In this guide, nodes join the cluster "
"as RAM nodes."
msgstr ""
"デフォルトのノード種別は disc ノードです。このガイドでは、ノードは RAM ノード"
"としてクラスターに参加します。"

msgid ""
"The first step is to install the database that sits at the heart of the "
"cluster. To implement high availability, run an instance of the database on "
"each controller node and use Galera Cluster to provide replication between "
"them. Galera Cluster is a synchronous multi-master database cluster, based "
"on MySQL and the InnoDB storage engine. It is a high-availability service "
"that provides high system uptime, no data loss, and scalability for growth."
msgstr ""
"最初の手順は、クラスターの中心になるデータベースをインストールすることです。"
"高可用性を実現するために、各コントローラーノードにおいてデータベースを実行"
"し、ノード間でレプリケーションできる Galera Cluster を使用します。Galera "
"Cluster は、MySQL と InnoDB ストレージエンジンをベースにした、同期型のマルチ"
"マスターデータベースクラスターです。高いシステム稼働時間、データ損失なし、ス"
"ケーラビリティーを提供する、高可用性サービスです。"

msgid "The following are the definitions of stateless and stateful services:"
msgstr "以下は、ステートレスサービスとステートフルサービスの定義です。"

msgid "The following are the standard hardware requirements:"
msgstr "標準ハードウェア要件:"

msgid ""
"The following components are currently unable to benefit from the use of a "
"proxy server:"
msgstr ""
"以下のコンポーネントは、現在、プロキシサーバーの利用による利点はありません。"

msgid "The following components/services can work with HA queues:"
msgstr "以下のコンポーネントやサービスは、HA キューを用いて動作できます。"

msgid ""
"The following section(s) detail how to add the OpenStack Identity resource "
"to Pacemaker on SUSE and Red Hat."
msgstr ""
"以下のセクションは、SUSE と Red Hat において OpenStack Identity のリソースを "
"Pacemaker にダウンロードする方法を記載します。"

msgid ""
"The majority of services, needing no real orchestration, are handled by "
"systemd on each node. This approach avoids the need to coordinate service "
"upgrades or location changes with the cluster and has the added advantage of "
"more easily scaling beyond Corosync's 16 node limit. However, it will "
"generally require the addition of an enterprise monitoring solution such as "
"Nagios or Sensu for those wanting centralized failure reporting."
msgstr ""
"実際のオーケストレーションを必要としない、大多数のサービスは各ノードにおいて "
"systemd により処理されます。このアプローチは、クラスターでサービスのアップグ"
"レードや位置の変更を調整する必要性を避けます。また、Corosync の 16 ノード制限"
"を超えて簡単にスケールするという利点を得られます。しかしながら一般的に、障害"
"レポートを集約するために、Nagios や Sensu のようなエンタープライズモニタリン"
"グソリューションを追加する必要があります。"

msgid ""
"The most popular AMQP implementation used in OpenStack installations is "
"RabbitMQ."
msgstr ""
"OpenStack 環境に使用される最も一般的な AMQP ソフトウェアは RabbitMQ です。"

msgid ""
"The proxy can be configured as a secondary mechanism for detecting service "
"failures. It can even be configured to look for nodes in a degraded state "
"(such as being too far behind in the replication) and take them out of "
"circulation."
msgstr ""
"プロキシーは、サービスの障害を検知するための 2 番目の機構として設定できます。"
"(長く複製から外れているなど) デグレード状態にあるノードを探して、それらを除外"
"するよう設定することもできます。"

msgid ""
"The quorum specifies the minimal number of nodes that must be functional in "
"a cluster of redundant nodes in order for the cluster to remain functional. "
"When one node fails and failover transfers control to other nodes, the "
"system must ensure that data and processes remain sane. To determine this, "
"the contents of the remaining nodes are compared and, if there are "
"discrepancies, a majority rules algorithm is implemented."
msgstr ""
"クォーラムは、クラスターが機能し続けるために、冗長化されたノードのクラスター"
"において機能し続ける必要がある最小ノード数を指定します。あるノードが停止し"
"て、他のノードに制御がフェールオーバーするとき、システムはデータとプロセスが"
"維持されることを保証する必要があります。これを判断するために、残りのノードの"
"内容が比較される必要があります。また、不整合がある場合、多数決論理が実装され"
"ます。"

msgid ""
"The service declaration for the Pacemaker service may be placed in the :file:"
"`corosync.conf` file directly or in its own separate file, :file:`/etc/"
"corosync/service.d/pacemaker`."
msgstr ""
"Pacemaker サービスに関するサービス定義は、直接 :file:`corosync.conf` ファイル"
"にあるか、単独ファイル :file:`/etc/corosync/service.d/pacemaker` にある可能性"
"があります。"

msgid "The steps to implement the Pacemaker cluster stack are:"
msgstr "Pacemaker クラスタースタックを実行する手順は、次のとおりです。"

msgid ""
"The votequorum library has been created to replace and eliminate ``qdisk``, "
"the disk-based quorum daemon for CMAN, from advanced cluster configurations."
msgstr ""
"votequorum ライブラリーは、高度なクラスター設定により、 ``qdisk`` 、CMAN 向け"
"ディスクベースのクォーラムデーモンを置き換えて除去するために作成されます。"

msgid ""
"The votequorum library is part of the Corosync project. It provides an "
"interface to the vote-based quorum service and it must be explicitly enabled "
"in the Corosync configuration file. The main role of votequorum library is "
"to avoid split-brain situations, but it also provides a mechanism to:"
msgstr ""
"votequorum ライブラリーは Corosync プロジェクトの一部です。投票ベースのクォー"
"ラムサービスへのインターフェースを提供し、Corosync 設定ファイルにおいて明示的"
"に有効化する必要があります。votequorum ライブラリーのおもな役割は、スプリット"
"ブレイン状態を避けるためですが、以下の機能も提供します。"

msgid ""
"These agents must conform to one of the `OCF <https://github.com/"
"ClusterLabs/ OCF-spec/blob/master/ra/resource-agent-api.md>`_, `SysV Init "
"<http://refspecs.linux-foundation.org/LSB_3.0.0/LSB-Core-generic/ LSB-Core-"
"generic/iniscrptact.html>`_, Upstart, or Systemd standards."
msgstr ""
"これらのエージェントは、 `OCF <https://github.com/ClusterLabs/ OCF-spec/blob/"
"master/ra/resource-agent-api.md>`_, `SysV Init <http://refspecs.linux-"
"foundation.org/LSB_3.0.0/LSB-Core-generic/ LSB-Core-generic/iniscrptact."
"html>`_, Upstart, Systemd 標準に従う必要があります。"

msgid "This can be achieved using the :command:`iptables` command:"
msgstr "これは :command:`iptables` コマンドを使用して実現できます。"

msgid ""
"This chapter describes the basic environment for high availability, such as "
"hardware, operating system, common services."
msgstr ""
"この章は高可用性を実現するための基本的な環境、例えばハードウェアやオペレー"
"ションシステム、共通サービスについて説明します。"

msgid ""
"This chapter describes the shared services for high availability, such as "
"database, messaging service."
msgstr ""
"この章では、データベース、メッセージングサービスといった共有サービスの高可用"
"性について説明します。"

msgid ""
"This configuration creates ``p_cinder-api``, a resource for managing the "
"Block Storage API service."
msgstr ""
"この設定は Block Storage API サービスを管理するためのリソース ``p_cinder-"
"api`` を作成します。"

msgid ""
"This configuration creates ``p_glance-api``, a resource for managing the "
"OpenStack Image API service."
msgstr ""
"この設定は ``p_glance-api`` を作成します。これは OpenStack Image API サービス"
"を管理するリソースです。"

msgid ""
"This configuration creates ``p_keystone``, a resource for managing the "
"OpenStack Identity service."
msgstr ""
"この設定は OpenStack Identity サービスを管理するためのリソース "
"``p_keystone`` を作成します。"

msgid ""
"This configuration creates ``p_manila-api``, a resource for managing the "
"Shared File Systems API service."
msgstr ""
"この設定は Shared File Systems API サービスを管理するためのリソース "
"``p_manila-api`` を作成します。"

msgid ""
"This configuration creates ``vip``, a virtual IP address for use by the API "
"node (``10.0.0.11``)."
msgstr ""
"この設定は、API ノード (``10.0.0.11``) により使用される仮想 IP アドレス "
"``vip`` を作成します。"

msgid ""
"This document discusses some common methods of implementing highly available "
"systems, with an emphasis on the core OpenStack services and other open "
"source services that are closely aligned with OpenStack."
msgstr ""
"このドキュメントは、高可用性システムを実行する方法をいくつか議論します。コア"
"な OpenStack サービス、OpenStack とかなり一緒に使われる他のオープンソースサー"
"ビスを強調しています。"

msgid ""
"This example assumes that you are using NFS for the physical storage, which "
"will almost never be true in a production installation."
msgstr ""
"この例は、物理ストレージに NFS を使用していることを仮定します。これは、ほとん"
"どの本番環境のインストールにおいて正しくありません。"

msgid ""
"This guide describes how to install and configure OpenStack for high "
"availability. It supplements the Installation Tutorials and Guides and "
"assumes that you are familiar with the material in those guides."
msgstr ""
"このガイドは OpenStack を高可用性にするインストール方法と設定方法を説明しま"
"す。インストールチュートリアル・ガイドを補完する位置付けであり、それらのガイ"
"ドの内容を前提に書かれています。"

msgid "This guide uses the following example IP addresses:"
msgstr "このガイドは、以下の IP アドレス例を使用します。"

msgid "This is the most common option and the one we document here."
msgstr "これは最も一般的なオプションで、ここにドキュメント化します。"

msgid ""
"This is why setting the quorum to a value less than ``floor(n/2) + 1`` is "
"dangerous. However it may be required for some specific cases, such as a "
"temporary measure at a point it is known with 100% certainty that the other "
"nodes are down."
msgstr ""
"これがクォーラムの値を ``floor(n/2) + 1`` より小さく設定することが危険な理由"
"です。しかしながら、いくつかの特別な場合に必要となる可能性があります。例え"
"ば、他のノードが 100% 確実に停止していることがわかっている場合の一時的な計測"
"などです。"

msgid ""
"This scenario can be visualized as below, where each box below represents a "
"cluster of three or more guests."
msgstr ""
"このシナリオは、以下のように可視化できます。以下の各ボックスは 3 つ以上のゲス"
"トのクラスターを表します。"

msgid "This scenario can be visualized as below."
msgstr "このシナリオは以下のように可視化できます。"

msgid ""
"This scenario has the advantage of requiring far fewer, if more powerful, "
"machines. Additionally, being part of a single cluster allows you to "
"accurately model the ordering dependencies between components."
msgstr ""
"このシナリオは、より高性能ならば、より少ないマシンを必要とする利点がありま"
"す。加えて、シングルクラスターの一部になることにより、コンポーネント間の順序"
"依存関係を正確にモデル化できます。"

msgid ""
"This section discusses ways to protect against data loss in your OpenStack "
"environment."
msgstr ""
"このセクションは、お使いの OpenStack 環境におけるデータ損失から保護する方法を"
"議論します。"

msgid ""
"This value increments with each transaction, so the most advanced node has "
"the highest sequence number and therefore is the most up to date."
msgstr ""
"この値は各トランザクションによりインクリメントされます。ほとんどの高度なノー"
"ドは、最大のシーケンス番号を持つため、ほとんど最新です。"

msgid ""
"To be sure that all data is highly available, ensure that everything is "
"stored in the MySQL database (which is also highly available):"
msgstr ""
"すべてのものを (高可用性) MySQL データベースに保存して、すべてのデータが高可"
"用性になっていることを確認します。"

msgid ""
"To configure AppArmor to work with Galera Cluster, complete the following "
"steps on each cluster node:"
msgstr ""
"各クラスターノードにおいて以下の手順を実行して、Galera Cluster を正常に動作さ"
"せるために AppArmor を設定します。"

msgid ""
"To configure SELinux to permit Galera Cluster to operate, you may need to "
"use the ``semanage`` utility to open the ports it uses. For example:"
msgstr ""
"SELinux を設定して Galera Cluster の動作を許可するために、``semanage`` ユー"
"ティリティーを使用して、使用ポートを開く必要があるかもしれません。例:"

msgid ""
"To configure the number of DHCP agents per network, modify the "
"``dhcp_agents_per_network`` parameter in the :file:`/etc/neutron/neutron."
"conf` file. By default this is set to 1. To achieve high availability, "
"assign more than one DHCP agent per network. For more information, see `High-"
"availability for DHCP <https://docs.openstack.org/newton/networking-guide/"
"config-dhcp-ha.html>`_."
msgstr ""
"ネットワークあたりの DHCP エージェント数を設定するには、 file:`/etc/neutron/"
"neutron.conf` ファイルの``dhcp_agents_per_network`` パラメーターを変更しま"
"す。このパラメーターのデフォルト値は １ です。高可用性を持たせるには、ネット"
"ワークあたりの DHCP エージェント数を １ 以上にする必要があります。詳細は "
"`High-availability for DHCP <https://docs.openstack.org/newton/networking-"
"guide/config-dhcp-ha.html>`_ を参照してください。"

msgid ""
"To enable high availability for configured routers, edit the :file:`/etc/"
"neutron/neutron.conf` file to set the following values:"
msgstr ""
"設定済みルーターを高可用性にするために、:file:`/etc/neutron/neutron.conf` "
"ファイルを編集し、以下の値を設定します。"

msgid ""
"To enable the compute agent to run multiple instances simultaneously with "
"workload partitioning, the ``workload_partitioning`` option must be set to "
"``True`` under the `compute section <https://docs.openstack.org/ocata/config-"
"reference/telemetry.html>`_ in the :file:`ceilometer.conf` configuration "
"file."
msgstr ""
"コンピュートエージェントがワークロード分割により同時に複数のインスタンスを実"
"行できるようにするために、``workload_partitioning`` オプションが :file:"
"`ceilometer.conf` 設定ファイルの `compute セクション <https://docs.openstack."
"org/ocata/config-reference/telemetry.html>`_ において ``True`` に設定する必要"
"があります。"

msgid ""
"To ensure that all queues except those with auto-generated names are "
"mirrored across all running nodes, set the ``ha-mode`` policy key to all by "
"running the following command on one of the nodes:"
msgstr ""
"自動生成された名前を持つキューを除いて、すべてのキューがすべての動作中のノー"
"ドで確実にミラーするために、以下のコマンドをどこかのノードで実行して、 ``ha-"
"mode`` ポリシーキーを all に設定します。"

msgid ""
"To find the most advanced cluster node, you need to check the sequence "
"numbers, or the ``seqnos``, on the last committed transaction for each. You "
"can find this by viewing ``grastate.dat`` file in database directory:"
msgstr ""
"最も高度なクラスターノードを見つけるために、各ノードの最新コミットのトランザ"
"クションにある ``seqnos`` を確認する必要があります。データベースディレクト"
"リーにある ``grastate.dat`` ファイルを表示すると、これを見つけられます。"

msgid ""
"To install and configure Memcached, read the `official documentation "
"<https://github.com/Memcached/Memcached/wiki#getting-started>`_."
msgstr ""
"Memcached をインストールして設定する方法は、 `公式ドキュメント <https://"
"github.com/Memcached/Memcached/wiki#getting-started>`_ を参照してください。"

msgid "To start the cluster, complete the following steps:"
msgstr "以下の手順を実行して、クラスターを起動します。"

msgid ""
"Tooz supports `various drivers <https://docs.openstack.org/developer/tooz/"
"drivers.html>`_ including the following back end solutions:"
msgstr ""
"Tooz は、以下のバックエンドソリューションを含む、 `さまざまなドライバー "
"<https://docs.openstack.org/developer/tooz/drivers.html>`_ をサポートします。"

msgid ""
"Traditionally, Pacemaker has been positioned as an all-encompassing "
"solution. However, as OpenStack services have matured, they are increasingly "
"able to run in an active/active configuration and gracefully tolerate the "
"disappearance of the APIs on which they depend."
msgstr ""
"伝統的に、Pacemaker は全方位的なソリューションとして位置づけられてきました。"
"しかしながら、OpenStack サービスが成熟するにつれて、徐々にアクティブ/アクティ"
"ブ設定にて動作でき、依存している API の消失に自然に耐えられます。"

msgid "True"
msgstr "True (真)"

msgid ""
"Typically, an active/active installation for a stateless service maintains a "
"redundant instance, and requests are load balanced using a virtual IP "
"address and a load balancer such as HAProxy."
msgstr ""
"一般的にステートレスサービスをアクティブ / アクティブにインストールすると、冗"
"長なインスタンスを維持することになります。リクエストは HAProxy のような仮想 "
"IP アドレスとロードバランサーを使用して負荷分散されます。"

msgid "Use HA queues in RabbitMQ (``x-ha-policy: all``):"
msgstr "RabbitMQ における HA キューの使用 (``x-ha-policy: all``):"

msgid ""
"Use MySQL/Galera in active/passive mode to avoid deadlocks on ``SELECT ... "
"FOR UPDATE`` type queries (used, for example, by nova and neutron). This "
"issue is discussed in the following:"
msgstr ""
"MySQL/Galera をアクティブ/パッシブモードで使用して、 ``SELECT ... FOR "
"UPDATE`` のような形式のクエリーにおけるデッドロックを避けます (例えば、nova "
"や neutron により使用されます)。この問題は、以下で議論されています。"

msgid "Use durable queues in RabbitMQ:"
msgstr "RabbitMQ での永続キューの使用:"

msgid ""
"Use that password to authenticate to the nodes that will make up the cluster:"
msgstr "このパスワードを使用して、クラスターを構成するノードに認証します。"

msgid ""
"Use the :command:`corosync-cfgtool` utility with the ``-s`` option to get a "
"summary of the health of the communication rings:"
msgstr ""
":command:`corosync-cfgtool` ユーティリティーに ``-s`` オプションを付けて実行"
"して、コミュニケーションリングの稼働状態の概要を取得します。"

msgid ""
"Use the :command:`corosync-objctl` utility to dump the Corosync cluster "
"member list:"
msgstr ""
":command:`corosync-objctl` ユーティリティーを使用して、Corosync クラスターの"
"メンバー一覧を出力します。"

msgid "Use these steps to configurate all services using RabbitMQ:"
msgstr ""
"これらの手順を使用して、RabbitMQ を使用するすべてのサービスを設定します。"

msgid "Value"
msgstr "値"

msgid "Verify that the nodes are running:"
msgstr "そのノードが動作していることを検証します。"

msgid "Verify the cluster status:"
msgstr "クラスターの状態を確認します。"

msgid "Virtualized hardware"
msgstr "仮想ハードウェア"

msgid ""
"We do not recommend setting the quorum to a value less than ``floor(n/2) + "
"1`` as it would likely cause a split-brain in a face of network partitions."
msgstr ""
"クォーラムの値を ``floor(n/2) + 1`` より小さく設定することは推奨しません。こ"
"れはネットワーク分割の発生時にスプリットブレインを引き起こす可能性がありま"
"す。"

msgid ""
"We recommend HAProxy as the load balancer, however, there are many "
"alternative load balancing solutions in the marketplace."
msgstr ""
"ロードバランサーとして HAProxy を推奨しますが、マーケットプレースにさまざまな"
"同等品があります。"

msgid ""
"We recommend two primary architectures for making OpenStack highly available."
msgstr ""
"OpenStack の高可用性のために基本的な 2 つのアーキテクチャーを推奨します。"

msgid ""
"We recommended that the maximum latency between any two controller nodes is "
"2 milliseconds. Although the cluster software can be tuned to operate at "
"higher latencies, some vendors insist on this value before agreeing to "
"support the installation."
msgstr ""
"すべての 2つのコントローラーノード間の最大レイテンシーが 2 ミリ秒であることを"
"推奨します。クラスターソフトウェアがより大きなレイテンシーで動作するよう"
"チューニングできますが、いくつかのベンダーはサポートする前にこの値を主張しま"
"す。"

msgid "What is a cluster manager?"
msgstr "クラスターマネージャーとは?"

msgid ""
"When Ceph RBD is used for ephemeral volumes as well as block and image "
"storage, it supports `live migration <https://docs.openstack.org/admin-guide/"
"compute-live-migration-usage.html>`_ of VMs with ephemeral drives. LVM only "
"supports live migration of volume-backed VMs."
msgstr ""
"Ceph RBD をブロックストレージやイメージストレージと同じように一時ストレージ用"
"に使用する場合、一時ボリュームを持つ仮想マシンの `ライブマイグレーション "
"<https://docs.openstack.org/admin-guide/compute-live-migration-usage.html>` "
"がサポートされます。LVM のみがボリュームをバックエンドとした仮想マシンのライ"
"ブマイグレーションをサポートします。"

msgid ""
"When configuring an OpenStack environment for study or demonstration "
"purposes, it is possible to turn off the quorum checking. Production systems "
"should always run with quorum enabled."
msgstr ""
"学習やデモの目的に OpenStack 環境を設定している場合、クォーラムのチェックを無"
"効化できます。本番システムは必ずクォーラムを有効化して実行すべきです。"

msgid ""
"When each cluster node starts, it checks the IP addresses given to the "
"``wsrep_cluster_address`` parameter. It then attempts to establish network "
"connectivity with a database server running there. Once it establishes a "
"connection, it attempts to join the Primary Component, requesting a state "
"transfer as needed to bring itself into sync with the cluster."
msgstr ""
"各クラスターノードが起動したとき、``wsrep_cluster_address`` パラメーターに指"
"定された IP アドレスを確認して、それで動作しているデータベースサーバーへの"
"ネットワーク接続性を確立しようとします。接続が確立されると、クラスターを同期"
"するために必要となる状態転送を要求する、Primary Component に参加しようとしま"
"す。"

msgid ""
"When four nodes fail simultaneously, the cluster would continue to function "
"as well. But if split to partitions of three and four nodes respectively, "
"the quorum of three would have made both sides to attempt to fence the other "
"and host resources. Without fencing enabled, it would go straight to running "
"two copies of each resource."
msgstr ""
"4 ノードが同時に停止するとき、クラスターは十分に動作し続けるでしょう。しか"
"し、ノードがそれぞれ 3 つと 4 つに分断された場合、3 つのクォーラムが両方で他"
"のノードとホストリソースをフェンスしようとするでしょう。フェンスを有効化して"
"いないと、各リソースの 2 つのコピーが動作し続けるでしょう。"

msgid ""
"When installing highly available OpenStack on VMs, be sure that your "
"hypervisor permits promiscuous mode and disables MAC address filtering on "
"the external network."
msgstr ""
"仮想マシン上に高可用性 OpenStack をインストールする場合、ハイパーバイザーが外"
"部ネットワークにおいてプロミスキャスモードを許可して、MAC アドレスフィルタリ"
"ングを無効化していることを確認してください。"

msgid ""
"When you finish installing and configuring the OpenStack database, you can "
"initialize the Galera Cluster."
msgstr ""
"OpenStack のデータベースをインストールして設定するとき、Galera Cluster を初期"
"化できます。"

msgid ""
"When you have all cluster nodes started, log into the database client of any "
"cluster node and check the ``wsrep_cluster_size`` status variable again:"
msgstr ""
"クラスターノードをどれか起動したとき、どれか 1 つにデータベースクライアントか"
"らログインして、``wsrep_cluster_size`` 状態変数を再び確認します。"

msgid ""
"When you start up a cluster (all nodes down) and set ``wait_for_all`` to 1, "
"the cluster quorum is held until all nodes are online and have joined the "
"cluster for the first time. This parameter is new in Corosync 2.0."
msgstr ""
"クラスター (全ノードダウン) を起動して、 ``wait_for_all`` を 1 に設定すると"
"き、クラスターのクォーラムはすべてのノードがオンラインになり、まずクラスター"
"に参加するまで保持されることを意味します。このパラメーターは Corosync 2.0 の"
"新機能です。"

msgid ""
"When you use high availability, consider the hardware requirements needed "
"for your application."
msgstr ""
"高可用性にするとき、アプリケーションに必要となるハードウェア要件を考慮してく"
"ださい。"

msgid ""
"While SYS-V init replacements like systemd can provide deterministic "
"recovery of a complex stack of services, the recovery is limited to one "
"machine and lacks the context of what is happening on other machines. This "
"context is crucial to determine the difference between a local failure, and "
"clean startup and recovery after a total site failure."
msgstr ""
"systemd のような SYS-V init の代替は、複雑なスタックのサービスにおける順序を"
"守った復旧を提供できますが、復旧は 1 台のマシンに限定され、他のマシンにおいて"
"起きたことを把握できません。このコンテキストは、ローカル障害間の違いを判断"
"し、全サイト障害から正常に起動して復旧するために重要です。"

msgid ""
"While all of the configuration parameters available to the standard MySQL, "
"MariaDB, or Percona XtraDB database servers are available in Galera Cluster, "
"there are some that you must define an outset to avoid conflict or "
"unexpected behavior."
msgstr ""
"標準的な MySQL、MariaDB、Percona XtraDB データベースに利用できる設定パラメー"
"ターは Galera Cluster で利用できますが、競合や予期しない動作を避けるために始"
"めに定義する必要があるものがあります。"

msgid ""
"While the application can still run after the failure of several instances, "
"it may not have sufficient capacity to serve the required volume of "
"requests. A cluster can automatically recover failed instances to prevent "
"additional load induced failures."
msgstr ""
"アプリケーションは、いくつかのインスタンスが故障した後も動作できますが、要求"
"されたリクエスト量を処理するための十分な容量がないかもしれません。クラスター"
"は自動的に故障したインスタンスを復旧して、さらなる負荷が障害を引き起こさない"
"ようにできます。"

msgid ""
"With ``secauth`` enabled, Corosync nodes mutually authenticates using a 128-"
"byte shared secret stored in the :file:`/etc/corosync/authkey` file. This "
"can be generated with the :command:`corosync-keygen` utility. Cluster "
"communications are encrypted when using ``secauth``."
msgstr ""
"``secauth`` を有効化すると、Corosync ノードが :file:`/etc/corosync/authkey` "
"に保存された 128 バイトの共有シークレットを使用して相互に認証されます。これ"
"は、 :command:`corosync-keygen` ユーティリティーを使用して生成できます。 "
"``secauth`` を使用するとき、クラスター通信は暗号化されます。"

msgid ""
"With this in mind, some vendors are restricting Pacemaker's use to services "
"that must operate in an active/passive mode (such as ``cinder-volume``), "
"those with multiple states (for example, Galera), and those with complex "
"bootstrapping procedures (such as RabbitMQ)."
msgstr ""
"この点を考慮して、いくつかのベンダーは、``cinder-volume`` などのアクティブ/"
"パッシブモードで動作させる必要があるサービス、Galera などの複数の状態を持つ"
"サービス、RabbitMQ のように複雑なブートストラップ手順を持つサービスに "
"Pacemaker を使用することを制限しています。"

msgid ""
"Within the ``nodelist`` directive, it is possible to specify specific "
"information about the nodes in the cluster. The directive can contain only "
"the node sub-directive, which specifies every node that should be a member "
"of the membership, and where non-default options are needed. Every node must "
"have at least the ``ring0_addr`` field filled."
msgstr ""
"``nodelist`` ディレクティブに、クラスター内のノードに関する具体的な情報を指定"
"できます。このディレクティブは、node サブディレクティブのみを含められます。こ"
"れは、メンバーシップのすべてのメンバーを指定し、デフォルト以外に必要となるオ"
"プションを指定します。すべてのノードは、少なくとも ``ring0_addr`` の項目を入"
"力する必要があります。"

msgid ""
"Work is in progress on a unified approach, which combines the best aspects "
"of existing upstream solutions. More details are available on `the HA VMs "
"user story wiki <https://wiki.openstack.org/wiki/ProductTeam/User_Stories/"
"HA_VMs>`_."
msgstr ""
"検討は統一された方法により進行中です。既存のアップストリームのソリューション"
"における利点を組み合わせます。詳細は `HA VMs user story wiki <https://wiki."
"openstack.org/wiki/ProductTeam/User_Stories/HA_VMs>`_ にあります。"

msgid ""
"You can achieve high availability for the OpenStack database in many "
"different ways, depending on the type of database that you want to use. "
"There are three implementations of Galera Cluster available to you:"
msgstr ""
"使用したいデータベースの種類に応じて、さまざまな情報で OpenStack のデータベー"
"スの高可用性を実現できます。Galera Cluster は 3 種類の実装があります。"

msgid ""
"You can also ensure the availability by other means, using Keepalived or "
"Pacemaker."
msgstr ""
"他の手段として、Pacemaker や Keepalived を使用して可能性を確保することもでき"
"ます。"

msgid ""
"You can have up to 16 cluster members (this is currently limited by the "
"ability of corosync to scale higher). In extreme cases, 32 and even up to 64 "
"nodes could be possible. However, this is not well tested."
msgstr ""
"クラスターのメンバーを 16 まで持てます (これは、corosync をよりスケールさせる"
"機能による、現在の制限です)。極端な場合、32 や 64 までのノードさえ利用できま"
"すが、十分にテストされていません。"

msgid ""
"You can now add the Pacemaker configuration for Block Storage API resource. "
"Connect to the Pacemaker cluster with the :command:`crm configure` command "
"and add the following cluster resources:"
msgstr ""
"Block Storage API リソース用の Pacemaker 設定を追加できます。 :command:`crm "
"configure` を用いて Pacemaker クラスターに接続し、以下のクラスターリソースを"
"追加します。"

msgid ""
"You can now check the ``corosync`` connectivity with one of these tools."
msgstr "corosyncの接続性をそれらのツールの一つで確認することができます。"

msgid ""
"You can read more about these concerns on the `Red Hat Bugzilla <https://"
"bugzilla.redhat.com/show_bug.cgi?id=1193229>`_ and there is a `psuedo "
"roadmap <https://etherpad.openstack.org/p/cinder-kilo-stabilisation-work>`_ "
"for addressing them upstream."
msgstr ""
"これらの課題の詳細は `Red Hat Bugzilla <https://bugzilla.redhat.com/show_bug."
"cgi?id=1193229>`_ にあります。また、アップストリームにおいて解決するための "
"`psuedo roadmap <https://etherpad.openstack.org/p/cinder-kilo-stabilisation-"
"work>`_ があります。"

msgid ""
"You can take periodic snap shots throughout the installation process and "
"roll back to a working configuration in the event of a problem."
msgstr ""
"インストール中に定期的にスナップショットを取得したり、問題発生時に動作する設"
"定にロールバックしたりできます。"

msgid "You can use the `ping` command to find the latency between two servers."
msgstr "`ping` コマンドを使用して、サーバー間のレイテンシーを調べられます。"

msgid ""
"You must also create the OpenStack Image API endpoint with this IP address. "
"If you are using both private and public IP addresses, create two virtual IP "
"addresses and define your endpoint. For example:"
msgstr ""
"この IP アドレスを用いて OpenStack Image API エンドポイントを作成する必要があ"
"ります。プライベート IP アドレスとパブリック IP アドレスを両方使用している場"
"合、2 つの仮想 IP アドレスを作成して、次のようにエンドポイントを定義します。"

msgid ""
"You must configure a supported Tooz driver for the HA deployment of the "
"Telemetry services."
msgstr ""
"Telemetry サービスの高可用性デプロイのために、サポートされる Tooz ドライバー"
"を設定する必要があります。"

msgid "You must create the Shared File Systems API endpoint with this IP."
msgstr ""
"この IP を用いて Shared File Systems API エンドポイントを作成する必要がありま"
"す。"

msgid ""
"You must select and assign a virtual IP address (VIP) that can freely float "
"between cluster nodes."
msgstr ""
"クラスターノード間で自由に移動できる仮想 IP アドレス (VIP) を選択して割り当て"
"る必要があります。"

msgid ""
"You must use the same name on every cluster node. The connection fails when "
"this value does not match."
msgstr ""
"すべてのクラスターノードにおいて同じ名前を使用する必要があります。この値が一"
"致しない場合、接続が失敗します。"

msgid ""
"You only need to do this on one cluster node. Galera Cluster replicates the "
"user to all the others."
msgstr ""
"どれか 1 つのクラスターノードにおいてのみ実行する必要があります。Galera "
"Cluster が、他のすべてのノードにユーザーを複製します。"

msgid ""
"You should see a ``status=joined`` entry for each of your constituent "
"cluster nodes."
msgstr ""
"構成している各クラスターノードが ``status=joined`` になっているはずです。"

msgid ""
"You will need to address high availability concerns for any applications "
"software that you run on your OpenStack environment. The important thing is "
"to make sure that your services are redundant and available. How you achieve "
"that is up to you."
msgstr ""
"お使いの OpenStack 環境で動作するアプリケーションソフトウェアすべてに対する高"
"可用性の課題を解決する必要があります。重要なことは、お使いのサービスが冗長で"
"あり利用できることを確実にすることです。どのように実現するのかは、あなた自身"
"によります。"

msgid ""
"You would choose this option if you prefer to have fewer but more powerful "
"boxes."
msgstr "より少数の高性能なマシンを好む場合、この選択肢を選択するでしょう。"

msgid ""
"You would choose this option if you prefer to have more but less powerful "
"boxes."
msgstr "より多数の低性能なマシンを好む場合、この選択肢を選択するでしょう。"

msgid ""
"Your OpenStack services must now point their Block Storage API configuration "
"to the highly available, virtual cluster IP address rather than a Block "
"Storage API server’s physical IP address as you would for a non-HA "
"environment."
msgstr ""
"OpenStack サービスは、非 HA 環境と同じように Block Storage API サーバーの物"
"理 IP アドレスを指定する代わりに、Block Storage API の設定が高可用性と仮想ク"
"ラスター IP アドレスを指し示す必要があります。"

msgid ""
"Your OpenStack services must now point their OpenStack Image API "
"configuration to the highly available, virtual cluster IP address instead of "
"pointing to the physical IP address of an OpenStack Image API server as you "
"would in a non-HA cluster."
msgstr ""
"OpenStack サービスが、非 HA クラスターであるような OpenStack Image API サー"
"バーの物理 IP アドレスを指し示す代わりに、高可用性な仮想クラスター IP アドレ"
"スを指し示すように、それらの OpenStack Image API の設定を変更する必要がありま"
"す。"

msgid ""
"Your OpenStack services must now point their Shared File Systems API "
"configuration to the highly available, virtual cluster IP address rather "
"than a Shared File Systems API server’s physical IP address as you would for "
"a non-HA environment."
msgstr ""
"OpenStack サービスは、通常の非高可用性環境のように、Shared File Systems API "
"サーバーの物理 IP アドレスを指定する代わりに、Shared File Systems API の設定"
"が高可用性と仮想クラスター IP アドレスを指し示す必要があります。"

msgid ""
"Your OpenStack services now point their OpenStack Identity configuration to "
"the highly available virtual cluster IP address."
msgstr ""
"OpenStack サービスが OpenStack Identity サーバーの設定が高可用性と仮想クラス"
"ター IP アドレスを指し示します。"

msgid "[TODO: need more discussion of these parameters]"
msgstr "[TODO: need more discussion of these parameters]"

msgid ""
"`Ceph RBD <https://ceph.com/>`_ is an innately high availability storage "
"back end. It creates a storage cluster with multiple nodes that communicate "
"with each other to replicate and redistribute data dynamically. A Ceph RBD "
"storage cluster provides a single shared set of storage nodes that can "
"handle all classes of persistent and ephemeral data (glance, cinder, and "
"nova) that are required for OpenStack instances."
msgstr ""
"`Ceph RBD <https://ceph.com/>`_ は、本質的に高可用性なストレージバックエンド"
"です。複数のノードを用いてストレージクラスターを作成し、お互いに通信して動的"
"にレプリケーションとデータ再配布を実行します。Ceph RBD ストレージクラスター"
"は、OpenStack インスタンスに必要となる、すべての種類の永続データと一時データ "
"(glance、cinder、nova) を取り扱える、単一の共有ストレージノードを提供します。"

msgid "`Clustering Guide <https://www.rabbitmq.com/clustering.html>`_"
msgstr "`Clustering Guide <https://www.rabbitmq.com/clustering.html>`_"

msgid "`Debian and Ubuntu <https://www.rabbitmq.com/install-debian.html>`_"
msgstr "`Debian および Ubuntu <https://www.rabbitmq.com/install-debian.html>`_"

msgid ""
"`Galera Cluster for MySQL <http://galeracluster.com>`_: The MySQL reference "
"implementation from Codership, Oy."
msgstr ""
"`Galera Cluster for MySQL <http://galeracluster.com>`_: The MySQL reference "
"implementation from Codership, Oy."

msgid "`Highly Available Queues <https://www.rabbitmq.com/ha.html>`_"
msgstr "`Highly Available Queues <https://www.rabbitmq.com/ha.html>`_"

msgid ""
"`IMPORTANT: MySQL Galera does *not* support SELECT ... FOR UPDATE <http://"
"lists.openstack.org/pipermail/openstack-dev/2014-May/035264.html>`_"
msgstr ""
"`IMPORTANT: MySQL Galera does *not* support SELECT ... FOR UPDATE <http://"
"lists.openstack.org/pipermail/openstack-dev/2014-May/035264.html>`_"

msgid ""
"`MariaDB Galera Cluster <https://mariadb.org>`_: The MariaDB implementation "
"of Galera Cluster, which is commonly supported in environments based on Red "
"Hat distributions."
msgstr ""
"`MariaDB Galera Cluster <https://mariadb.org>`_: The MariaDB implementation "
"of Galera Cluster, which is commonly supported in environments based on Red "
"Hat distributions."

msgid ""
"`Pacemaker <http://clusterlabs.org/>`_ cluster stack is a state-of-the-art "
"high availability and load balancing stack for the Linux platform. Pacemaker "
"is used to make OpenStack infrastructure highly available."
msgstr ""
"`Pacemaker <http://clusterlabs.org/>`_ クラスタースタックは、Linux プラット"
"フォーム向けの最高水準の高可用性と負荷分散を実現します。Pacemaker は "
"OpenStack インフラを高可用化するために役立ちます。"

msgid ""
"`Percona XtraDB Cluster <https://www.percona.com>`_: The XtraDB "
"implementation of Galera Cluster from Percona."
msgstr ""
"`Percona XtraDB Cluster <https://www.percona.com>`_: The XtraDB "
"implementation of Galera Cluster from Percona."

msgid ""
"`RPM based <https://www.rabbitmq.com/install-rpm.html>`_ (RHEL, Fedora, "
"CentOS, openSUSE)"
msgstr ""
"`RPM 系 <https://www.rabbitmq.com/install-rpm.html>`_ (RHEL, Fedora, CentOS, "
"openSUSE)"

msgid ""
"`Understanding reservations, concurrency, and locking in Nova <http://www."
"joinfu.com/2015/01/understanding-reservations-concurrency-locking-in-nova/>`_"
msgstr ""
"`Understanding reservations, concurrency, and locking in Nova <http://www."
"joinfu.com/2015/01/understanding-reservations-concurrency-locking-in-nova/>`_"

msgid "``crmsh``"
msgstr "``crmsh``"

msgid ""
"``last_man_standing_window`` specifies the time, in milliseconds, required "
"to recalculate quorum after one or more hosts have been lost from the "
"cluster. To perform a new quorum recalculation, the cluster must have quorum "
"for at least the interval specified for ``last_man_standing_window``. The "
"default is 10000ms."
msgstr ""
"``last_man_standing_window`` は、1 つ以上のホストがクラスターから失われた後、"
"クォーラムを再計算するために必要となる時間をミリ秒単位で指定します。新しく"
"クォーラムを再計算するために、クラスターは少なくとも "
"``last_man_standing_window`` に指定された間隔はクォーラムを保持する必要があり"
"ます。デフォルトは 10000ms です。"

msgid ""
"``nodeid`` is optional when using IPv4 and required when using IPv6. This is "
"a 32-bit value specifying the node identifier delivered to the cluster "
"membership service. If this is not specified with IPv4, the node ID is "
"determined from the 32-bit IP address of the system to which the system is "
"bound with ring identifier of 0. The node identifier value of zero is "
"reserved and should not be used."
msgstr ""
"``nodeid`` は、IPv4 を使用するときにオプション、IPv6 を使用するときに必須で"
"す。クラスターメンバーシップサービスに配信される、ノード識別子を指定する 32 "
"ビットの値です。IPv4 で指定されていない場合、ノード ID は、システムがリング識"
"別子 0 に割り当てた 32 ビットの IP アドレスになります。ノード識別子の値 0 "
"は、予約済みであり、使用してはいけません。"

msgid "``pcs``"
msgstr "``pcs``"

msgid ""
"``ring{X}_addr`` specifies the IP address of one of the nodes. ``{X}`` is "
"the ring number."
msgstr ""
"``ring{X}_addr`` は、1 つのノードの IP アドレスを指定します。 ``{X}`` はリン"
"グの番号です。"

msgid ""
"`a mistral-based auto-recovery workflow <https://github.com/gryf/mistral-"
"evacuate>`_, by Intel"
msgstr ""
"`a mistral-based auto-recovery workflow <https://github.com/gryf/mistral-"
"evacuate>`_, by Intel"

msgid "`corosync`"
msgstr "`corosync`"

msgid "`fence-agents` (CentOS or RHEL) or cluster-glue"
msgstr "`fence-agents` (CentOS、RHEL) または cluster-glue"

msgid "`libqb0`"
msgstr "`libqb0`"

msgid "`masakari <https://launchpad.net/masakari>`_, by NTT"
msgstr "`masakari <https://launchpad.net/masakari>`_, by NTT"

msgid "`pacemaker`"
msgstr "`pacemaker`"

msgid "`pcs` (CentOS or RHEL) or crmsh"
msgstr "`pcs` (CentOS、RHEL) または crmsh"

msgid "`resource-agents`"
msgstr "`resource-agents`"

msgid "allow_automatic_l3agent_failover"
msgstr "allow_automatic_l3agent_failover"

msgid "compute node"
msgstr "コンピュートノード"

msgid "controller node"
msgstr "コントローラーノード"

msgid "l3_ha"
msgstr "l3_ha"

msgid "max_l3_agents_per_router"
msgstr "max_l3_agents_per_router"

msgid "min_l3_agents_per_router"
msgstr "min_l3_agents_per_router"
